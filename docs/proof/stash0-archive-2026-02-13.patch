commit 337f9e0aaf213a51d1ca85bb5ba859aa7f3f5b23
Merge: 61028d8 213c02c e740d17
Author: suavé <34923421+penquinspecz@users.noreply.github.com>
Date:   Fri Feb 13 15:18:00 2026 -0500

    On codex/docs-roadmap-archive-restore: WIP safety stash before restoring main

diff --cc .gitignore
index 6b42888,6b42888,0000000..b497745
mode 100644,100644,000000..100644
--- a/.gitignore
+++ b/.gitignore
@@@@ -1,103 -1,103 -1,0 +1,102 @@@@
  +.venv/
  +__pycache__/
  +*.pyc
  +.env
  +.env.*
  +*.sqlite
  +.DS_Store
  +ci_data/
  +ci_state/
  +ci_artifacts/
  +smoke_artifacts/
  +data/ashby_cache_json/
  +data/ashby_cache/
  +data/openai_raw_jobs.json
  +data/openai_labeled_jobs.json
  +data/openai_enriched_jobs.json
  +data/openai_ranked_jobs.csv
  +data/jobs_enriched.json
  +
  +# Secrets
  +.env
  +
  +# Runtime state
  +data/state/
  +state/
  +
  +# Generated job data artifacts
  +data/openai_*_jobs.json
  +data/openai_ranked_*.json
  +data/openai_ranked_*.csv
  +data/openai_shortlist*.md
  +
  +# Optional: keep snapshots if you want reproducibility; otherwise ignore them too
  +# data/openai_snapshots/
  +
  +# Python junk
  +__pycache__/
  +*.pyc
  +.venv/
  +
  +# generated job data
  +data/openai_raw_jobs.json
  +data/openai_labeled_jobs.json
  +data/openai_enriched_jobs.json
  +data/openai_ranked_jobs*.json
  +data/openai_ranked_jobs*.csv
  +data/openai_ranked_families.json
  +data/openai_shortlist*.md
  +
  +# state
  +data/state/
  +
  +# local config overrides (if any)
  +config/*.local.*
  +
  +# local secrets
  +.env
  +
  +# allow template env file
  +!.env.example
  +
  +# ---- job-intelligence-engine generated outputs ----
  +data/*.json
  +data/*.csv
  +data/*.md
  +data/state/*.json
  +
  +# Keep snapshots if you want them versioned; comment these out if you want snapshots ignored too
  +# data/openai_snapshots/
  +
  +# packaging/build artifacts
  +*.egg-info/
  +
  +# Local AI cache
  +/data/ai_cache/
  +
  +# Local smoke outputs
  +smoke_artifacts/
  +
  +# Personal notes (local-only)
  +# Terraform local artifacts (aws infra)
  +ops/aws/infra/.terraform/
  +ops/aws/infra/terraform.tfstate*
  +ops/aws/infra/terraform.tfvars
  +ops/aws/infra/.terraform.lock.hcl
  +.terraform/
  +
  +# local caches
  +.cache/
  +
  +# Terraform (local)
  +ops/aws/infra/eks/.terraform/
  +ops/aws/infra/eks/*.tfstate*
  +ops/aws/infra/eks/.terraform.lock.hcl
  +ops/aws/infra/eks/local.auto.tfvars.json
  +
  +# Local proof artifacts (run receipts/logs)
  +ops/proof/
  +!ops/proof/PROOF_STORAGE.md
  +
  +# Local Python venvs
  +.venv*/
-- data/xai_snapshots/
diff --cc config/providers.json
index 86fc922,86fc922,0000000..4068b65
mode 100644,100644,000000..100644
--- a/config/providers.json
+++ b/config/providers.json
@@@@ -1,234 -1,234 -1,0 +1,309 @@@@
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "anthropic",
  +      "name": "Anthropic",
  +      "careers_urls": [
  +        "https://jobs.ashbyhq.com/anthropic"
  +      ],
  +      "allowed_domains": [
  +        "jobs.ashbyhq.com"
  +      ],
  +      "extraction_mode": "ashby",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/anthropic_snapshots",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "normal"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 3
  +      },
  +      "display_name": "Anthropic",
  +      "enabled": true
  +    },
  +    {
  +      "provider_id": "openai",
  +      "name": "OpenAI",
  +      "display_name": "OpenAI",
  +      "enabled": true,
  +      "careers_urls": [
  +        "https://jobs.ashbyhq.com/openai"
  +      ],
  +      "allowed_domains": [
  +        "jobs.ashbyhq.com"
  +      ],
  +      "extraction_mode": "ashby",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/openai_snapshots",
  +      "snapshot_path": "data/openai_snapshots/index.html",
  +      "live_enabled": true,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "high"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 3
  +      }
  +    },
  +    {
  +      "provider_id": "scaleai",
  +      "name": "Scale AI",
  +      "display_name": "Scale AI",
  +      "enabled": true,
  +      "careers_urls": [
  +        "https://jobs.ashbyhq.com/scaleai"
  +      ],
  +      "allowed_domains": [
  +        "jobs.ashbyhq.com"
  +      ],
  +      "extraction_mode": "ashby_api",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/scaleai_snapshots",
  +      "snapshot_path": "data/scaleai_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": "daily",
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 3
  +      }
  +    },
  +    {
  +      "provider_id": "replit",
  +      "name": "Replit",
  +      "display_name": "Replit",
  +      "enabled": true,
  +      "careers_urls": [
  +        "https://jobs.ashbyhq.com/replit"
  +      ],
  +      "allowed_domains": [
  +        "jobs.ashbyhq.com"
  +      ],
  +      "extraction_mode": "ashby_api",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/replit_snapshots",
  +      "snapshot_path": "data/replit_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": "daily",
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 3
  +      }
  +    },
  +    {
  +      "provider_id": "xai",
  +      "name": "xAI",
  +      "careers_urls": [
  +        "https://x.ai/careers"
  +      ],
  +      "allowed_domains": [
  +        "x.ai"
  +      ],
  +      "extraction_mode": "jsonld",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/xai_snapshots",
  +      "snapshot_path": "data/xai_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "normal"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 2
  +      },
  +      "display_name": "xAI",
  +      "enabled": true
  +    },
  +    {
  +      "provider_id": "cohere",
  +      "name": "Cohere",
  +      "careers_urls": [
  +        "https://cohere.com/careers"
  +      ],
  +      "allowed_domains": [
  +        "cohere.com"
  +      ],
  +      "extraction_mode": "jsonld",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/cohere_snapshots",
  +      "snapshot_path": "data/cohere_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "normal"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 2
  +      },
  +      "display_name": "Cohere",
  +      "enabled": true
  +    },
  +    {
  +      "provider_id": "huggingface",
  +      "name": "Hugging Face",
  +      "careers_urls": [
  +        "https://huggingface.co/jobs"
  +      ],
  +      "allowed_domains": [
  +        "huggingface.co"
  +      ],
  +      "extraction_mode": "jsonld",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/huggingface_snapshots",
  +      "snapshot_path": "data/huggingface_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "normal"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 2
  +      },
  +      "display_name": "Hugging Face",
  +      "enabled": true
  +    },
  +    {
  +      "provider_id": "mistral",
  +      "name": "Mistral",
  +      "careers_urls": [
  +        "https://mistral.ai/careers"
  +      ],
  +      "allowed_domains": [
  +        "mistral.ai"
  +      ],
  +      "extraction_mode": "jsonld",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/mistral_snapshots",
  +      "snapshot_path": "data/mistral_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "priority": "normal"
  +      },
  +      "politeness": {
  +        "min_delay_s": 1.0,
  +        "max_attempts": 2
  +      },
  +      "display_name": "Mistral",
  +      "enabled": true
  +    },
  +    {
  +      "provider_id": "perplexity",
  +      "name": "Perplexity",
  +      "careers_urls": [
  +        "https://www.perplexity.ai/careers"
  +      ],
  +      "allowed_domains": [
  +        "www.perplexity.ai"
  +      ],
  +      "extraction_mode": "jsonld",
  +      "mode": "snapshot",
  +      "snapshot_dir": "data/perplexity_snapshots",
  +      "snapshot_path": "data/perplexity_snapshots/index.html",
  +      "live_enabled": false,
  +      "update_cadence": {
  +        "min_interval_hours": 24,
  +        "max_staleness_hours": 72,
  +        "priority": "normal",
  +        "schedule_hint": "daily"
  +      },
  +      "politeness": {
  +        "defaults": {
  +          "max_qps": 1.0,
  +          "max_attempts": 2,
  +          "max_inflight_per_host": 1
  +        },
  +        "host_overrides": {
  +          "www.perplexity.ai": {
  +            "max_qps": 0.5,
  +            "max_inflight_per_host": 1
  +          }
  +        }
  +      },
  +      "display_name": "Perplexity",
  +      "enabled": true
+++    },
+++    {
+++      "provider_id": "airtable",
+++      "name": "Airtable",
+++      "display_name": "Airtable",
+++      "enabled": true,
+++      "careers_urls": [
+++        "https://www.airtable.com/careers"
+++      ],
+++      "allowed_domains": [
+++        "www.airtable.com"
+++      ],
+++      "extraction_mode": "jsonld",
+++      "mode": "snapshot",
+++      "snapshot_dir": "data/airtable_snapshots",
+++      "snapshot_path": "data/airtable_snapshots/index.html",
+++      "live_enabled": false,
+++      "update_cadence": {
+++        "min_interval_hours": 24,
+++        "priority": "normal"
+++      },
+++      "politeness": {
+++        "min_delay_s": 1.0,
+++        "max_attempts": 2
+++      }
+++    },
+++    {
+++      "provider_id": "canva",
+++      "name": "Canva",
+++      "display_name": "Canva",
+++      "enabled": true,
+++      "careers_urls": [
+++        "https://www.canva.com/careers"
+++      ],
+++      "allowed_domains": [
+++        "www.canva.com"
+++      ],
+++      "extraction_mode": "jsonld",
+++      "mode": "snapshot",
+++      "snapshot_dir": "data/canva_snapshots",
+++      "snapshot_path": "data/canva_snapshots/index.html",
+++      "live_enabled": false,
+++      "update_cadence": {
+++        "min_interval_hours": 24,
+++        "priority": "normal"
+++      },
+++      "politeness": {
+++        "min_delay_s": 1.0,
+++        "max_attempts": 2
+++      }
+++    },
+++    {
+++      "provider_id": "figma",
+++      "name": "Figma",
+++      "display_name": "Figma",
+++      "enabled": true,
+++      "careers_urls": [
+++        "https://www.figma.com/careers"
+++      ],
+++      "allowed_domains": [
+++        "www.figma.com"
+++      ],
+++      "extraction_mode": "jsonld",
+++      "mode": "snapshot",
+++      "snapshot_dir": "data/figma_snapshots",
+++      "snapshot_path": "data/figma_snapshots/index.html",
+++      "live_enabled": false,
+++      "update_cadence": {
+++        "min_interval_hours": 24,
+++        "priority": "normal"
+++      },
+++      "politeness": {
+++        "min_delay_s": 1.0,
+++        "max_attempts": 2
+++      }
  +    }
  +  ]
  +}
diff --cc docs/OPERATIONS.md
index fdf72a3,fdf72a3,0000000..4f732ff
mode 100644,100644,000000..100644
--- a/docs/OPERATIONS.md
+++ b/docs/OPERATIONS.md
@@@@ -1,579 -1,579 -1,0 +1,585 @@@@
  +# Operations
  +
  +## How to run
  +
  +Local (recommended for debugging):
  +
  +```bash
  +python scripts/run_daily.py --profiles cs --us_only --no_post --snapshot-only --offline
  +```
  +
  +Docker (build runs tests):
  +
  +```bash
  +docker build -t jobintel:local .
  +docker run --rm \
  +  -v "$PWD/data:/app/data" \
  +  -v "$PWD/state:/app/state" \
  +  --env-file .env \
  +  jobintel:local --profiles cs --us_only --no_post --snapshot-only --offline
  +```
  +
  +CI:
  +
  +```bash
  +make gate-ci
  +```
  +
  +CI smoke gate contract and failure-mode diagnostics:
  +- `docs/CI_SMOKE_GATE.md`
  +
  +Dependency lock workflow (source of truth):
  +
  +- `requirements.txt` is generated, not hand-edited.
  +- Lock-generation tooling contract is pinned for CI/repro runs via `make tooling-sync` (`pip==25.0.1`, `pip-tools==7.4.1`).
  +- Update lockfiles with `make deps-sync`.
  +- Enforce parity with `make deps-check` (used in CI).
  +- If runtime dependencies change, update `pyproject.toml` first, then re-run `make deps-sync`.
  +- Export policy is explicit: `pip-compile` is invoked with `--strip-extras` so lock output does not depend on
  +  pip-tools default changes.
  +- CI strict mode (`GITHUB_ACTIONS=true` / `JIE_DEPS_TARGET=ci`) is fail-closed: if `pip-compile` fails, `deps-check`
  +  fails (no fallback).
  +- If local `pip`/`pip-tools` are incompatible, export falls back to deterministic installed-env resolution; run
  +  `make tooling-sync` to return to the pinned toolchain.
  +- Determinism check (optional): run `make deps-sync` twice and confirm no diff in `requirements.txt`.
  +
  +Dashboard install/run (optional):
  +- Core install does not include dashboard runtime deps.
  +- Install dashboard runtime explicitly:
  +
  +```bash
  +pip install -e ".[dashboard]"
  +```
  +
  +- Run dashboard API:
  +
  +```bash
  +make dashboard
  +```
  +
  +If `fastapi`/`uvicorn` are missing, dashboard startup fails closed with a clear install command.
  +
  +Provider registry (Milestone 5 foundation):
  +
  +- Canonical file: `config/providers.json` (schema: `schemas/providers.schema.v1.json`)
  +- Registry loader: `src/ji_engine/providers/registry.py`
  +- Schema validation is enforced at load time (unknown keys fail closed).
  +- Provider selection resolver used by both:
  +  - `scripts/run_scrape.py`
  +  - `scripts/run_daily.py`
  +- `providers=all` resolves only providers with `enabled=true` (deterministic sort by `provider_id`).
+++- Provider registry hash + schema version are recorded in run report provenance under
+++  `provenance.build.provider_registry`.
  +- Supported extraction modes:
  +  - `ashby_api` (canonical config alias, normalized to runtime `ashby`)
  +  - `jsonld` (structured JobPosting JSON-LD parser)
  +  - `html_rules` (canonical config alias, normalized to runtime `html_list`)
  +  - `llm_fallback` (optional; requires `llm_fallback.enabled=true`)
  +  - Back-compat inputs (`ashby`, `snapshot_json`, `html_list`) remain accepted.
  +
  +How to add a provider (deterministic path):
  +
  +1. Add entry in `config/providers.json` under `providers[]`:
  +   - required: `provider_id`, `display_name`, `enabled`, `extraction_mode`, `careers_urls`
  +   - scrape mode + snapshots: `mode`, `snapshot_path` or `snapshot_dir`
  +   - capability flags: `live_enabled` (optional), `snapshot_enabled` (optional)
  +   - allowlist + cadence hints: `allowed_domains`, `update_cadence` (informational string or structured object)
  +   - politeness defaults/overrides:
  +     - `politeness.defaults` (provider-level defaults)
  +     - `politeness.host_overrides` (per-host override map)
  +     - back-compat flat keys still accepted (`min_delay_s`, `max_attempts`, etc.)
  +   - optional LLM fallback (cache-only, deterministic):
  +     - `llm_fallback.enabled=true` + `llm_fallback.cache_dir` (required)
  +     - `llm_fallback.temperature=0` (enforced)
  +2. Add/update snapshot fixture under `data/<provider_id>_snapshots/`.
+++   - CI enforces snapshot fixture existence and validation for all enabled snapshot providers.
+++   - Use `tombstone` + `enabled=false` to retire a provider without deleting history:
+++     - `tombstone.reason` required
+++     - optional metadata: `ticket`, `replaced_by`, `removed_at`
  +3. Run scrape in snapshot mode:
  +
  +```bash
  +python scripts/run_scrape.py --providers <provider_id> --providers-config config/providers.json --mode SNAPSHOT
  +```
  +
  +4. Verify deterministic output:
  +   - `data/ashby_cache/<provider_id>_raw_jobs.json`
  +   - `data/ashby_cache/<provider_id>_scrape_meta.json`
  +
  +Proof-of-design provider in-tree:
  +- `perplexity` (`jsonld`, snapshot-first)
  +- Snapshot fixture: `data/perplexity_snapshots/index.html`
  +
  +Snapshot validation semantics (no footguns):
  +- `jobintel snapshots validate --provider <id>` validates only the requested providers.
  +- `jobintel snapshots validate --all` validates only providers with snapshots present on disk,
  +  and skips missing snapshot paths to avoid false failures.
  +
  +Kubernetes CronJob (portable, K8s-first):
  +
  +Use the kustomize base at `ops/k8s/jobintel` or the AWS EKS overlay at
  +`ops/k8s/overlays/aws-eks` for IRSA + publish toggles. See `ops/k8s/README.md`
  +for the runtime contract and apply steps.
  +
  +Deterministic gate (recommended):
  +
  +```bash
  +make gate-truth
  +```
  +
  +Fast local/PR gate (no Docker):
  +
  +```bash
  +make gate-fast
  +```
  +
  +Roadmap discipline guard:
  +- CI now runs a warn-only guard in `ci.yml`:
  +  - `.venv/bin/python scripts/check_roadmap_discipline.py`
  +- Local checks:
  +
  +```bash
  +python scripts/check_roadmap_discipline.py
  +python scripts/check_roadmap_discipline.py --strict
  +```
  +
  +Snapshot-only violations fail fast with exit code 2 and a message naming the provider.
  +
  +## Add Provider Checklist (M5)
  +
  +1. **Config entry (required):**
  +   - `provider_id`, `extraction_mode`, and one of `careers_urls`/`careers_url`/`board_url`.
  +   - `snapshot_path` or `snapshot_dir` (defaults to `data/<provider_id>_snapshots/index.html`).
  +   - `snapshot_enabled: true` (default) unless intentionally disabled.
  +   - `allowed_domains` + `update_cadence` (optional but recommended).
  +2. **Fixture requirements (deterministic):**
  +   - Commit `data/<provider_id>_snapshots/index.html` (or JSON snapshot for `snapshot_json`).
  +   - Add a test fixture under `tests/fixtures/providers/<provider_id>/index.html`.
  +3. **Validation commands (offline):**
  +   - `python -m src.jobintel.cli snapshots validate --provider <id>`
  +   - `python -m src.jobintel.cli snapshots validate --all` (skips missing snapshot dirs)
  +4. **Tests to update/add:**
  +   - Registry schema rejection test (missing fields / unknown keys).
  +   - JSON-LD parsing test (stable ordering + stable `job_id` across runs).
  +   - Snapshot validation selection/skip semantics.
  +5. **Debug schema failures:**
  +   - Errors are fail-closed and name the missing field or bad key.
  +   - Check `schemas/providers.schema.v1.json` and `src/ji_engine/providers/registry.py`.
  +6. **LLM fallback (cache-only, disabled by default):**
  +   - `llm_fallback.enabled` is `false` unless explicitly configured.
  +   - If enabled, requires `llm_fallback.cache_dir` and `temperature=0`.
  +
  +## Provider failure policy
  +
  +Live scraping is guarded by deterministic, fail-closed thresholds:
  +- Transient or unavailable error rate above `JOBINTEL_PROVIDER_ERROR_RATE_MAX` (default `0.25`) fails the run.
  +- Parsed job count below `JOBINTEL_PROVIDER_MIN_JOBS` (default `1`) in live mode fails the run.
  +- Parsed job count below `JOBINTEL_PROVIDER_MIN_SNAPSHOT_RATIO` (default `0.2`) of the snapshot baseline fails the run
  +  (when a baseline count is available).
  +
  +These outcomes are recorded under `provenance_by_provider[*].failure_policy` in the run report and surfaced in Discord
  +run summaries (when enabled).
  +
  +## Robots / policy handling
  +
  +Live scraping enforces a robots/policy decision before any network fetch:
  +
  +- Allowlist: `JOBINTEL_LIVE_ALLOWLIST_DOMAINS` (comma-separated) or provider-specific
  +  `JOBINTEL_LIVE_ALLOWLIST_DOMAINS_<PROVIDER>` controls which hosts are permitted for live fetches.
  +  If the allowlist is set and a host is not listed, live scraping is skipped and the run falls back to snapshots.
  +- Robots: the runner fetches `https://<host>/robots.txt` and evaluates `User-agent` rules using a consistent
  +  `JOBINTEL_USER_AGENT` (default: `signalcraft-bot/1.0 (+https://github.com/penquinspecz/SignalCraft)`).
  +  Disallow or fetch failures are treated conservatively (live skipped → snapshot fallback).
  +
  +Every decision is logged as `[provider_retry][robots] ...` and recorded in provenance:
  +`robots_fetched`, `robots_allowed`, `allowlist_allowed`, `robots_final_allowed`, `robots_reason`, `robots_url`.
  +
  +To override for dev/test, set:
  +
  +```bash
  +export JOBINTEL_LIVE_ALLOWLIST_DOMAINS="jobs.ashbyhq.com"
  +```
  +
  +## Discord diff gating
  +
  +Discord run summaries are diff-gated by default:
  +- Post only when diffs exist (new/changed/removed) or the run fails.
  +- Override with `JOBINTEL_DISCORD_ALWAYS_POST=1` to always post a summary.
  +- `--no_post` still suppresses posting entirely.
  +- Summaries use identity-based deltas (`job_id` first, provider identity fallback) and include top new/changed items.
  +- Tune summary detail with `JOBINTEL_DISCORD_DIFF_TOP_N` (default `5`).
  +
  +## Redaction enforcement
  +
  +Secret scanning runs on proof bundles by default (fail-closed unless `--allow-secrets` is passed to the proof wrapper).
  +
  +Run report and diff artifact writes support opt-in fail-closed mode:
  +
  +```bash
  +export REDACTION_ENFORCE=1
  +python scripts/run_daily.py --profiles cs --us_only --no_post --snapshot-only --offline
  +```
  +
  +With `REDACTION_ENFORCE=1`, secret-like patterns in generated JSON/markdown artifacts raise an error instead of only warning.
  +
  +## Identity diff artifacts
  +
  +Each run writes deterministic identity delta artifacts under `state/runs/<run_id>/`:
  +
  +- `diff.json`: provider/profile diff payload with `added`, `changed`, `removed` buckets.
  +- `diff.md`: human-readable summary from the same payload.
  +
  +Identity semantics:
  +- `new`: `job_id` not present in the prior run.
  +- `changed`: same identity, but one or more tracked fields changed:
  +  `title`, `location`, `team`, `level`, `score`, `jd_hash`.
  +- `removed`: identity present in prior run but not current run.
  +
  +## Input selection rules
  +
  +Scoring input resolution is handled by `scripts/run_daily.py`:
  +
  +- Default (no flags): requires `data/openai_enriched_jobs.json`.
  +- `--no_enrich`: uses `data/openai_enriched_jobs.json` only if it exists and is newer than `data/openai_labeled_jobs.json`; otherwise falls back to labeled.
  +- `--ai`: runs AI augment and adds `--prefer_ai` when scoring, but still follows the same input selection as above.
  +- `--ai_only`: requires `data/openai_enriched_jobs_ai.json` and fails if missing.
  +- `--prefer_ai`: passed to `score_jobs.py` only when `--ai` or `--ai_only` is set by `run_daily.py`.
  +
  +## Semantic Safety Net (M7, bounded and deterministic)
  +
  +Semantic is deterministic and runs in one of two modes controlled by `SEMANTIC_MODE`:
  +- `sidecar` (default when `SEMANTIC_ENABLED=1`): compute semantic evidence only; does not mutate ranked outputs.
  +- `boost`: apply bounded semantic boost to scoring output.
  +
  +When `SEMANTIC_MODE=boost`, final score contract is:
  +- `final_score = clamp(base_score + semantic_boost, 0, 100)`
  +- `semantic_boost` is bounded in `[0, SEMANTIC_MAX_BOOST]`
  +- If semantic is disabled/unavailable, or mode is `sidecar`, ranked outputs must match pre-semantic behavior exactly.
  +
  +Environment flags:
  +- `SEMANTIC_ENABLED=1` enables semantic processing (default off).
  +- `SEMANTIC_MODE` selects semantic behavior: `sidecar` or `boost` (default `sidecar`).
  +- `SEMANTIC_MODEL_ID` selects model id (default `deterministic-hash-v1`).
  +- `SEMANTIC_MAX_JOBS` bounds per-run embedding workload (default `200`).
  +- `SEMANTIC_TOP_K` only evaluates semantic similarity for the top-K base-scored jobs (default `50`).
  +- `SEMANTIC_MAX_BOOST` caps semantic contribution per job (default `5`).
  +- `SEMANTIC_MIN_SIMILARITY` minimum rounded similarity required for non-zero boost (default `0.72`).
  +
  +Determinism contract:
  +- Text normalization is deterministic (`semantic_norm_v1`) before embedding/hash.
  +- Default backend is offline and deterministic (hash-based vectors, no network calls).
  +- Similarity is rounded to 6 decimals before threshold checks and boost math.
  +- Cache keys include `job_id`, `job_content_hash`, `candidate_profile_hash`, and `semantic_norm_v1`.
  +- Cache location: `state/embeddings/<model_id>/<cache_key>.json`.
  +- Cache entries store only model id, deterministic metadata, input hashes, and vector values.
  +
  +Run artifact:
  +- Every run writes `state/runs/<run_id-sanitized>/semantic/semantic_summary.json` even when disabled.
  +- Every run writes `state/runs/<run_id-sanitized>/semantic/semantic_scores.json` (possibly empty when disabled).
  +- Short-circuit behavior:
  +  - `SEMANTIC_ENABLED=1` + `SEMANTIC_MODE=sidecar`: keep short-circuit when ranked outputs are fresh, and write semantic artifacts from existing ranked JSON.
  +  - `SEMANTIC_ENABLED=1` + `SEMANTIC_MODE=boost`: bypass short-circuit and rerun deterministic scoring so bounded boost can be applied.
  +- Summary includes:
  +  - `enabled`
  +  - `model_id`
  +  - `policy` (`mode`, `max_jobs`, `top_k`, `max_boost`, `min_similarity`)
  +  - `used_short_circuit`
  +  - `attempted_provider_profiles`
  +  - `cache_hit_counts`
  +  - `embedded_job_count`
  +  - `skipped_reason` (when disabled/unavailable/fail-closed)
  +- Scores artifact includes:
  +  - `job_id`
  +  - `base_score`
  +  - `similarity` (rounded, optional when not evaluated)
  +  - `semantic_boost`
  +  - `final_score`
  +  - `reasons` (for threshold/boost decisions)
  +
  +Privacy boundary:
  +- Semantic artifacts do not store raw job description text.
  +- Only hashes, job ids, cache keys, and minimal provider/profile metadata are persisted.
  +
  +Debug checklist:
  +- Confirm semantic config in env (`SEMANTIC_ENABLED`, `SEMANTIC_MODE`, `SEMANTIC_MODEL_ID`, `SEMANTIC_TOP_K`, `SEMANTIC_MAX_BOOST`, `SEMANTIC_MIN_SIMILARITY`).
  +- Inspect `state/runs/<run_id-sanitized>/semantic/semantic_summary.json` for `skipped_reason` and cache counters.
  +- Inspect `state/runs/<run_id-sanitized>/semantic/semantic_scores.json` for per-job similarity/boost decisions.
  +- Compare ranked output hashes with `SEMANTIC_ENABLED=0` or `SEMANTIC_MODE=sidecar` when validating no-rescore parity behavior.
  +
  +## Artifacts and where they live
  +
  +Data outputs (`./data`):
  +- `openai_raw_jobs.json`
  +- `<provider_id>_raw_jobs.json` (per-provider raw output)
  +- `openai_labeled_jobs.json`
  +- `openai_enriched_jobs.json`
  +- `openai_enriched_jobs_ai.json` (if AI augment ran)
  +- `openai_ranked_jobs.<profile>.json`
  +- `openai_ranked_jobs.<profile>.csv`
  +- `openai_ranked_families.<profile>.json`
  +- `openai_shortlist.<profile>.md`
  +
  +State (`./state`):
  +- `history/` per-run archived artifacts by profile
  +- `history/<profile>/runs/<run_id>/pointer.json` canonical run pointer to `state/runs/<run_id-sanitized>/`
  +- `history/<profile>/runs/<run_id>/identity_map.json` deterministic per-run identity map (compact `job_id` keyed view)
  +- `history/<profile>/runs/<run_id>/provenance.json` deterministic per-run scrape/provenance summary for that profile
  +- `history/<profile>/daily/<YYYY-MM-DD>/pointer.json` canonical day pointer to selected run_id
  +- `history/<profile>/retention.json` active retention settings for profile pointer pruning
  +- `runs/` run metadata JSON
  +- `last_run.json` last run telemetry snapshot
  +- `user_state/` reserved for user-scoped state files
  +
  +Weekly insights inputs (deterministic):
  +- `state/runs/<run_id-sanitized>/ai/insights_input.<profile>.json`
  +- Built before weekly AI insights generation from deterministic artifacts only:
  +  - diffs (`new/changed/removed` counts + top titles)
  +  - 7-run rolling diff counts (`rolling_diff_counts_7`)
  +  - top families
  +  - score bucket distribution
  +  - deterministic top recurring skill tokens (`top_recurring_skill_tokens`)
  +  - median score trend delta (`median_score_trend_delta`)
  +- Prompt contract version: `weekly_insights_v3`.
  +- Cache key includes structured input hash + prompt version/hash (deterministic).
  +- No raw JD text is written into `insights_input.<profile>.json`; payload is summary-only.
  +
  +## AI + Embedding Budget Guardrails
  +
  +Deterministic per-run cost artifact:
  +- `state/runs/<run_id>/costs.json`
  +- fields:
  +  - `embeddings_count`
  +  - `embeddings_estimated_tokens`
  +  - `ai_calls`
  +  - `ai_estimated_tokens`
  +  - `total_estimated_tokens`
  +
  +Guardrail env vars:
  +- `MAX_AI_TOKENS_PER_RUN` (default `0`, disabled)
  +- `MAX_EMBEDDINGS_PER_RUN` (default `0`, disabled)
  +
  +Fail-closed behavior:
  +- If `ai_estimated_tokens > MAX_AI_TOKENS_PER_RUN`, run fails closed with validation-style exit code `2`.
  +- If `embeddings_count > MAX_EMBEDDINGS_PER_RUN`, run fails closed with validation-style exit code `2`.
  +- Failure is recorded in run report with `failed_stage=cost_guardrails`.
  +
  +Notes:
  +- Estimation is deterministic and local only; no billing/provider APIs are called.
  +- AI model selection and temperature are unchanged by budget controls.
  +
  +Run reports:
  +- `state/runs/<run_id>.json` (run metadata)
  +- Includes `run_report_schema_version`, inputs, outputs, scoring inputs, and selection reasons per profile.
  +
  +Run ID in logs:
  +- Every run prints a machine-parseable line early: `JOBINTEL_RUN_ID=<run_id>`.
  +- Use this as the canonical run_id for orchestrators and log parsers.
  +
  +Success pointer:
  +- `state/last_success.json` is updated only on successful runs.
  +- It includes `run_id`, completion timestamp, provider/profile summaries, and key artifact hashes.
  +
  +History retention controls:
  +- Disabled by default (safe): `HISTORY_ENABLED=0` unless explicitly set.
  +- Enable deterministic pointer retention: `HISTORY_ENABLED=1`
  +- Tune limits:
  +  - `HISTORY_KEEP_RUNS` (default `30`)
  +  - `HISTORY_KEEP_DAYS` (default `90`)
  +- CLI overrides:
  +  - `--history-enabled`
  +  - `--history-keep-runs <N>`
  +  - `--history-keep-days <D>`
  +
  +Machine-parseable history logs:
  +- `HISTORY_RETENTION enabled=<0|1> ... run_id=<run_id>`
  +- Per profile on success:
  +  - `HISTORY_RETENTION profile=<profile> run_id=<run_id> enabled=1 keep_runs=<N> keep_days=<D> runs_kept=<n> runs_pruned=<n> daily_kept=<n> daily_pruned=<n> identity_count=<n> identity_map=<path> provenance=<path> run_pointer=<path> daily_pointer=<path>`
  +- Log retention summary:
  +  - `LOG_RETENTION enabled=<0|1> keep_runs=<N> runs_seen=<n> runs_kept=<n> log_dirs_pruned=<n> run_id=<run_id>`
  +  - retention is logs-only under `state/runs/<run_id>/logs/`; run artifacts are not deleted.
  +
  +Inspect history identity/provenance artifacts:
  +
  +```bash
  +cat state/history/<profile>/runs/<run_id>/identity_map.json
  +cat state/history/<profile>/runs/<run_id>/provenance.json
  +```
  +
  +## Observability contract (lean)
  +
  +Stdout remains canonical. Optional structured log sink can also write per-run JSONL logs:
  +
  +```bash
  +python scripts/run_daily.py --profiles cs --log_file
  +# or
  +JOBINTEL_LOG_FILE=1 python scripts/run_daily.py --profiles cs
  +```
  +
  +Per-run log pointers are written into `run_report.json` under `logs`.
  +The pointer contract is best-effort and cloud-agnostic:
  +- `logs.local`: local run/log paths plus optional structured JSONL sink path
  +- `logs.k8s`: kubectl command templates to locate pod/job logs by `JOBINTEL_RUN_ID`
  +- `logs.cloud`: CloudWatch group/stream (when env is set) plus a run-id filter pattern
  +
  +Find logs by run_id locally:
  +
  +```bash
  +run_id=<run_id>
  +safe_id=$(echo "$run_id" | tr -d ':-.')
  +cat "state/runs/$safe_id/run_report.json" | jq '.logs'
  +cat "state/runs/$safe_id/logs/run.log.jsonl"   # if log sink enabled
  +```
  +
  +Find logs in k3s:
  +
  +```bash
  +run_id=<run_id>
  +safe_id=$(echo "$run_id" | tr -d ':-.')
  +cat "state/runs/$safe_id/run_report.json" | jq '.logs.k8s'
  +kubectl -n jobintel get pods --sort-by=.metadata.creationTimestamp
  +kubectl -n jobintel get jobs --sort-by=.metadata.creationTimestamp
  +kubectl -n jobintel logs <pod-or-job> | rg "JOBINTEL_RUN_ID=$run_id"
  +```
  +
  +Find logs in AWS (if `logs.cloud` pointers are populated in run report):
  +
  +```bash
  +run_id=<run_id>
  +safe_id=$(echo "$run_id" | tr -d ':-.')
  +group=$(jq -r '.logs.cloud.cloudwatch_log_group // empty' "state/runs/$safe_id/run_report.json")
  +stream=$(jq -r '.logs.cloud.cloudwatch_log_stream // empty' "state/runs/$safe_id/run_report.json")
  +region=$(jq -r '.logs.cloud.region // empty' "state/runs/$safe_id/run_report.json")
  +pattern=$(jq -r '.logs.cloud.cloudwatch_filter_pattern // empty' "state/runs/$safe_id/run_report.json")
  +aws logs filter-log-events --region "$region" --log-group-name "$group" --filter-pattern "$pattern"
  +aws logs get-log-events --region "$region" --log-group-name "$group" --log-stream-name "$stream"
  +```
  +
  +## Replayability and verification
  +
  +Replay a prior run (strict verification):
  +
  +```bash
  +make replay RUN_ID=<run_id>
  +```
  +
  +Or directly:
  +
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --profile cs --strict
  +```
  +
  +Replay with recompute (archived inputs → regenerated outputs):
  +
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --profile cs --strict --recalc
  +```
  +
  +Snapshot immutability check:
  +
  +```bash
  +make verify-snapshots
  +```
  +
  +Local replay gate (offline-safe):
  +
  +```bash
  +make gate-replay
  +```
  +
  +Exit codes:
  +- `0`: all checked artifacts match
  +- `2`: missing artifacts or mismatched hashes
  +- `>=3`: unexpected runtime errors
  +
  +Replay JSON output:
  +
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --json
  +```
  +
  +## Common failure modes and debugging
  +
  +Exit codes:
  +- `0` success (including short-circuit runs)
  +- `2` validation/missing required inputs
  +- `>=3` runtime/provider failures (including subprocess stage failures)
  +
  +Typical issues:
  +- Missing snapshot: ensure `data/openai_snapshots/index.html` exists.
  +- Missing input files: check `data/openai_labeled_jobs.json` and/or `data/openai_enriched_jobs.json` based on flags.
  +- AI-only missing: `--ai_only` requires `data/openai_enriched_jobs_ai.json`.
  +- Permission errors after Docker runs: fix ownership on `data/` and `state/` if needed.
  +- US-only filter removes all jobs: usually indicates missing/unnormalized locations; verify enrichment inputs.
  +
  +Debug tips:
  +- Use `JOBINTEL_TEST_DEBUG_PATHS=1` to print temp paths in tests.
  +- Inspect `state/runs/*.json` for the inputs/outputs and hash provenance for a run.
  +- Providers are configured in `config/providers.json`; run `scripts/run_scrape.py --providers openai,anthropic` to scrape multiple providers.
  +
  +## Docker daemon troubleshooting
  +
  +If Docker commands fail with daemon `_ping` errors (e.g., HTTP 500), the daemon is unhealthy. Try:
  +- Restart Docker Desktop or the Docker daemon.
  +- Run `docker info` to confirm the daemon is reachable.
  +- Re-run `docker build` and the smoke command after the daemon recovers.
  +
  +## User state
  +
  +User state lives under `state/user_state/<profile>.json` and uses schema:
  +
  +```json
  +{
  +  "schema_version": 1,
  +  "jobs": {
  +    "<job_id>": {
  +      "status": "ignore|saved|applied|interviewing",
  +      "date": "YYYY-MM-DD",
  +      "notes": "optional"
  +    }
  +  }
  +}
  +```
  +
  +Semantics:
  +- `ignore`: suppress from shortlist and suppress from diff/Discord notifications.
  +- `applied` / `interviewing`: keep in shortlist with annotation, suppress from `new` notifications.
  +- `saved`: keep in shortlist with annotation; notifications allowed.
  +
  +Invalid user-state JSON/schema is fail-closed for overlays (pipeline continues, warning is logged).
  +
  +Examples:
  +
  +```bash
  +python scripts/user_state.py add-status --profile cs --job-id job_123 --status applied --notes "Reached out."
  +python scripts/user_state.py add-status --profile cs --url https://example.com/jobs/123 --status ignore
  +python scripts/user_state.py list --profile cs
  +python scripts/user_state.py export --profile cs --out /tmp/user_state.cs.json
  +```
  +
  +A “Quality Gates” section:
  +
  +Developer default: `make gate` (alias for `gate-fast`).
  +
  +Source-of-truth gate: `make gate-truth` (includes Docker no-cache).
  +
  +CI uses `make gate-ci` (alias for `gate-truth`).
  +
  +Order matters: pytest → snapshot immutability → replay smoke → Docker (truth gate only).
  +
  +Snapshot behavior:
  +
  +Providers may run in live or snapshot mode
  +
  +Snapshots are first-class artifacts with:
  +
  +atomic writes
  +
  +sha256 verification
  +
  +sidecar metadata
  +
  +Determinism guarantees:
  +
  +Snapshot + provider config + profile ⇒ deterministic output
  +
  +Golden E2E tests enforce this
diff --cc docs/ROADMAP.md
index 2ede32e,2ede32e,0000000..96f4f34
mode 100644,100644,000000..100644
--- a/docs/ROADMAP.md
+++ b/docs/ROADMAP.md
@@@@ -1,410 -1,410 -1,0 +1,480 @@@@
  +© 2026 Chris Menendez. Source Available — All Rights Reserved.  
  +See LICENSE for permitted use.
  +
  +# SignalCraft Roadmap
  +
  +This roadmap is the anti-chaos anchor.  
  +We optimize for:
  +
  +1) Deterministic outputs  
  +2) Debuggability  
  +3) Deployability  
  +4) Incremental intelligence  
  +5) Productization without chaos  
  +6) Infrastructure portability  
  +
  +If a change doesn’t advance a milestone’s Definition of Done (DoD), it’s churn.
  +
  +---
  +
  +# Document Contract
  +
  +This file is the plan. The repo is the truth.
  +
  +Every merged PR must:
  +- Declare which milestone moved
  +- Include evidence paths (tests, logs, proof bundles)
  +- Keep “Current State” aligned with actual behavior
  +
  +---
  +
  +# Non-Negotiable Guardrails
  +
  +- One canonical pipeline entrypoint (`scripts/run_daily.py`)
  +- Determinism > cleverness
  +- Explicit input selection reasoning
  +- Small, test-backed changes
  +- Operational truth lives in artifacts
  +- AI is last-mile only
  +- No credentialed scraping
  +- Legal constraints enforced in design
  +- CI must prove determinism offline
  +- Cloud runs must be replayable locally
  +- Milestone completion requires receipts
  +
  +---
  +
  +# Legal + Ethical Operation Contract
  +
  +SignalCraft is a discovery and alerting net, not a job board replacement.
  +
  +Hard rules:
  +
  +- Canonical outbound links always included
  +- UI-safe artifacts never replace original job pages
  +- Robots and policy decisions logged in provenance
  +- Per-host rate limits enforced
  +- Opt-out supported via provider tombstone
  +- Honest, stable User-Agent
  +- No paywall bypass or login scraping
  +
  +Evidence expectations:
  +
  +- Provenance includes scrape_mode + policy decision
  +- Provider availability reasons surfaced explicitly
  +
  +---
  +
  +# Current State
  +
  +Last verified: 2026-02-13T01:00:59Z @ 555b095292109864c3016a52084e78e6616bd9d6  
  +Latest release: v0.1.0
  +
  +Foundation exists:
  +- Deterministic scoring
  +- Replayability
  +- Snapshot-backed providers
  +- AI weekly insights (guardrailed)
  +- Per-job briefs
  +- Cost guardrails
  +- Discord alerts
  +- Minimal dashboard API
  +- CI smoke enforcement
  +
  +Phase 1 is real.
  +
  +---
  +
  +# NEW ROADMAP — Thick Milestones
  +
  +---
  +
  +## Milestone 10 — Provider Platform v1
  +
  +Goal: Provider expansion becomes boring.
  +
  +Definition of Done
  +
  +- [ ] Versioned provider registry schema exists
  +- [ ] Registry hash recorded in provenance
  +- [ ] Provider config validated in CI
  +- [ ] Snapshot fixtures enforced per provider
  +- [ ] Provider tombstone supported
  +- [ ] At least 3 new providers added via registry only
  +- [ ] No core pipeline modification required to add provider
  +
  +Receipts Required
  +
  +- Deterministic ordering tests
  +- Snapshot completeness enforcement
  +- Proof doc in docs/proof/
  +
  +---
  +
  +## Milestone 11 — Artifact Model v2
  +
  +Goal: Legality + replayability enforced by shape.
  +
  +Definition of Done
  +
  +- [ ] UI-safe artifact schema versioned
  +- [ ] Replay-safe artifact schema versioned
  +- [ ] UI-safe artifacts contain no raw JD text
  +- [ ] Redaction boundaries enforced by tests
  +- [ ] Retention policy documented
  +- [ ] Artifact backward compatibility defined
  +
  +Receipts Required
  +
  +- Schema validation suite
  +- Artifact redaction tests
  +- Proof doc
  +
  +---
  +
  +## Milestone 12 — Operations Hardening Pack
  +
  +Goal: Failure is explicit and inspectable.
  +
  +Definition of Done
  +
  +- [ ] failed_stage always populated
  +- [ ] Cost telemetry always written
  +- [ ] Provider availability artifact generated
  +- [ ] One-command run inspection tooling
  +- [ ] CI smoke matches real run structure
  +- [ ] Failure playbook updated
  +
  +Receipts Required
  +
  +- Forced failure proof run
  +- Artifact inspection proof
  +
  +---
  +
  +## Milestone 13 — AI Insights v1 (Grounded Intelligence)
  +
  +Goal: Weekly insights are useful and bounded.
  +
  +Definition of Done
  +
  +- [ ] Deterministic input schema versioned
  +- [ ] 7/14/30 day trend analysis
  +- [ ] Skill token extraction from structured fields
  +- [ ] Strict output schema enforcement
  +- [ ] Cache keyed by input hash + prompt version
  +- [ ] “Top 5 Actions” section included
  +- [ ] No raw JD leakage
  +
  +Receipts Required
  +
  +- Two-run determinism proof
  +- Schema validation tests
  +
  +---
  +
  +## Milestone 14 — AI Per-Job Briefs v1
  +
  +Goal: Profile-aware coaching per job.
  +
  +Definition of Done
  +
  +- [ ] Candidate profile hash contract defined
  +- [ ] ai_job_brief.schema.json enforced
  +- [ ] Cache keyed by job_hash + profile_hash
  +- [ ] Cost accounting integrated
  +- [ ] Deterministic hash stability verified
  +- [ ] Schema validation enforced in CI
  +
  +Receipts Required
  +
  +- Deterministic diff proof
  +- Cost artifact proof
  +
  +---
  +
  +## Milestone 15 — Explainability v1
  +
  +Goal: Scores are interpretable.
  +
  +Definition of Done
  +
  +- [ ] explanation_v1 structure implemented
  +- [ ] Top contributing signals surfaced
  +- [ ] Penalties visible
  +- [ ] Semantic contribution bounded + surfaced
  +- [ ] Deterministic ordering enforced
  +
  +Receipts Required
  +
  +- Artifact snapshot proof
  +- Ordering tests
  +
  +---
  +
  +## Milestone 16 — Dashboard Plumbing v2
  +
  +Goal: Backend-first UI readiness.
  +
  +Definition of Done
  +
  +- [ ] /version endpoint
  +- [ ] /runs/latest endpoint
  +- [ ] Artifact index endpoint
  +- [ ] API contract documented
  +- [ ] Optional deps isolated cleanly
  +
  +Receipts Required
  +
  +- API proof doc
  +- Simulated UI proof
  +
  +---
  +
  +## Milestone 17 — Release Discipline v1
  +
  +Goal: Releases are proof events.
  +
  +Definition of Done
  +
  +- [ ] Release checklist codified
  +- [ ] Preflight validation script exists
  +- [ ] Changelog enforcement policy
  +- [ ] Every release includes proof bundle
  +- [ ] Reproducible build instructions verified
  +
  +---
  +
  +# INFRASTRUCTURE EVOLUTION
  +
  +---
  +
  +## Milestone 18 — AWS DR & Failover Hardening
  +
  +Goal: Cloud execution survives failure.
  +
  +Definition of Done
  +
  +- [ ] S3 versioning enabled
  +- [ ] S3 lifecycle policy defined
  +- [ ] Backup bucket replication strategy documented
  +- [ ] Disaster recovery restore rehearsal executed
  +- [ ] RTO + RPO explicitly defined
  +- [ ] Infrastructure config versioned
  +- [ ] Recovery playbook tested
  +
  +Receipts Required
  +
  +- Restore rehearsal proof
  +- Recovery time measurement
  +- Backup verification artifact
  +
  +---
  +
  +## Milestone 19 — AWS → On-Prem Migration Contract
  +
  +Goal: Migration is engineered, not improvised.
  +
  +Definition of Done
  +
  +- [ ] Data migration plan documented
  +- [ ] Artifact compatibility verified
  +- [ ] Backwards compatibility test suite passes
  +- [ ] Rollback plan documented
  +- [ ] Dual-run validation (AWS vs on-prem output diff)
  +- [ ] Zero artifact schema changes required
  +- [ ] Migration dry run executed
  +
  +Receipts Required
  +
  +- Side-by-side artifact diff proof
  +- Migration dry run log
  +- Rollback rehearsal doc
  +
  +---
  +
  +## Milestone 20 — On-Prem Stability Proof (Post-Migration)
  +
  +Goal: On-prem becomes primary without chaos.
  +
  +Definition of Done
  +
  +- [ ] 72-hour continuous k3s run
  +- [ ] CronJob stability verified
  +- [ ] Storage durability verified
  +- [ ] Backup + restore rehearsal on-prem
  +- [ ] Resource utilization captured
  +- [ ] Determinism validated against AWS baseline
  +
  +Receipts Required
  +
  +- Stability logs
  +- Restore proof
  +- Deterministic diff proof
  +
  +---
  +
  +# GOVERNANCE & HYGIENE
  +
  +---
  +
  +## Milestone 21 — Security Review Pack v1
  +
  +Goal: Security posture is audited, not assumed.
  +
  +Definition of Done
  +
  +- [ ] Threat model document created
  +- [ ] Attack surface review performed
  +- [ ] Secrets handling reviewed
  +- [ ] Dependency audit completed
  +- [ ] Least-privilege IAM documented
  +- [ ] Static analysis tool integrated
  +- [ ] Security.md aligned with reality
  +
  +Receipts Required
  +
  +- Threat model artifact
  +- Dependency audit report
  +- IAM review checklist
  +
  +---
  +
  +## Milestone 22 — Code Surface & Bloat Review
  +
  +Goal: Eliminate entropy.
  +
  +Definition of Done
  +
  +- [ ] Dead code removed
  +- [ ] Unused deps removed
  +- [ ] Duplicate logic consolidated
  +- [ ] File structure rationalized
  +- [ ] Public API boundaries clarified
  +- [ ] Complexity hotspots documented
  +- [ ] Size diff documented
  +
  +Receipts Required
  +
  +- Before/after LOC diff
  +- Dependency tree comparison
  +- Simplification proof doc
  +
  +---
  +
  +## Milestone 23 — Multi-User Plumbing (Foundation Only)
  +
  +Goal: Prepare for product without UI.
  +
  +Definition of Done
  +
  +- [ ] candidate_profile.schema.json defined
  +- [ ] candidate_id integrated in registry
  +- [ ] Artifact path namespaced by candidate
  +- [ ] Cross-user leakage tests implemented
  +- [ ] Backward compatibility maintained
  +- [ ] No UI implemented
  +
  +Receipts Required
  +
  +- Isolation test suite
  +- Artifact namespace proof
  +
  +---
  +
  +# Phase 3 Preview (24–30)
  +
  +- Authentication + authz
  +- Resume ingestion
  +- AI coaching expansion
  +- AI outreach generation
  +- Advanced analytics
  +- Provider scaling
  +- Cost optimization
  +- Architecture pruning
  +- Partnership-ready ingestion
  +- Production UI
  +
  +---
  +
  +# Milestone Philosophy
  +
  +Fewer, thicker milestones.
  +
  +Every milestone must:
  +- Produce artifacts
  +- Produce tests
  +- Produce receipts
  +- Reduce chaos
  +- Increase product clarity
  +- Increase infrastructure resilience
+++
+++---
+++
+++## Archive — Milestones 1–9 (Completed)
+++
+++This archive is retained for historical continuity; the active roadmap above remains canonical. These milestones are completed and superseded by the current structure and PR receipts.
+++
+++# ARCHIVE — Milestones 1–9 (Completed / Superseded)
+++
+++**Archive rule:** These milestones are “done enough” for Phase 1.  
+++Do not reopen unless a regression threatens determinism, replayability, or deployability.
+++
+++## Milestone 1 — Daily run deterministic & debuggable (Local + Docker + CI) ✅
+++**Receipts:** see `docs/OPERATIONS.md`, CI smoke contracts, snapshot helpers.
+++- [x] `pytest -q` passes locally/CI
+++- [x] Docker smoke produces ranked outputs + run report
+++- [x] Exit codes normalized
+++- [x] Snapshot debugging helpers (`make debug-snapshots`)
+++- [x] CI deterministic artifact validation
+++
+++## Milestone 2 — Determinism Contract & Replayability ✅
+++**Receipts:** `docs/RUN_REPORT.md`, `scripts/replay_run.py`, tests covering selection reasons + archival + `--recalc`.
+++- [x] Run report explains selection
+++- [x] Schema contract documented
+++- [x] Selected inputs archived per run
+++- [x] Replay workflow + hash verification
+++
+++## Milestone 3 — Scheduled run + object-store publishing (K8s CronJob first) ✅
+++**Receipts:** proof bundles under `ops/proof/bundles/m3-*`, runbooks, publish/verify scripts.
+++- [x] CronJob runs end-to-end
+++- [x] S3 publish plan + offline verification
+++- [x] Real bucket publish verified (+ latest pointers)
+++- [x] Live scrape proof in-cluster
+++- [x] Politeness/backoff/circuit breaker enforced
+++
+++## Milestone 4 — On-Prem primary + Cloud DR (proven once; stability pending) ◐
+++**Status:** Partially complete: backup/restore + cloud DR proven once, on-prem 72h stability not yet proven.
+++**Receipts:** `ops/proof/bundles/m4-*`, runbooks in repo.
+++- [x] Backup/restore rehearsal (encrypted + checksummed)
+++- [x] DR rehearsal end-to-end (bring up → restore → run → teardown)
+++- [ ] On-prem 72h stability receipts (blocked by hardware timing)
+++
+++## Milestone 5 — Provider Expansion (config-driven, offline proof) ◐
+++**Status:** Offline multi-provider proof exists; “fully config-driven provider registry” still needs consolidation/hardening as a single coherent milestone (see Milestone 10 below).
+++**Receipts:** `docs/proof/m5-offline-multi-provider-2026-02-11.md`
+++
+++## Milestone 6 — History & intelligence (identity, dedupe, user state) ✅
+++**Receipts:** `src/ji_engine/history_retention.py`, tests, `docs/OPERATIONS.md`.
+++- [x] Stable job identity + identity-based diffs
+++- [x] Retention rules enforced
+++- [x] User state overlay affects outputs and alerts
+++
+++## Milestone 7 — Semantic Safety Net (deterministic) ✅ (Phase 1 scope)
+++**Receipts:** `docs/proof/m7-semantic-safety-net-offline-2026-02-12.md`, tests.
+++- [x] Deterministic embedding backend (hash backend) + cache
+++- [x] Sidecar + boost modes
+++- [x] Thresholds testable/documented
+++- [x] Evidence artifacts produced
+++
+++## Milestone 8 — Hardening & scaling (Phase 1 subset done) ◐
+++**Status:** Several elements exist (cost guardrails, provider availability reasons, observability basics), but consolidation is needed (see Milestone 12).
+++- [x] Cost guardrails + costs artifact
+++- [x] Provider unavailable reasons surfaced
+++- [x] CI smoke gate failure modes documented
+++- [ ] Full “operational hardening pack” milestone still needed
+++
+++## Milestone 9 — Multi-user (deferred to Phase 3) ⏸
+++**Status:** intentionally deferred; do not start UX/product complexity until Phase 2/3 tranche.
+++
+++---
diff --cc docs/RUN_REPORT.md
index 491efdb,491efdb,0000000..06334db
mode 100644,100644,000000..100644
--- a/docs/RUN_REPORT.md
+++ b/docs/RUN_REPORT.md
@@@@ -1,153 -1,153 -1,0 +1,173 @@@@
  +# Run Report Reference
  +
  +Run reports are written to `state/runs/<run_id>.json` and copied to
  +`state/runs/<run_id>/run_report.json`. They include metadata for reproducibility,
  +debugging, and audit trails. They are versioned with `run_report_schema_version`.
  +
  +## Schema version
  +`run_report_schema_version`: integer. Current version: **1**.
  +
  +## Timestamp format
  +All run report timestamps use UTC ISO 8601 with trailing `Z` and seconds precision
  +(no fractional seconds). This includes `run_id`, `timestamps.started_at`,
  +`timestamps.ended_at`, and any `*_at`/`*_time` fields in provenance or artifacts.
  +
  +## Top-level fields
  +- `run_id`: ISO timestamp used to identify the run.
  +- `status`: status string (`success`, `short_circuit`, `error`).
  +- `success`: boolean derived from status.
  +- `failed_stage`: present when status is `error`.
  +- `profiles`: list of profiles processed (e.g., `["cs"]`).
  +- `providers`: list of providers processed (e.g., `["openai"]`).
  +- `flags`: CLI flags and thresholds (including `min_score`, `min_alert_score`).
  +- `timestamps`: `started_at`, `ended_at`.
  +- `stage_durations`: per-stage timings.
  +- `diff_counts`: per-profile diff counts (new/changed/removed).
  +- `provenance_by_provider`: scrape provenance (snapshot/live, hashes, parsed counts).
  +- `selection`: top-level selection summary including:
  +  - `scrape_provenance`
  +  - `classified_job_count`
  +  - `classified_job_count_by_provider`
  +- `inputs`: raw/labeled/enriched input file metadata (path, mtime, sha256).
  +- `outputs_by_profile`: ranked outputs (paths + sha256).
  +- Ranked job artifacts include a deterministic `job_id` per posting:
  +  - Preserve source `job_id` when provided.
  +  - Else use canonical apply/detail URL.
  +  - Else fall back to provider-aware deterministic identity.
  +- `inputs_by_provider`, `outputs_by_provider`: provider-specific input/output metadata.
  +- `verifiable_artifacts`: mapping of logical artifact keys to hashes:
  +  - Keys are `"<provider>:<profile>:<output_key>"`.
  +  - Values include `path` (relative to `JOBINTEL_DATA_DIR`), `sha256`, `bytes`, and `hash_algo`.
  +- `config_fingerprint`: sha256 of the effective, non-secret configuration inputs.
  +- `environment_fingerprint`: best-effort environment details (python version, platform, image tag, git sha, TZ, PYTHONHASHSEED).
  +- `logs`: observability pointers for this run:
  +  - `schema_version`, `run_id`
  +  - `local`: `run_dir`, `logs_dir`, `stdout`, and optional `structured_log_jsonl`
  +  - `k8s` (best-effort): `namespace`, optional `context`, and command templates:
  +    - `pod_list_command`
  +    - `job_list_command`
  +    - `logs_command_template` (grep by `JOBINTEL_RUN_ID=<run_id>`)
  +  - `cloud` (best-effort): AWS `region`, `cloudwatch_log_group`, `cloudwatch_log_stream` when available in env.
  +    - includes `cloudwatch_filter_pattern` pinned to `JOBINTEL_RUN_ID=<run_id>` for deterministic discovery.
  +- `log_retention`: deterministic logs-only retention summary:
  +  - `keep_runs`, `runs_seen`, `runs_kept`, `log_dirs_pruned`, `pruned_log_dirs`, `reason`
  +  - pruning only removes `state/runs/<run_id>/logs/` for older runs; it does not delete run artifacts.
  +- `scoring_inputs_by_profile`: selected scoring input metadata (path/mtime/sha256).
  +- `scoring_input_selection_by_profile`: decision metadata for scoring inputs:
  +  - `selected_path`
  +  - `candidate_paths_considered` (path/mtime/sha/exists)
  +  - `selection_reason` (enum string)
  +  - `selection_reason_details` (stable reasoning object):
  +    - `labeled_vs_enriched` and `enriched_vs_ai` each include:
  +      - `rule_id` (stable string)
  +      - `chosen_path`
  +      - `candidate_paths`
  +      - `compared_fields` (mtimes/hashes used)
  +      - `decision` (short enum)
  +      - `decision_timestamp` (ISO 8601)
  +  - `comparison_details` (e.g., newer_by_seconds, prefer_ai)
  +  - `decision` (human-readable rule and reason)
  +- `archived_inputs_by_provider_profile`: archived copies of the selected scoring inputs and profiles config:
  +  - `<provider>` → `<profile>` → `{selected_scoring_input, profile_config}`
  +  - Each archived entry includes `source_path`, `archived_path` (relative to `JOBINTEL_STATE_DIR`), `sha256`, `bytes`.
  +- `delta_summary`: delta intelligence summary if available.
  +- `git_sha`: best-effort git sha when available.
  +- `image_tag`: container image tag if set.
  +- `s3_bucket`, `s3_prefixes`, `uploaded_files_count`, `dashboard_url`: S3 publishing metadata (when enabled).
+++- `artifact_model`: Artifact Model v2 pointers:
+++  - `ui_safe`: `{schema_version: 2, path: state/runs/<run_id>/artifact_ui_safe.v2.json}`
+++  - `replay_safe`: `{schema_version: 2, path: state/runs/<run_id>/artifact_replay_safe.v2.json}`
  +- Diff summary artifacts are written under the run directory (`state/runs/<run_id>/diff_summary.json` and `.md`).
  +
+++## Artifact Model v2
+++
+++Artifact Model v2 separates UI-facing payloads from replay payloads:
+++
+++- UI-safe (`artifact_ui_safe.v2.json`)
+++  - Schema: `schemas/artifact_ui_safe.schema.v2.json`
+++  - Policy: no raw JD text and no excerpts (`raw_jd_allowed=false`, `excerpt_allowed=false`)
+++  - Includes sanitized ranked job projections suitable for dashboard-style views.
+++- Replay-safe (`artifact_replay_safe.v2.json`)
+++  - Schema: `schemas/artifact_replay_safe.schema.v2.json`
+++  - Includes deterministic pointers, hashes, and provenance needed for replay/debug flows.
+++
+++Backward compatibility policy:
+++- Existing runs without Artifact Model v2 files remain valid.
+++- Readers must derive a deterministic UI-safe projection from `run_report.json` when
+++  `artifact_ui_safe.v2.json` is missing (legacy fallback path).
+++
  +### Provider provenance additions
  +Each provider entry in `provenance_by_provider` may include:
  +- `live_error_type`: one of `success`, `transient_error`, `unavailable`, `invalid_response` (when live was attempted).
  +- `snapshot_baseline_count`: job count from the snapshot baseline (when available).
  +- `failure_policy`: deterministic policy evaluation result and thresholds:
  +  - `decision`: `ok` or `fail`
  +  - `reason`: short enum-like reason string
  +  - `parsed_job_count`, `snapshot_baseline_count`
  +  - `error_rate`, `error_rate_max`, `min_jobs`, `min_snapshot_ratio`
  +  - `enrich_stats`: `{total, enriched, unavailable, failed}`
  +
  +## Selection reason enums
  +Selection reasons are deterministic strings such as:
  +- `ai_only`
  +- `no_enrich_enriched_newer`
  +- `no_enrich_labeled_newer_or_equal`
  +- `no_enrich_enriched_only`
  +- `no_enrich_labeled_only`
  +- `no_enrich_missing`
  +- `default_enriched_required`
  +- `default_enriched_missing`
  +- `prefer_ai_enriched`
  +
  +### selection_reason_details.rule_id
  +Rule IDs are stable strings scoped to the decision boundary:
  +- `labeled_vs_enriched.<decision>`
  +- `enriched_vs_ai.<decision>`
  +
  +## How to debug a run
  +Use these paths to inspect artifacts:
  +
  +- Run report:
  +  - `state/runs/<run_id>.json`
  +  - `state/runs/<run_id>/run_report.json`
  +- Run registry:
  +  - `state/runs/<run_id>/index.json`
  +- Ranked outputs:
  +  - `data/<provider>_ranked_jobs.<profile>.json`
  +  - `data/<provider>_ranked_jobs.<profile>.csv`
  +  - `data/<provider>_ranked_families.<profile>.json`
  +  - `data/<provider>_shortlist.<profile>.md`
  +  - `data/<provider>_top.<profile>.md`
  +- Alerts:
  +  - `data/<provider>_alerts.<profile>.json`
  +  - `data/<provider>_alerts.<profile>.md`
  +- Diff summary:
  +  - `state/runs/<run_id>/diff_summary.json`
  +  - `state/runs/<run_id>/diff_summary.md`
  +- AI insights (when enabled):
  +  - `state/runs/<run_id>/ai_insights.<profile>.json`
  +  - `state/runs/<run_id>/ai_insights.<profile>.md`
  +- AI job briefs (when enabled):
  +  - `state/runs/<run_id>/ai_job_briefs.<profile>.json`
  +  - `state/runs/<run_id>/ai_job_briefs.<profile>.md`
  +
  +## Replayability
  +To validate reproducibility:
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --profile cs
  +```
  +- Exit code `0`: reproducible
  +- Exit code `2`: missing inputs or mismatched hashes
  +- Exit code `>=3`: runtime error
  +
  +To recompute scoring outputs from archived inputs and compare hashes:
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --profile cs --strict --recalc
  +```
  +- Uses archived scoring inputs + profile config from the run directory (no `data/` dependency).
  +- Writes regenerated outputs under `state/runs/<run_id>/_recalc/` and compares hashes to the run report.
  +
  +Machine-readable replay output:
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --json
  +```
diff --cc ops/k8s/README.md
index 856b4a8,856b4a8,0000000..f04dbf3
mode 100644,100644,000000..100644
--- a/ops/k8s/README.md
+++ b/ops/k8s/README.md
@@@@ -1,336 -1,336 -1,0 +1,342 @@@@
  +# Kubernetes CronJob (SignalCraft)
  +
  +This directory contains a minimal, Kubernetes-native CronJob shape for running SignalCraft daily.
  +It is intentionally plain YAML + kustomize (no Helm).
  +The CronJob YAML uses a placeholder image name; replace it with your registry/repo tag.
  +
  +Runbook: `ops/k8s/RUNBOOK.md` (deploy, inspect, rollback, rotate secrets).
  +Nodegroup upgrade recovery: `ops/k8s/RUNBOOK_NODEGROUP_UPGRADE_RECOVERY.md`.
  +Triage helper: `scripts/ops/eks_nodegroup_upgrade_triage.sh` (read-only; prints recommended actions).
  +On-prem baseline docs: `ops/onprem/README.md`, `ops/onprem/RUNBOOK_ONPREM_INSTALL.md`, `ops/onprem/RUNBOOK_DEPLOY.md`, `ops/onprem/RUNBOOK_UPGRADES.md`.
  +
  +## Runtime contract
  +
  +Required env vars (names only):
  +- `JOBINTEL_S3_BUCKET` (only when publishing)
  +- `AWS_REGION` or `JOBINTEL_AWS_REGION` (when publishing to AWS)
  +
  +Optional env vars:
  +- `CAREERS_MODE` (`SNAPSHOT` default; set `LIVE` for live mode)
  +- `EMBED_PROVIDER` (default `stub`)
  +- `ENRICH_MAX_WORKERS` (default `1`)
  +- `AI_ENABLED` (default `0`)
  +- `PUBLISH_S3` / `PUBLISH_S3_DRY_RUN`
  +- `JOBINTEL_S3_PREFIX` (default `jobintel`)
  +- `DISCORD_WEBHOOK_URL`, `OPENAI_API_KEY`
  +
  +Storage expectations:
  +- Local scratch: `emptyDir` for `/tmp` and `/work`.
  +- Local run data: `emptyDir` for `/app/data` and `/app/state` (swap for PVCs if you want persistence).
  +- Persistent artifacts: object-store publish via `publish_s3` (`S3` compatible).
  +
  +Container command:
  +- `python scripts/run_daily.py --profiles cs --us_only --no_post --snapshot-only --offline`
  +
  +Deterministic defaults:
  +- `CAREERS_MODE=SNAPSHOT`
  +- `ENRICH_MAX_WORKERS=1`
  +- `AI_ENABLED=0`
  +
  +Live mode requires explicit provider policy env vars:
  +- `JOBINTEL_PROVIDER_ERROR_RATE_MAX`
  +- `JOBINTEL_PROVIDER_MIN_JOBS`
  +- `JOBINTEL_PROVIDER_MIN_SNAPSHOT_RATIO`
  +- `JOBINTEL_PROVIDER_MAX_ATTEMPTS`
  +- `JOBINTEL_PROVIDER_BACKOFF_BASE`
  +- `JOBINTEL_PROVIDER_BACKOFF_MAX`
  +
  +## Kustomize packages
  +
  +Base (portable):
  +- `ops/k8s/jobintel/`
  +
  +AWS EKS overlay (IRSA + publish toggles):
  +- `ops/k8s/overlays/aws-eks/`
  +
  +Live overlay (opt-in):
  +- `ops/k8s/overlays/live/`
  +
  +On-prem overlay (k3s/local-path PVCs + ingress):
  +- `ops/k8s/jobintel/overlays/onprem/`
+++- `ops/k8s/overlays/onprem-pi/` (recommended for 4-node Pi clusters)
+++
+++On-prem (4-node Pi profile) defaults:
+++- CronJob prefers `jobintel.io/node-class=pi5` nodes for heavy runs.
+++- Dashboard prefers `jobintel.io/node-class=pi4` nodes to preserve Pi5 headroom.
+++- Storage class `local-path-nvme` targets `/mnt/nvme/local-path-provisioner`.
  +
  +IRSA note (conceptual):
  +- IRSA maps a Kubernetes ServiceAccount to a cloud IAM role so pods can access object stores without static keys.
  +- The AWS overlay expects `JOBINTEL_IRSA_ROLE_ARN` at render time (no manual YAML edits).
  +
  +## Apply (order)
  +
  +```bash
  +kubectl apply -k ops/k8s/jobintel
  +```
  +
  +For EKS with IRSA:
  +```bash
  +export JOBINTEL_IMAGE=<account>.dkr.ecr.<region>.amazonaws.com/jobintel:<tag-or-digest>
  +JOBINTEL_IRSA_ROLE_ARN=arn:aws:iam::<account>:role/<role> \
  +  JOBINTEL_IMAGE="$JOBINTEL_IMAGE" \
  +  python scripts/k8s_render.py --overlay aws-eks --image "$JOBINTEL_IMAGE" > /tmp/jobintel.yaml
  +kubectl apply -f /tmp/jobintel.yaml
  +```
  +
  +Full AWS flow (ECR push + preflight + render/apply): `ops/aws/EKS_ECR_GOLDEN_PATH.md`.
  +
  +For on-prem k3s:
  +```bash
-- python scripts/k8s_render.py --overlay onprem > /tmp/jobintel-onprem.yaml
+++python scripts/k8s_render.py --overlay onprem-pi > /tmp/jobintel-onprem.yaml
  +kubectl apply -f /tmp/jobintel-onprem.yaml
  +```
  +
  +Preflight (offline, no AWS calls):
  +```bash
  +make preflight
  +```
  +
  +## Mode matrix
  +
  +SNAPSHOT (default, deterministic):
  +- Args include `--snapshot-only --offline`
  +- `CAREERS_MODE=SNAPSHOT`
  +- Safe for scheduled daily runs
  +
  +LIVE (opt-in, provider-dependent):
  +- Args remove `--snapshot-only --offline`
  +- `CAREERS_MODE=LIVE`
  +- Requires explicit provider policy env vars (see above)
  +
  +Publish (object store):
  +- `PUBLISH_S3=1` and `PUBLISH_S3_DRY_RUN=0`
  +- Use `aws-eks` overlay to enable publish
  +
  +## Run a one-off Job
  +
  +Canonical one-off execution (offline-safe, deterministic):
  +```bash
  +kubectl apply -k ops/k8s/jobintel
  +kubectl delete job -n jobintel jobintel-manual --ignore-not-found
  +kubectl create job -n jobintel --from=cronjob/jobintel-daily jobintel-manual
  +kubectl logs -n jobintel job/jobintel-manual
  +```
  +
  +You can print the full run-once command sequence locally:
  +```bash
  +make k8s-run-once
  +```
  +
  +Alternative: use the CronJob template to run a single execution:
  +```bash
  +kubectl create job -n jobintel --from=cronjob/jobintel-daily jobintel-run-once
  +kubectl logs -n jobintel job/jobintel-run-once
  +```
  +
  +## Required secrets
  +
  +Create a secret named `jobintel-secrets` with the following keys (use only what you need):
  +- `JOBINTEL_S3_BUCKET`: target bucket for publish
  +- `AWS_ACCESS_KEY_ID`: AWS access key (only if not using IRSA)
  +- `AWS_SECRET_ACCESS_KEY`: AWS secret key (only if not using IRSA)
  +- `DISCORD_WEBHOOK_URL`: optional alerts
  +- `OPENAI_API_KEY`: optional AI features
  +
  +Example:
  +```bash
  +kubectl -n jobintel create secret generic jobintel-secrets \
  +  --from-literal=JOBINTEL_S3_BUCKET=your-bucket \
  +  --from-literal=AWS_ACCESS_KEY_ID=... \
  +  --from-literal=AWS_SECRET_ACCESS_KEY=... \
  +  --from-literal=DISCORD_WEBHOOK_URL=... \
  +  --from-literal=OPENAI_API_KEY=...
  +```
  +
  +Notes:
  +- Secrets-based auth is the primary example.
  +- IRSA / workload identity is supported and preferred for EKS runs.
  +- `role.yaml` is intentionally empty; remove Role/RoleBinding if you don’t need in-cluster RBAC.
  +
  +## ConfigMap + Secret expectations
  +
  +ConfigMap defaults (ops/k8s/jobintel/configmap.yaml):
  +- Required for publish: `JOBINTEL_S3_PREFIX` (optional; defaults to `jobintel`), `JOBINTEL_AWS_REGION` (or set `AWS_REGION`).
  +- Deterministic defaults baked in: `CAREERS_MODE=SNAPSHOT`, `EMBED_PROVIDER=stub`, `ENRICH_MAX_WORKERS=1`.
  +- Publish toggles: `PUBLISH_S3` and `PUBLISH_S3_DRY_RUN` (set `PUBLISH_S3_DRY_RUN=1` for offline plan-only runs).
  +
  +Secret expectations (ops/k8s/jobintel/secret.example.yaml):
  +- Required to actually publish: `JOBINTEL_S3_BUCKET` + AWS credentials (or IRSA/workload identity instead).
  +- Optional: `DISCORD_WEBHOOK_URL`, `OPENAI_API_KEY`.
  +
  +## Dry-run / deterministic mode
  +
  +To run without AWS calls:
  +- Set `PUBLISH_S3_DRY_RUN=1` (or `PUBLISH_S3=0`) in the ConfigMap or at runtime.
  +- CronJob args already include `--snapshot-only` for offline determinism.
  +
  +Example override:
  +```bash
  +kubectl set env cronjob/jobintel-daily -n jobintel PUBLISH_S3_DRY_RUN=1
  +```
  +
  +To override any env var without editing YAML:
  +```bash
  +kubectl set env cronjob/jobintel-daily -n jobintel JOBINTEL_S3_PREFIX=jobintel-dev
  +```
  +
  +## Verification
  +
  +Generate a publish plan (offline, no AWS calls):
  +```bash
  +python scripts/publish_s3.py --run-id <run_id> --plan --json > /tmp/jobintel_plan.json
  +```
  +
  +Verify the plan offline (no AWS calls):
  +```bash
  +python scripts/verify_published_s3.py --offline --plan-json /tmp/jobintel_plan.json
  +```
  +
  +After a real publish, verify S3 objects (requires credentials):
  +```bash
  +python scripts/verify_published_s3.py \
  +  --bucket "$JOBINTEL_S3_BUCKET" \
  +  --run-id <run_id> \
  +  --verify-latest
  +```
  +
  +Run replay smoke locally against a run report:
  +```bash
  +python scripts/replay_run.py --run-id <run_id> --strict
  +```
  +
  +## AWS/EKS hosting runbook (one-off proof run)
  +
  +Set kube context (example):
  +```bash
  +kubectl config use-context <your-eks-context>
  +```
  +
  +EKS bootstrap (Terraform -> kubeconfig -> proof run):
  +- See `ops/aws/infra/eks/README.md` for required variables and apply steps.
  +
  +Then set the ServiceAccount IRSA annotation using the Terraform output:
  +```bash
  +JOBINTEL_IRSA_ROLE_ARN="$(terraform -chdir=ops/aws/infra/eks output -raw jobintel_irsa_role_arn)"
  +```
  +
  +Create secrets (no secrets committed):
  +```bash
  +kubectl -n jobintel create secret generic jobintel-secrets \
  +  --from-literal=JOBINTEL_S3_BUCKET=your-bucket \
  +  --from-literal=AWS_ACCESS_KEY_ID=... \
  +  --from-literal=AWS_SECRET_ACCESS_KEY=... \
  +  --from-literal=DISCORD_WEBHOOK_URL=... \
  +  --from-literal=OPENAI_API_KEY=...
  +```
  +
  +IAM permissions (IRSA):
  +- Use IRSA to map the ServiceAccount to an IAM role with the actions documented in `ops/aws/README.md`.
  +- The AWS overlay adds the ServiceAccount annotation placeholder; replace it with your IAM role ARN.
  +
  +Apply base + AWS overlay:
  +```bash
  +JOBINTEL_IRSA_ROLE_ARN="$JOBINTEL_IRSA_ROLE_ARN" \
  +  python scripts/k8s_render.py --overlay aws-eks > /tmp/jobintel.yaml
  +kubectl apply -f /tmp/jobintel.yaml
  +```
  +
  +Apply base + LIVE overlay:
  +```bash
  +kubectl apply -k ops/k8s/overlays/live
  +```
  +
  +Run a one-off Job from the CronJob template:
  +```bash
  +kubectl delete job -n jobintel jobintel-manual-$(date +%Y%m%d) --ignore-not-found
  +kubectl create job -n jobintel --from=cronjob/jobintel-daily jobintel-manual-$(date +%Y%m%d)
  +kubectl logs -n jobintel job/jobintel-manual-$(date +%Y%m%d)
  +```
  +
  +Capture proof (extracts run_id from logs if omitted):
  +```bash
  +python scripts/prove_cloud_run.py \
  +  --bucket "$JOBINTEL_S3_BUCKET" \
  +  --prefix "$JOBINTEL_S3_PREFIX" \
  +  --namespace jobintel \
  +  --job-name jobintel-manual-$(date +%Y%m%d) \
  +  --kube-context <your-eks-context>
  +```
  +
  +## EKS proof run - copy/paste commands
  +
  +Publish-enabled, snapshot-only proof run (real S3, no dry-run):
  +```bash
  +export JOBINTEL_S3_BUCKET=<bucket>
  +python scripts/preflight_env.py --mode publish
  +
  +export JOBINTEL_IRSA_ROLE_ARN="$(terraform -chdir=ops/aws/infra/eks output -raw jobintel_irsa_role_arn)"
  +python scripts/k8s_render.py --overlay aws-eks > /tmp/jobintel.yaml
  +kubectl apply -f /tmp/jobintel.yaml
  +
  +kubectl -n jobintel create secret generic jobintel-secrets \
  +  --from-literal=JOBINTEL_S3_BUCKET="$JOBINTEL_S3_BUCKET" \
  +  --from-literal=DISCORD_WEBHOOK_URL=... \
  +  --from-literal=OPENAI_API_KEY=...
  +
  +kubectl -n jobintel create job --from=cronjob/jobintel-daily jobintel-manual-<yyyymmdd>
  +kubectl -n jobintel logs -f job/jobintel-manual-<yyyymmdd>
  +
  +python scripts/prove_cloud_run.py \
  +  --bucket <bucket> \
  +  --prefix jobintel \
  +  --namespace jobintel \
  +  --job-name jobintel-manual-<yyyymmdd> \
  +  --kube-context <context>
  +
  +python scripts/verify_published_s3.py \
  +  --bucket <bucket> \
  +  --run-id <run_id> \
  +  --verify-latest
  +```
  +
  +Live + publish (opt-in, provider-dependent):
  +```bash
  +python scripts/k8s_render.py --overlay live --overlay aws-eks > /tmp/jobintel-live.yaml
  +kubectl apply -f /tmp/jobintel-live.yaml
  +```
  +
  +Notes:
  +- Run the secret command once; use `kubectl set env` or secret updates to change values.
  +- The AWS overlay enables `PUBLISH_S3=1` and `PUBLISH_S3_DRY_RUN=0` with a bucket placeholder.
  +
  +## Local smoke
  +
  +Simulate the CronJob shape locally (offline, deterministic):
  +```bash
  +make cronjob-smoke
  +```
  +
  +## Expected outputs
  +
  +- Run report: `state/runs/<run_id>/run_report.json`
  +- Run artifacts (S3): `s3://<bucket>/<prefix>/runs/<run_id>/<provider>/<profile>/...`
  +- Latest pointers (S3): `s3://<bucket>/<prefix>/latest/<provider>/<profile>/...`
  +
  +## Retention guidance
  +
  +- Local state: keep the last N runs (for example 30) if using PVCs; prune out-of-band.
  +- Object store: apply a lifecycle policy to expire `runs/<run_id>/` after N days; keep `latest/` and `state/` keys indefinitely.
  +
  +## Troubleshooting
  +
  +- Permission errors: ensure the secret exists and S3 credentials are correct.
  +- Missing secrets: CronJob will fail on startup; check the namespace and secret name.
  +- Stuck or failing jobs: check `kubectl describe job` and pod logs.
  +- Plan/verify failures: confirm `run_report.json` exists and `verifiable_artifacts` is populated.
  +- If GitHub Actions are queued or flaky, rerun the workflow or wait for recovery.
  +
  +## Storage notes
  +
  +The CronJob uses `emptyDir` for `/app/data` and `/app/state` by default.
  +For persistence across runs, replace these with PVCs or a CSI-backed volume.
diff --cc ops/k8s/jobintel/overlays/onprem/README.md
index f6ff445,f6ff445,0000000..dbc1266
mode 100644,100644,000000..100644
--- a/ops/k8s/jobintel/overlays/onprem/README.md
+++ b/ops/k8s/jobintel/overlays/onprem/README.md
@@@@ -1,30 -1,30 -1,0 +1,52 @@@@
  +# On-Prem Overlay Contract
  +
  +This overlay is the canonical on-prem deploy target:
  +
  +```bash
  +kubectl apply -k ops/k8s/jobintel/overlays/onprem
  +```
  +
  +## What it includes
  +
-- - `CronJob` (`jobintel-daily`)
-- - Dashboard deployment/service
-- - PVCs for persistent `state` and `data` paths
-- - Ingress resource for dashboard with Traefik class and TLS secret reference
+++- `Namespace` labels for on-prem inventory (`jobintel.io/environment=onprem`)
+++- On-prem config + secret example manifests (no real credentials in repo)
+++- `CronJob` (`jobintel-daily`) with Pi scheduling policy:
+++  - `arm64` node selector
+++  - strong preference for Pi5 nodes (`jobintel.io/node-class=pi5`) for heavy run workloads
+++- Dashboard deployment/service with arm64 scheduling and spread constraints
+++- NVMe-backed storage strategy:
+++  - storage class `local-path-nvme`
+++  - PVCs for persistent `state` and `data` paths
+++- Ingress resource for dashboard with Traefik class, TLS, and default RFC1918 source-range allowlist
  +
  +## Storage + database stance
  +
  +- Primary persisted state is filesystem-backed (`/app/state`, `/app/data/ashby_cache`) on PVCs.
+++- Expected host mount: `/mnt/nvme/local-path-provisioner` on every worker node.
+++- Ensure local-path-provisioner is configured to honor the `nodePath` parameter for
+++  `local-path-nvme` storage class in your cluster baseline.
  +- Postgres is optional and not enabled in this default overlay.
  +- If you need Postgres on-prem, use `ops/k8s/jobintel/overlays/onprem-postgres`.
  +
  +## Access strategy
  +
  +- VPN-first (Tailscale/WireGuard) is required by default.
  +- Do not expose dashboard/API to WAN without a documented exception.
  +
  +## TLS strategy
  +
  +- Ingress expects `jobintel-dashboard-tls` secret.
  +- Use internal CA or ACME flow compatible with private/VPN DNS.
+++
+++## Pi node labels
+++
+++Label nodes once during cluster bootstrap:
+++
+++```bash
+++kubectl label nodes <pi5-node-a> jobintel.io/node-class=pi5 --overwrite
+++kubectl label nodes <pi5-node-b> jobintel.io/node-class=pi5 --overwrite
+++kubectl label nodes <pi4-node-a> jobintel.io/node-class=pi4 --overwrite
+++kubectl label nodes <pi4-node-b> jobintel.io/node-class=pi4 --overwrite
+++```
+++
+++If labels are missing, workloads still schedule (preferences are soft), but heavy jobs may land on Pi4 nodes.
diff --cc ops/k8s/jobintel/overlays/onprem/ingress-dashboard.yaml
index e6139b1,e6139b1,d32af90..15dbf2e
--- a/ops/k8s/jobintel/overlays/onprem/ingress-dashboard.yaml
+++ b/ops/k8s/jobintel/overlays/onprem/ingress-dashboard.yaml
@@@@ -5,7 -5,7 -5,10 +5,11 @@@@ metadata
     namespace: jobintel
     annotations:
       kubernetes.io/ingress.class: traefik
++     traefik.ingress.kubernetes.io/router.entrypoints: websecure
+++    # Restrict to RFC1918 ranges by default; override only with explicit approval.
++     traefik.ingress.kubernetes.io/whitelist-source-range: 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
   spec:
++   ingressClassName: traefik
     tls:
       - hosts:
           - jobintel.internal
diff --cc ops/k8s/jobintel/overlays/onprem/kustomization.yaml
index e9ac617,e9ac617,b2823ce..f25dcec
--- a/ops/k8s/jobintel/overlays/onprem/kustomization.yaml
+++ b/ops/k8s/jobintel/overlays/onprem/kustomization.yaml
@@@@ -1,12 -1,12 -1,18 +1,18 @@@@
   apiVersion: kustomize.config.k8s.io/v1beta1
   kind: Kustomization
  +
   resources:
  -  - ../../jobintel
  +  - ../..
++   - storageclass-local-path-nvme.yaml
     - pvc-data.yaml
     - pvc-state.yaml
     - ingress-dashboard.yaml
  -commonLabels:
  -  jobintel.io/overlay: onprem-pi
  +
   patchesStrategicMerge:
++   - namespace-patch.yaml
++   - configmap-onprem.yaml
++   - secret.onprem.example.yaml
     - patch-cronjob-storage.yaml
++   - patch-cronjob-scheduling.yaml
     - patch-dashboard-storage.yaml
++   - patch-dashboard-scheduling.yaml
diff --cc ops/onprem/README.md
index 6004e13,6004e13,0000000..371c544
mode 100644,100644,000000..100644
--- a/ops/onprem/README.md
+++ b/ops/onprem/README.md
@@@@ -1,84 -1,84 -1,0 +1,104 @@@@
  +# On-Prem Baseline (k3s on ARM)
  +
  +This directory is the on-prem operations scaffold for SignalCraft primary runtime.
  +
  +## Golden Path
  +
-- 1. Prepare 3 nodes with static IPs and hostnames (`jobintel-pi1`, `jobintel-pi2`, `jobintel-pi3`).
+++1. Prepare 4 nodes with static IPs and hostnames (`pi4-a`, `pi4-b`, `pi5-a`, `pi5-b`).
  +2. Attach USB3 SSD to the server node and mount it (`ops/onprem/mount-ssd.sh`).
  +3. Install k3s server (`ops/onprem/install-k3s-server.sh`).
  +4. Join agent nodes (`ops/onprem/install-k3s-agent.sh`).
-- 5. Deploy manifests from `ops/k8s/jobintel/overlays/onprem`.
+++5. Label nodes for scheduler preferences (`jobintel.io/node-class=pi4|pi5`).
+++5. Deploy manifests from `ops/k8s/overlays/onprem-pi`.
  +6. Access dashboard over VPN + internal TLS.
  +
  +## Hardware
  +
-- - Recommended: 3 nodes total.
-- - Path A: `3 x Raspberry Pi 4 (8GB)`.
-- - Path B: `2 x Raspberry Pi 4 (8GB) + 1 x Raspberry Pi 5 (8GB)`.
-- - Storage: USB3 SSD for stateful paths (do not use SD for persistent writes).
+++- Recommended: 4 nodes total.
+++- Suggested: `2 x Raspberry Pi 4 (8GB) + 2 x Raspberry Pi 5 (8GB)`.
+++- Scheduling strategy:
+++  - CronJob prefers Pi5 for heavy batch workloads.
+++  - Dashboard prefers Pi4 to keep Pi5 headroom.
+++- Storage: NVMe or SSD for stateful paths (do not use SD for persistent writes).
  +
  +## OS
  +
  +- Preferred: SLE Micro ARM (if your team already operates SUSE tooling).
  +- Fallback: Ubuntu Server 24.04 LTS ARM.
  +
  +## Network assumptions
  +
  +- Layer 2 LAN with static IP reservations.
  +- Outbound internet allowed for job providers and object-store API.
  +- Inbound app access over VPN only (Tailscale or WireGuard), no direct WAN exposure.
  +
  +## Storage choice
  +
-- - Default storage class: `local-path` (bundled with k3s).
+++- On-prem overlay provides `local-path-nvme` storage class.
  +- On-prem overlay provisions PVCs for `/app/state` and `/app/data/ashby_cache`.
+++- Expected node path for local-path provisioner: `/mnt/nvme/local-path-provisioner`.
  +
  +## GitOps-ready posture
  +
  +- All runtime manifests are kustomize resources in repo.
  +- On-prem overlay path: `ops/k8s/jobintel/overlays/onprem`.
+++- On-prem Pi overlay path: `ops/k8s/overlays/onprem-pi`.
  +- This is Flux-compatible (`Kustomization` can target the overlay path directly).
  +
  +## Next docs
  +
  +- `ops/onprem/RUNBOOK_ONPREM_INSTALL.md`
  +- `ops/onprem/RUNBOOK_DEPLOY.md`
  +- `ops/onprem/RUNBOOK_UPGRADES.md`
  +- `ops/onprem/RUNBOOK_BACKUPS.md`
+++- `ops/onprem/RUNBOOK_BACKUP_REHEARSAL_DRYRUN.md`
  +- `ops/onprem/RUNBOOK_BORING_72H_PROOF.md`
  +- `ops/dr/RUNBOOK_DISASTER_RECOVERY.md`
  +
+++## k3s backup/restore helpers (plan-first)
+++
+++Backup rehearsal (dry run by default):
+++
+++```bash
+++DRY_RUN=1 RUN_ID=$(date -u +%Y%m%dT%H%M%SZ) ops/onprem/k3s_backup_artifacts.sh
+++```
+++
+++Restore rehearsal (dry run by default):
+++
+++```bash
+++DRY_RUN=1 BACKUP_DIR=ops/proof/onprem-backups/<run_id> ops/onprem/k3s_restore_artifacts.sh
+++```
+++
  +## Milestone 4 prove-it bundle
  +
  +```bash
  +python scripts/ops/prove_it_m4.py \
  +  --plan \
  +  --run-id m4-plan \
  +  --output-dir ops/proof/bundles \
  +  --aws-region us-east-1 \
  +  --backup-bucket <bucket> \
  +  --backup-prefix <prefix>/backups/m4-plan \
  +  --backup-uri s3://<bucket>/<prefix>/backups/m4-plan
  +```
  +
  +## 72h proof harness (plan-first)
  +
  +```bash
  +python scripts/ops/prove_m4_onprem.py \
  +  --run-id 20260207T120000Z \
  +  --output-dir ops/proof/bundles \
  +  --namespace jobintel \
  +  --cluster-context <k3s-context>
  +```
  +
  +Capture baseline evidence (execute-explicit):
  +
  +```bash
  +python scripts/ops/prove_m4_onprem.py \
  +  --run-id 20260207T120000Z \
  +  --output-dir ops/proof/bundles \
  +  --namespace jobintel \
  +  --cluster-context <k3s-context> \
  +  --execute
  +```
diff --cc ops/onprem/RUNBOOK_BACKUPS.md
index c718952,c718952,0000000..ea99e2e
mode 100644,100644,000000..100644
--- a/ops/onprem/RUNBOOK_BACKUPS.md
+++ b/ops/onprem/RUNBOOK_BACKUPS.md
@@@@ -1,79 -1,79 -1,0 +1,80 @@@@
  +# Runbook: Backups (On-Prem Primary)
  +
  +This runbook defines backup + verify + restore checks for Milestone 4.
+++For k3s PVC rehearsal (no cloud calls), use `RUNBOOK_BACKUP_REHEARSAL_DRYRUN.md`.
  +
  +## Preflight checks
  +
  +```bash
  +python --version
  +aws --version
  +test -n "$AWS_REGION" && echo AWS_REGION=ok
  +test -n "$BACKUP_BUCKET" && echo BACKUP_BUCKET=ok
  +test -n "$BACKUP_PREFIX" && echo BACKUP_PREFIX=ok
  +```
  +
  +Success criteria:
  +- Required env vars are present.
  +- Operator has IAM permissions for backup bucket prefix.
  +
  +If it fails:
  +- Fix credentials/role first; do not run partial backup.
  +
  +## 1) Plan-mode contract (safe default)
  +
  +```bash
  +python scripts/ops/prove_it_m4.py \
  +  --plan \
  +  --run-id m4-backup-plan \
  +  --output-dir ops/proof/bundles \
  +  --aws-region us-east-1 \
  +  --backup-bucket <bucket> \
  +  --backup-prefix <prefix>/backups/m4-backup-plan \
  +  --backup-uri s3://<bucket>/<prefix>/backups/m4-backup-plan
  +```
  +
  +Expected receipts:
  +- `ops/proof/bundles/m4-m4-backup-plan/backup_plan.json`
  +- `ops/proof/bundles/m4-m4-backup-plan/manifest.json`
  +
  +## 2) Execute one backup run
  +
  +```bash
  +python scripts/ops/backup_onprem.py \
  +  --run-id 20260207T000000Z \
  +  --bundle-root ops/proof/bundles \
  +  --aws-region "$AWS_REGION" \
  +  --backup-bucket "$BACKUP_BUCKET" \
  +  --backup-prefix "$BACKUP_PREFIX"
  +```
  +
  +Success criteria:
  +- backup receipt written.
  +- checksum verification log written.
  +
  +If it fails:
  +- inspect `ops/proof/bundles/m4-<run_id>/backup.log`
  +- inspect `ops/proof/bundles/m4-<run_id>/checksum_verify.log`
  +
  +## 3) Restore verification (contract check)
  +
  +```bash
  +python scripts/ops/restore_onprem.py \
  +  --run-id 20260207T000000Z \
  +  --bundle-root ops/proof/bundles \
  +  --backup-uri s3://<bucket>/<prefix>/backups/20260207T000000Z
  +```
  +
  +Success criteria:
  +- restore receipt exists with restored artifact counts.
  +- verify log shows expected files present.
  +
  +If it fails:
  +- inspect `ops/proof/bundles/m4-<run_id>/restore.log`
  +- inspect `ops/proof/bundles/m4-<run_id>/restore_verify.log`
  +
  +## 4) Retention policy contract
  +
  +- Daily backups: keep 14 days.
  +- Weekly backups: keep 8 weeks.
  +- Backups are encrypted + checksummed before acceptance.
diff --cc ops/onprem/RUNBOOK_DEPLOY.md
index c65c8f3,c65c8f3,0000000..0e78fe0
mode 100644,100644,000000..100644
--- a/ops/onprem/RUNBOOK_DEPLOY.md
+++ b/ops/onprem/RUNBOOK_DEPLOY.md
@@@@ -1,80 -1,80 -1,0 +1,81 @@@@
  +# Runbook: Deploy (On-Prem Primary Path)
  +
  +## Preflight checks
  +
  +```bash
  +kubectl config current-context
  +kubectl get nodes
  +kubectl -n jobintel get secret jobintel-secrets
  +```
  +
  +Success criteria:
  +- Context is the intended k3s cluster.
  +- Nodes are `Ready`.
  +- `jobintel-secrets` exists (created out-of-band; no plaintext secrets in repo).
  +
  +If it fails:
  +- Re-select context and re-run.
  +- If secret missing: create from secret manager material before deploy.
  +
  +## 1) One-command deploy (kustomize source of truth)
  +
  +```bash
-- kubectl apply -k ops/k8s/jobintel/overlays/onprem
+++kubectl apply -k ops/k8s/overlays/onprem-pi
  +```
  +
  +Success criteria:
  +- Apply exits zero and creates/updates namespace resources.
  +
  +If it fails:
  +- `kubectl kustomize ops/k8s/jobintel/overlays/onprem | head -n 80`
+++- `kubectl kustomize ops/k8s/overlays/onprem-pi | head -n 80`
  +- `kubectl -n jobintel get events --sort-by=.metadata.creationTimestamp | tail -n 50`
  +
  +## 2) Verify CronJob + dashboard + persistence
  +
  +```bash
  +kubectl -n jobintel get cronjob,deploy,svc,pvc,ingress -o wide
  +kubectl -n jobintel get pods -o wide
  +```
  +
  +Success criteria:
  +- `jobintel-daily` CronJob exists.
  +- `jobintel-dashboard` deployment available.
  +- `jobintel-state-pvc` and `jobintel-data-pvc` are `Bound`.
  +
  +If it fails:
  +- `kubectl -n jobintel describe pvc jobintel-state-pvc`
  +- `kubectl -n jobintel describe deploy jobintel-dashboard`
  +
  +## 3) Trigger one manual job run
  +
  +```bash
  +RUN_NAME=jobintel-manual-$(date +%Y%m%d-%H%M%S)
  +kubectl -n jobintel create job --from=cronjob/jobintel-daily "$RUN_NAME"
  +kubectl -n jobintel wait --for=condition=complete "job/$RUN_NAME" --timeout=20m
  +kubectl -n jobintel logs "job/$RUN_NAME" | tail -n 120
  +```
  +
  +Success criteria:
  +- Job completes successfully.
  +- Logs include `JOBINTEL_RUN_ID=` and run pipeline summary lines.
  +
  +If it fails:
  +- `kubectl -n jobintel describe job "$RUN_NAME"`
  +- `kubectl -n jobintel get pods -l job-name="$RUN_NAME" -o wide`
  +
  +## 4) Dashboard access (VPN + internal TLS)
  +
  +```bash
  +kubectl -n jobintel get ingress jobintel-dashboard -o yaml
  +kubectl -n jobintel describe ingress jobintel-dashboard
  +kubectl -n jobintel get secret jobintel-dashboard-tls
  +```
  +
  +Success criteria:
  +- Ingress host resolves on VPN.
  +- TLS secret exists and is referenced by ingress.
  +
  +If it fails:
  +- If host does not resolve: fix VPN DNS route first.
  +- If TLS secret missing: provision cert and recreate secret before exposing endpoint.
diff --cc schemas/providers.schema.v1.json
index 5d117fb,5d117fb,0000000..a316009
mode 100644,100644,000000..100644
--- a/schemas/providers.schema.v1.json
+++ b/schemas/providers.schema.v1.json
@@@@ -1,321 -1,321 -1,0 +1,367 @@@@
  +{
  +  "$schema": "https://json-schema.org/draft/2020-12/schema",
  +  "title": "JobIntel Providers Config v1",
  +  "type": "object",
  +  "required": [
  +    "schema_version",
  +    "providers"
  +  ],
  +  "properties": {
  +    "schema_version": {
  +      "const": 1
  +    },
  +    "providers": {
  +      "type": "array",
  +      "items": {
  +        "$ref": "#/$defs/provider"
  +      }
  +    }
  +  },
  +  "$defs": {
  +    "provider": {
  +      "type": "object",
  +      "required": [
  +        "provider_id"
  +      ],
  +      "properties": {
  +        "provider_id": {
  +          "type": "string",
  +          "pattern": "^[a-z0-9_\\-]+$"
  +        },
  +        "name": {
  +          "type": "string",
  +          "minLength": 1
  +        },
  +        "display_name": {
  +          "type": "string",
  +          "minLength": 1
  +        },
  +        "careers_urls": {
  +          "type": "array",
  +          "minItems": 1,
  +          "items": {
  +            "type": "string",
  +            "minLength": 1
  +          }
  +        },
  +        "careers_url": {
  +          "type": "string",
  +          "minLength": 1
  +        },
  +        "board_url": {
  +          "type": "string",
  +          "minLength": 1
  +        },
  +        "allowed_domains": {
  +          "type": "array",
  +          "items": {
  +            "type": "string",
  +            "minLength": 1
  +          }
  +        },
  +        "extraction_mode": {
  +          "type": "string",
  +          "enum": [
  +            "ashby_api",
  +            "ashby",
  +            "jsonld",
  +            "html_rules",
  +            "llm_fallback",
  +            "snapshot_json",
  +            "html_list"
  +          ]
  +        },
  +        "type": {
  +          "type": "string",
  +          "enum": [
  +            "ashby",
  +            "jsonld",
  +            "snapshot_json",
  +            "html_list",
  +            "openai",
  +            "snapshot"
  +          ]
  +        },
  +        "mode": {
  +          "type": "string",
  +          "enum": [
  +            "snapshot",
  +            "live",
  +            "auto"
  +          ]
  +        },
  +        "enabled": {
  +          "type": "boolean"
  +        },
  +        "live_enabled": {
  +          "type": "boolean"
  +        },
  +        "snapshot_enabled": {
  +          "type": "boolean"
  +        },
  +        "snapshot_path": {
  +          "type": "string"
  +        },
  +        "snapshot_dir": {
  +          "type": "string"
  +        },
  +        "llm_fallback": {
  +          "$ref": "#/$defs/llm_fallback"
  +        },
+++        "tombstone": {
+++          "$ref": "#/$defs/tombstone"
+++        },
  +        "update_cadence": {
  +          "oneOf": [
  +            {
  +              "$ref": "#/$defs/update_cadence"
  +            },
  +            {
  +              "type": "string",
  +              "minLength": 1
  +            }
  +          ]
  +        },
  +        "politeness": {
  +          "$ref": "#/$defs/politeness"
  +        }
  +      },
  +      "anyOf": [
  +        {
  +          "required": [
--             "careers_urls"
--           ]
--         },
--         {
--           "required": [
--             "careers_url"
+++            "tombstone"
  +          ]
  +        },
  +        {
--           "required": [
--             "board_url"
+++          "anyOf": [
+++            {
+++              "required": [
+++                "careers_urls"
+++              ]
+++            },
+++            {
+++              "required": [
+++                "careers_url"
+++              ]
+++            },
+++            {
+++              "required": [
+++                "board_url"
+++              ]
+++            }
  +          ]
  +        }
  +      ],
  +      "allOf": [
  +        {
  +          "anyOf": [
  +            {
  +              "required": [
--                 "extraction_mode"
+++                "tombstone"
  +              ]
  +            },
  +            {
--               "required": [
--                 "type"
+++              "anyOf": [
+++                {
+++                  "required": [
+++                    "extraction_mode"
+++                  ]
+++                },
+++                {
+++                  "required": [
+++                    "type"
+++                  ]
+++                }
  +              ]
  +            }
  +          ]
  +        }
  +      ],
  +      "additionalProperties": false
  +    },
  +    "update_cadence": {
  +      "type": "object",
  +      "properties": {
  +        "min_interval_hours": {
  +          "type": "integer",
  +          "minimum": 1
  +        },
  +        "max_staleness_hours": {
  +          "type": "integer",
  +          "minimum": 1
  +        },
  +        "priority": {
  +          "type": "string",
  +          "enum": [
  +            "low",
  +            "normal",
  +            "high"
  +          ]
  +        },
  +        "schedule_hint": {
  +          "type": "string",
  +          "minLength": 1
  +        }
  +      },
  +      "additionalProperties": false
  +    },
  +    "llm_fallback": {
  +      "type": "object",
  +      "properties": {
  +        "enabled": {
  +          "type": "boolean"
  +        },
  +        "cache_dir": {
  +          "type": "string",
  +          "minLength": 1
  +        },
  +        "temperature": {
  +          "type": "number",
  +          "const": 0
  +        }
  +      },
  +      "additionalProperties": false
  +    },
+++    "tombstone": {
+++      "type": "object",
+++      "required": [
+++        "reason"
+++      ],
+++      "properties": {
+++        "reason": {
+++          "type": "string",
+++          "minLength": 1
+++        },
+++        "ticket": {
+++          "type": "string",
+++          "minLength": 1
+++        },
+++        "replaced_by": {
+++          "type": "string",
+++          "minLength": 1
+++        },
+++        "removed_at": {
+++          "type": "string",
+++          "minLength": 1
+++        }
+++      },
+++      "additionalProperties": false
+++    },
  +    "politeness_defaults": {
  +      "type": "object",
  +      "properties": {
  +        "min_delay_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_qps": {
  +          "type": "number",
  +          "exclusiveMinimum": 0
  +        },
  +        "rate_jitter_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_attempts": {
  +          "type": "integer",
  +          "minimum": 1
  +        },
  +        "backoff_base_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "backoff_max_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "backoff_jitter_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_consecutive_failures": {
  +          "type": "integer",
  +          "minimum": 0
  +        },
  +        "cooldown_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_inflight_per_host": {
  +          "type": "integer",
  +          "minimum": 1
  +        }
  +      },
  +      "additionalProperties": false
  +    },
  +    "politeness": {
  +      "type": "object",
  +      "properties": {
  +        "defaults": {
  +          "$ref": "#/$defs/politeness_defaults"
  +        },
  +        "host_overrides": {
  +          "type": "object",
  +          "additionalProperties": {
  +            "$ref": "#/$defs/politeness_defaults"
  +          }
  +        },
  +        "host_qps_caps": {
  +          "type": "object",
  +          "additionalProperties": {
  +            "type": "number",
  +            "exclusiveMinimum": 0
  +          }
  +        },
  +        "host_concurrency_caps": {
  +          "type": "object",
  +          "additionalProperties": {
  +            "type": "integer",
  +            "minimum": 1
  +          }
  +        },
  +        "min_delay_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_qps": {
  +          "type": "number",
  +          "exclusiveMinimum": 0
  +        },
  +        "rate_jitter_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_attempts": {
  +          "type": "integer",
  +          "minimum": 1
  +        },
  +        "backoff_base_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "backoff_max_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "backoff_jitter_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_consecutive_failures": {
  +          "type": "integer",
  +          "minimum": 0
  +        },
  +        "cooldown_s": {
  +          "type": "number",
  +          "minimum": 0
  +        },
  +        "max_inflight_per_host": {
  +          "type": "integer",
  +          "minimum": 1
  +        }
  +      },
  +      "additionalProperties": false
  +    }
  +  },
  +  "additionalProperties": false
  +}
diff --cc scripts/k8s_render.py
index c5406ac,c5406ac,0000000..5fbd224
mode 100644,100644,000000..100644
--- a/scripts/k8s_render.py
+++ b/scripts/k8s_render.py
@@@@ -1,211 -1,211 -1,0 +1,212 @@@@
  +#!/usr/bin/env python3
  +from __future__ import annotations
  +
  +import argparse
  +import json
  +import os
  +import re
  +import shutil
  +import subprocess
  +import sys
  +from pathlib import Path
  +
  +REPO_ROOT = Path(__file__).resolve().parents[1]
  +BASE_DIR = REPO_ROOT / "ops" / "k8s" / "jobintel"
  +OVERLAY_DIRS = {
  +    "eks": REPO_ROOT / "ops" / "k8s" / "overlays" / "aws-eks",
  +    "aws-eks": REPO_ROOT / "ops" / "k8s" / "overlays" / "aws-eks",
  +    "eks-wrapper": REPO_ROOT / "ops" / "k8s" / "overlays" / "eks",
  +    "live": REPO_ROOT / "ops" / "k8s" / "overlays" / "live",
  +    "onprem": REPO_ROOT / "ops" / "k8s" / "jobintel" / "overlays" / "onprem",
  +    "onprem-wrapper": REPO_ROOT / "ops" / "k8s" / "overlays" / "onprem",
+++    "onprem-pi": REPO_ROOT / "ops" / "k8s" / "overlays" / "onprem-pi",
  +}
  +
  +REQUIRED_SECRET_KEYS = ["JOBINTEL_S3_BUCKET"]
  +OPTIONAL_SECRET_KEYS = [
  +    "AWS_ACCESS_KEY_ID",
  +    "AWS_SECRET_ACCESS_KEY",
  +    "AWS_SESSION_TOKEN",
  +    "AWS_PROFILE",
  +    "DISCORD_WEBHOOK_URL",
  +    "OPENAI_API_KEY",
  +]
  +
  +
  +def _render_manifest(path: Path) -> str:
  +    kubectl = shutil.which("kubectl")
  +    kustomize = shutil.which("kustomize")
  +    if kubectl:
  +        result = subprocess.run(
  +            [kubectl, "kustomize", str(path)],
  +            check=False,
  +            capture_output=True,
  +            text=True,
  +        )
  +    elif kustomize:
  +        result = subprocess.run(
  +            [kustomize, "build", str(path)],
  +            check=False,
  +            capture_output=True,
  +            text=True,
  +        )
  +    else:
  +        raise RuntimeError("kubectl or kustomize is required to render manifests")
  +    if result.returncode != 0:
  +        raise RuntimeError(result.stderr.strip() or "kustomize render failed")
  +    return result.stdout
  +
  +
  +def _collect_patch_paths(overlay_dir: Path) -> list[Path]:
  +    kustomization = overlay_dir / "kustomization.yaml"
  +    if not kustomization.exists():
  +        raise RuntimeError(f"overlay missing kustomization.yaml: {overlay_dir}")
  +    patches: list[Path] = []
  +    in_patches = False
  +    for line in kustomization.read_text(encoding="utf-8").splitlines():
  +        stripped = line.strip()
  +        if stripped.startswith("patchesStrategicMerge:"):
  +            in_patches = True
  +            continue
  +        if in_patches:
  +            if stripped.startswith("- "):
  +                patch = stripped[2:].strip()
  +                patches.append(overlay_dir / patch)
  +                continue
  +            if stripped and not line.startswith(" "):
  +                in_patches = False
  +    return patches
  +
  +
  +_PLACEHOLDER_RE = re.compile(r"\$\{([A-Z0-9_]+)\}")
  +
  +
  +def _substitute_placeholders(content: str, source_path: Path) -> str:
  +    missing: set[str] = set()
  +
  +    def _replace(match: re.Match[str]) -> str:
  +        key = match.group(1)
  +        value = os.getenv(key)
  +        if value is None or value.strip() == "":
  +            missing.add(key)
  +            return match.group(0)
  +        return value
  +
  +    rendered = _PLACEHOLDER_RE.sub(_replace, content)
  +    if missing:
  +        missing_list = ", ".join(sorted(missing))
  +        raise RuntimeError(f"missing env vars for {source_path}: {missing_list}")
  +    return rendered
  +
  +
  +def _render_with_overlays(overlays: list[str], image_override_arg: str | None = None) -> str:
  +    overlay_dirs = [OVERLAY_DIRS[name] for name in overlays]
  +    patch_paths: list[Path] = []
  +    for overlay_dir in overlay_dirs:
  +        patch_paths.extend(_collect_patch_paths(overlay_dir))
  +    patch_paths = sorted(patch_paths, key=lambda path: path.as_posix())
  +    if not patch_paths:
  +        return _render_manifest(BASE_DIR)
  +
  +    require_image = any(name in {"aws-eks", "eks"} for name in overlays)
  +    image_override = image_override_arg or (os.getenv("JOBINTEL_IMAGE") if require_image else None)
  +    if require_image and not image_override:
  +        raise RuntimeError("JOBINTEL_IMAGE is required when rendering aws-eks overlay")
  +
  +    import tempfile
  +
  +    with tempfile.TemporaryDirectory(dir=REPO_ROOT / "ops" / "k8s") as tmp_dir:
  +        tmp_path = Path(tmp_dir)
  +        rel_base = Path(os.path.relpath(BASE_DIR, tmp_path))
  +        rel_patches: list[Path] = []
  +        for patch in patch_paths:
  +            content = patch.read_text(encoding="utf-8")
  +            content = _substitute_placeholders(content, patch)
  +            patch_name = f"{patch.parent.name}__{patch.name}"
  +            tmp_patch = tmp_path / patch_name
  +            tmp_patch.write_text(content, encoding="utf-8")
  +            rel_patches.append(Path(os.path.relpath(tmp_patch, tmp_path)))
  +
  +        kustomization = [
  +            "apiVersion: kustomize.config.k8s.io/v1beta1",
  +            "kind: Kustomization",
  +            "resources:",
  +            f"  - {rel_base.as_posix()}",
  +            "patchesStrategicMerge:",
  +        ]
  +        for patch in rel_patches:
  +            kustomization.append(f"  - {patch.as_posix()}")
  +        if image_override:
  +            if ":" not in image_override:
  +                raise RuntimeError("JOBINTEL_IMAGE must include a tag (e.g. repo:tag)")
  +            image_repo, image_tag = image_override.rsplit(":", 1)
  +            kustomization.extend(
  +                [
  +                    "images:",
  +                    "  - name: ghcr.io/yourorg/jobintel",
  +                    f"    newName: {image_repo}",
  +                    f"    newTag: {image_tag}",
  +                ]
  +            )
  +        (tmp_path / "kustomization.yaml").write_text("\n".join(kustomization) + "\n", encoding="utf-8")
  +        return _render_manifest(tmp_path)
  +
  +
  +def _validate(manifest: str) -> int:
  +    kubectl = shutil.which("kubectl")
  +    if not kubectl:
  +        print("kubectl not found; skipping validation", file=sys.stderr)
  +        return 0
  +    result = subprocess.run(
  +        [kubectl, "apply", "--dry-run=client", "-f", "-"],
  +        input=manifest,
  +        text=True,
  +        check=False,
  +        capture_output=True,
  +    )
  +    if result.returncode != 0:
  +        sys.stderr.write(result.stderr)
  +        return result.returncode
  +    print(result.stdout.strip())
  +    return 0
  +
  +
  +def main(argv: list[str] | None = None) -> int:
  +    parser = argparse.ArgumentParser(description="Render JobIntel kustomize manifests")
  +    parser.add_argument(
  +        "--overlay",
  +        action="append",
  +        choices=sorted(OVERLAY_DIRS.keys()),
  +        help="Render one or more overlays (default: base). Repeat to stack.",
  +    )
  +    parser.add_argument(
  +        "--image",
  +        default=None,
  +        help="Override container image URI (e.g., <acct>.dkr.ecr.<region>.amazonaws.com/jobintel:<tag>).",
  +    )
  +    parser.add_argument("--validate", action="store_true", help="kubectl apply --dry-run=client")
  +    parser.add_argument("--secrets", action="store_true", help="Print required secret keys")
  +    args = parser.parse_args(argv)
  +
  +    overlays = args.overlay or []
  +    if overlays:
  +        manifest = _render_with_overlays(overlays, image_override_arg=args.image)
  +    else:
  +        manifest = _render_manifest(BASE_DIR)
  +    print(manifest)
  +
  +    if args.secrets:
  +        payload = {
  +            "required": REQUIRED_SECRET_KEYS,
  +            "optional": OPTIONAL_SECRET_KEYS,
  +        }
  +        print("---")
  +        print(json.dumps(payload, sort_keys=True))
  +
  +    if args.validate:
  +        return _validate(manifest)
  +    return 0
  +
  +
  +if __name__ == "__main__":
  +    raise SystemExit(main())
diff --cc scripts/run_daily.py
index a39490e,a39490e,0000000..f739997
mode 100644,100644,000000..100644
--- a/scripts/run_daily.py
+++ b/scripts/run_daily.py
@@@@ -1,4512 -1,4512 -1,0 +1,4610 @@@@
  +#!/usr/bin/env python3
  +"""
  +Daily runner: pipeline -> score -> diff -> optional Discord alert.
  +
  +State files live in data/state/.
  +Designed to run locally now and on AWS later (cron/EventBridge).
  +
  +Examples:
  +  python scripts/run_daily.py --profile cs --us_only --no_post
  +  python scripts/run_daily.py --profiles cs,tam,se --us_only --min_alert_score 85 --no_post
  +  DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/..." python scripts/run_daily.py --profile cs --us_only
  +"""
  +
  +from __future__ import annotations
  +
  +try:
  +    import _bootstrap  # type: ignore
  +except ModuleNotFoundError:
  +    from scripts import _bootstrap  # noqa: F401
  +
  +import argparse
  +import atexit
  +import importlib
  +import json
  +import logging
  +import os
  +import platform
  +import runpy
  +import shutil
  +import subprocess
  +import sys
  +import tempfile
  +import time
  +import urllib.error
  +import urllib.request
  +from datetime import datetime, timezone
  +from pathlib import Path
  +from typing import Any, Dict, List, Optional, Tuple
  +
+++from ji_engine.artifacts.model_v2 import (
+++    REPLAY_SAFE_FILENAME,
+++    REPLAY_SAFE_SCHEMA_VERSION,
+++    UI_SAFE_FILENAME,
+++    UI_SAFE_SCHEMA_VERSION,
+++    build_replay_safe_artifact,
+++    build_ui_safe_artifact,
+++)
  +from ji_engine.config import (
  +    DATA_DIR,
  +    DEFAULT_CANDIDATE_ID,
  +    ENRICHED_JOBS_JSON,
  +    HISTORY_DIR,
  +    LABELED_JOBS_JSON,
  +    LOCK_PATH,
  +    RAW_JOBS_JSON,
  +    REPO_ROOT,
  +    RUN_METADATA_DIR,
  +    SNAPSHOT_DIR,
  +    STATE_DIR,
  +    USER_STATE_DIR,
  +    ensure_dirs,
  +    ranked_families_json,
  +    ranked_jobs_csv,
  +    ranked_jobs_json,
  +    sanitize_candidate_id,
  +    state_last_ranked,
  +)
  +from ji_engine.config import (
  +    shortlist_md as shortlist_md_path,
  +)
  +from ji_engine.history_retention import update_history_retention, write_history_run_artifacts
  +from ji_engine.providers.registry import load_providers_config, resolve_provider_ids
  +from ji_engine.semantic.core import DEFAULT_SEMANTIC_MODEL_ID, EMBEDDING_BACKEND_VERSION
  +from ji_engine.semantic.step import finalize_semantic_artifacts, semantic_score_artifact_path
  +from ji_engine.utils.atomic_write import atomic_write_text
  +from ji_engine.utils.content_fingerprint import content_fingerprint
  +from ji_engine.utils.diff_report import build_diff_markdown, build_diff_report
  +from ji_engine.utils.dotenv import load_dotenv
  +from ji_engine.utils.job_identity import job_identity
-- from ji_engine.utils.redaction import scan_json_for_secrets, scan_text_for_secrets
+++from ji_engine.utils.redaction import (
+++    scan_json_for_raw_jd,
+++    scan_json_for_secrets,
+++    scan_text_for_raw_jd,
+++    scan_text_for_secrets,
+++)
  +from ji_engine.utils.time import utc_now_naive, utc_now_z
  +from ji_engine.utils.user_state import load_user_state_checked, normalize_user_status
  +from ji_engine.utils.verification import (
  +    build_verifiable_artifacts,
  +    compute_sha256_bytes,
  +    compute_sha256_file,
  +)
  +from jobintel.alerts import (
  +    build_last_seen,
  +    compute_alerts,
  +    load_last_seen,
  +    resolve_score_delta,
  +    write_alerts,
  +    write_last_seen,
  +)
  +from jobintel.aws_runs import (
  +    BaselineInfo,
  +    download_baseline_ranked,
  +    get_most_recent_successful_run_id_before,
  +    parse_pointer,
  +    read_last_success_state,
  +    read_provider_last_success_state,
  +    s3_enabled,
  +)
  +from jobintel.delta import compute_delta
  +from jobintel.discord_notify import build_run_summary_message, post_discord, resolve_webhook
  +
+++try:
+++    from scripts.schema_validate import resolve_named_schema_path, validate_payload
+++except ModuleNotFoundError:  # pragma: no cover - fallback for direct script execution
+++    from schema_validate import resolve_named_schema_path, validate_payload  # type: ignore
+++
  +OUTPUT_DIR = DATA_DIR
  +
  +try:
  +    import scripts.publish_s3 as publish_s3  # type: ignore
  +except ModuleNotFoundError:  # pragma: no cover - fallback for direct script execution
  +    import importlib.util
  +
  +    _spec = importlib.util.spec_from_file_location("publish_s3", REPO_ROOT / "scripts" / "publish_s3.py")
  +    if _spec and _spec.loader:
  +        publish_s3 = importlib.util.module_from_spec(_spec)
  +        _spec.loader.exec_module(publish_s3)
  +    else:
  +        raise
  +
  +
  +def _unavailable_summary_for(provider: str) -> str:
  +    enriched_path = _provider_enriched_jobs_json(provider)
  +    try:
  +        data = json.loads(enriched_path.read_text(encoding="utf-8"))
  +    except Exception:
  +        return ""
  +    reasons: Dict[str, int] = {}
  +    for j in data if isinstance(data, list) else []:
  +        if j.get("enrich_status") == "unavailable":
  +            r = j.get("enrich_reason") or "unavailable"
  +            reasons[r] = reasons.get(r, 0) + 1
  +    if not reasons:
  +        return ""
  +    return ", ".join(f"{k}={v}" for k, v in sorted(reasons.items()))
  +
  +
  +def _unavailable_summary() -> str:
  +    return _unavailable_summary_for("openai")
  +
  +
  +logger = logging.getLogger(__name__)
  +USE_SUBPROCESS = True
  +LAST_RUN_JSON = STATE_DIR / "last_run.json"
  +LAST_SUCCESS_JSON = STATE_DIR / "last_success.json"
  +RUN_REPORT_SCHEMA_VERSION = 1
  +PROOFS_DIR = STATE_DIR / "proofs"
  +PROOF_RECEIPT_SCHEMA_VERSION = 1
  +try:
  +    CANDIDATE_ID = sanitize_candidate_id(os.environ.get("JOBINTEL_CANDIDATE_ID", DEFAULT_CANDIDATE_ID))
  +except ValueError as exc:
  +    raise SystemExit(f"invalid JOBINTEL_CANDIDATE_ID: {exc}")
  +
  +
  +def _flush_logging() -> None:
  +    for handler in logging.getLogger().handlers:
  +        handler.flush()
  +
  +
  +def _warn_if_not_user_writable(paths: List[Path], *, context: str) -> None:
  +    """
  +    Best-effort warning: if a path exists but is not writable by the current user,
  +    log a helpful warning (common when artifacts were created as root in Docker).
  +
  +    This is intentionally non-fatal and cross-platform.
  +    """
  +    non_writable: List[Path] = []
  +    for p in paths:
  +        try:
  +            if not p.exists():
  +                continue
  +            if not os.access(str(p), os.W_OK):
  +                non_writable.append(p)
  +        except Exception:
  +            # Never fail the run due to a permissions check.
  +            continue
  +
  +    if not non_writable:
  +        return
  +
  +    hint = (
  +        "Some artifacts exist but are not writable by your current user. "
  +        "This often happens if you previously ran the pipeline in Docker as root. "
  +        "Fix ownership/permissions and re-run."
  +    )
  +    if os.name == "posix":
  +        hint += " Example fix: `sudo chown -R $(id -u):$(id -g) data state`"
  +
  +    logger.warning(
  +        "Non-writable pipeline artifacts detected (%s): %s. %s",
  +        context,
  +        ", ".join(str(p) for p in non_writable[:12]) + (" ..." if len(non_writable) > 12 else ""),
  +        hint,
  +    )
  +
  +
  +class JsonFormatter(logging.Formatter):
  +    def format(self, record: logging.LogRecord) -> str:  # type: ignore[override]
  +        payload = {
  +            "time": utc_now_naive().isoformat(),
  +            "level": record.levelname,
  +            "msg": record.getMessage(),
  +        }
  +        return json.dumps(payload, ensure_ascii=False)
  +
  +
  +class StructuredLogFormatter(logging.Formatter):
  +    """Deterministic structured formatter for per-run log files."""
  +
  +    def format(self, record: logging.LogRecord) -> str:  # type: ignore[override]
  +        payload = {
  +            "level": record.levelname,
  +            "logger": record.name,
  +            "msg": record.getMessage(),
  +        }
  +        return json.dumps(payload, ensure_ascii=False, sort_keys=True)
  +
  +
  +load_dotenv()  # loads .env if present; won't override exported env vars
  +
  +
  +def _utcnow_iso() -> str:
  +    return utc_now_z(seconds_precision=True)
  +
  +
  +def _pid_alive(pid: int) -> bool:
  +    """Return True if PID exists (best-effort)."""
  +    try:
  +        os.kill(pid, 0)  # does not kill; just checks
  +        return True
  +    except ProcessLookupError:
  +        return False
  +    except PermissionError:
  +        # Process exists but we don't have permission to signal it.
  +        return True
  +
  +
  +def _acquire_lock(timeout_sec: int = 0) -> None:
  +    """
  +    Prevent overlapping runs.
  +    Creates a lock file with the current PID. If it already exists:
  +      - if timeout_sec == 0: exit immediately
  +      - else: wait up to timeout_sec
  +    Also detects stale locks (PID no longer running).
  +    """
  +    start = time.time()
  +    pid = os.getpid()
  +
  +    while True:
  +        try:
  +            # exclusive create
  +            fd = os.open(str(LOCK_PATH), os.O_CREAT | os.O_EXCL | os.O_WRONLY)
  +            with os.fdopen(fd, "w") as f:
  +                f.write(str(pid))
  +            break
  +        except FileExistsError:
  +            # stale-lock detection
  +            try:
  +                existing_pid = int(LOCK_PATH.read_text(encoding="utf-8").strip() or "0")
  +            except Exception:
  +                existing_pid = 0
  +
  +            if existing_pid and not _pid_alive(existing_pid):
  +                logger.warning(f"⚠️ Stale lock detected (pid={existing_pid}). Removing {LOCK_PATH}.")
  +                try:
  +                    LOCK_PATH.unlink(missing_ok=True)
  +                except Exception:
  +                    pass
  +                continue
  +
  +            if timeout_sec == 0:
  +                raise SystemExit(f"Another run is already in progress (lock: {LOCK_PATH}).")
  +            if time.time() - start > timeout_sec:
  +                raise SystemExit(f"Timed out waiting for lock: {LOCK_PATH}.")
  +            time.sleep(2)
  +
  +    def _cleanup() -> None:
  +        try:
  +            LOCK_PATH.unlink(missing_ok=True)
  +        except Exception:
  +            pass
  +
  +    atexit.register(_cleanup)
  +
  +
  +def _run(cmd: List[str], *, stage: str) -> None:
  +    logger.info("\n$ " + " ".join(cmd))
  +    if USE_SUBPROCESS:
  +        result = subprocess.run(
  +            cmd,
  +            cwd=str(REPO_ROOT),
  +            text=True,
  +            capture_output=True,
  +        )
  +
  +        stdout_tail = (result.stdout or "")[-4000:]
  +        stderr_tail = (result.stderr or "")[-4000:]
  +
  +        if stdout_tail:
  +            logger.info(stdout_tail.rstrip())
  +        if stderr_tail:
  +            logger.info(stderr_tail.rstrip())
  +        _flush_logging()
  +
  +        if result.returncode != 0:
  +            raise subprocess.CalledProcessError(
  +                result.returncode,
  +                cmd,
  +                output=stdout_tail,
  +                stderr=stderr_tail,
  +            )
  +        return
  +
  +    # In-process fallback: attempt to run module/script directly
  +    argv = cmd[1:] if cmd and cmd[0] == sys.executable else cmd
  +    if argv and argv[0] == "-m":
  +        module_name = argv[1]
  +        args = argv[2:]
  +        old_argv = sys.argv
  +        sys.argv = [module_name, *args]
  +        try:
  +            mod = importlib.import_module(module_name)
  +            if hasattr(mod, "main"):
  +                rc = mod.main()
  +                if rc not in (None, 0):
  +                    raise SystemExit(rc)
  +            else:
  +                raise SystemExit(f"Module {module_name} has no main()")
  +        except SystemExit as e:
  +            if _normalize_exit_code(e.code) != 0:
  +                raise
  +        finally:
  +            sys.argv = old_argv
  +        _flush_logging()
  +    else:
  +        script_path = argv[0]
  +        args = argv[1:]
  +        old_argv = sys.argv
  +        sys.argv = [script_path, *args]
  +        try:
  +            runpy.run_path(script_path, run_name="__main__")
  +        except SystemExit as e:
  +            if _normalize_exit_code(e.code) != 0:
  +                raise
  +        finally:
  +            sys.argv = old_argv
  +        _flush_logging()
  +
  +
  +def _read_json(path: Path) -> Any:
  +    return json.loads(path.read_text(encoding="utf-8"))
  +
  +
  +def _write_json(path: Path, obj: Any) -> None:
  +    path.parent.mkdir(parents=True, exist_ok=True)
  +    _redaction_guard_json(path, obj)
  +    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
  +
  +
  +def _write_canonical_json(path: Path, obj: Any) -> None:
  +    path.parent.mkdir(parents=True, exist_ok=True)
  +    _redaction_guard_json(path, obj)
  +    payload = json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(",", ":")) + "\n"
  +    path.write_text(payload, encoding="utf-8")
  +
  +
  +def _redaction_enforce_enabled() -> bool:
  +    return os.environ.get("REDACTION_ENFORCE", "").strip() == "1"
  +
  +
  +def _redaction_guard_text(path: Path, text: str) -> None:
--     findings = scan_text_for_secrets(text)
+++    findings = scan_text_for_secrets(text) + scan_text_for_raw_jd(text)
  +    if not findings:
  +        return
  +    summary = ", ".join(sorted({f"{item.pattern}@{item.location}" for item in findings}))
  +    msg = f"Potential secret-like content detected for {path}: {summary}"
  +    if _redaction_enforce_enabled():
  +        raise RuntimeError(msg)
  +    logger.warning("%s (set REDACTION_ENFORCE=1 to fail closed)", msg)
  +
  +
  +def _redaction_guard_json(path: Path, payload: Any) -> None:
--     findings = scan_json_for_secrets(payload)
+++    findings = scan_json_for_secrets(payload) + scan_json_for_raw_jd(payload)
  +    if not findings:
  +        return
  +    summary = ", ".join(sorted({f"{item.pattern}@{item.location}" for item in findings}))
  +    msg = f"Potential secret-like JSON content detected for {path}: {summary}"
  +    if _redaction_enforce_enabled():
  +        raise RuntimeError(msg)
  +    logger.warning("%s (set REDACTION_ENFORCE=1 to fail closed)", msg)
  +
  +
  +_PROOF_PROVENANCE_FIELDS = [
  +    "provider_id",
  +    "mode",
  +    "scrape_mode",
  +    "live_attempted",
  +    "live_result",
  +    "snapshot_used",
  +    "parsed_job_count",
  +    "snapshot_baseline_count",
  +    "live_http_status",
  +    "live_status_code",
  +    "live_error_type",
  +    "live_error_reason",
  +    "live_unavailable_reason",
  +    "availability",
  +    "unavailable_reason",
  +    "attempts_made",
  +    "rate_limit_min_delay_s",
  +    "rate_limit_jitter_s",
  +    "max_attempts",
  +    "backoff_base_s",
  +    "backoff_max_s",
  +    "backoff_jitter_s",
  +    "circuit_breaker_threshold",
  +    "circuit_breaker_cooldown_s",
  +    "policy_snapshot",
  +    "chaos_mode_enabled",
  +    "chaos_triggered",
  +    "robots_url",
  +    "robots_fetched",
  +    "robots_status",
  +    "robots_allowed",
  +    "allowlist_allowed",
  +    "robots_final_allowed",
  +    "robots_reason",
  +    "robots_user_agent",
  +]
  +
  +
  +def _build_proof_receipt(
  +    run_report: Dict[str, Any],
  +    *,
  +    run_report_path: Path,
  +    s3_meta: Dict[str, Any],
  +    publish_section: Dict[str, Any],
  +) -> Dict[str, Any]:
  +    providers = run_report.get("providers") or []
  +    profiles = run_report.get("profiles") or []
  +    provenance = run_report.get("provenance_by_provider") or {}
  +    proof_provenance: Dict[str, Dict[str, Any]] = {}
  +    for provider in providers or list(provenance.keys()):
  +        meta = provenance.get(provider) or {}
  +        proof_provenance[provider] = {key: meta.get(key) for key in _PROOF_PROVENANCE_FIELDS if key in meta}
  +    pointer_write = publish_section.get("pointer_write") or {}
  +    return {
  +        "proof_receipt_schema_version": PROOF_RECEIPT_SCHEMA_VERSION,
  +        "run_id": run_report.get("run_id"),
  +        "timestamp": run_report.get("timestamp"),
  +        "status": run_report.get("status"),
  +        "providers": providers,
  +        "profiles": profiles,
  +        "run_report_path": str(run_report_path),
  +        "publish": {
  +            "enabled": publish_section.get("enabled"),
  +            "required": publish_section.get("required"),
  +            "bucket": publish_section.get("bucket"),
  +            "prefix": publish_section.get("prefix"),
  +            "s3_status": s3_meta.get("status") if isinstance(s3_meta, dict) else None,
  +            "s3_reason": s3_meta.get("reason") if isinstance(s3_meta, dict) else None,
  +            "pointer_global": pointer_write.get("global"),
  +            "pointer_profiles": pointer_write.get("provider_profile"),
  +            "pointer_error": pointer_write.get("error"),
  +        },
  +        "provenance": proof_provenance,
  +    }
  +
  +
  +def _write_proof_receipt(
  +    run_report_path: Path,
  +    run_report: Dict[str, Any],
  +    *,
  +    s3_meta: Dict[str, Any],
  +    publish_section: Dict[str, Any],
  +) -> Optional[Path]:
  +    run_id = run_report.get("run_id")
  +    if not run_id:
  +        return None
  +    proof = _build_proof_receipt(
  +        run_report,
  +        run_report_path=run_report_path,
  +        s3_meta=s3_meta,
  +        publish_section=publish_section,
  +    )
  +    proof_path = PROOFS_DIR / f"{run_id}.json"
  +    _write_canonical_json(proof_path, proof)
  +    return proof_path
  +
  +
  +def _update_run_metadata_s3(path: Path, s3_meta: Dict[str, Any]) -> None:
  +    if not path.exists():
  +        return
  +    payload = json.loads(path.read_text(encoding="utf-8"))
  +    if not isinstance(payload, dict):
  +        return
  +    payload["s3_bucket"] = s3_meta.get("bucket")
  +    payload["s3_prefixes"] = s3_meta.get("prefixes")
  +    payload["uploaded_files_count"] = s3_meta.get("uploaded_files_count")
  +    payload["dashboard_url"] = s3_meta.get("dashboard_url")
  +    _write_json(path, payload)
  +
  +
  +def _update_run_metadata_publish(
  +    path: Path,
  +    publish_section: Dict[str, Any],
  +    *,
  +    success_override: Optional[bool] = None,
  +    status_override: Optional[str] = None,
  +) -> None:
  +    if not path.exists():
  +        return
  +    payload = json.loads(path.read_text(encoding="utf-8"))
  +    if not isinstance(payload, dict):
  +        return
  +    payload["publish"] = publish_section
  +    if success_override is not None:
  +        payload["success"] = success_override
  +    if status_override is not None:
  +        payload["status"] = status_override
  +    _write_json(path, payload)
  +
  +
+++def _update_run_metadata_artifact_model(path: Path, artifact_model: Dict[str, Any]) -> None:
+++    if not path.exists():
+++        return
+++    payload = json.loads(path.read_text(encoding="utf-8"))
+++    if not isinstance(payload, dict):
+++        return
+++    payload["artifact_model"] = artifact_model
+++    _write_json(path, payload)
+++
+++
  +def _pointer_write_ok(pointer_write: Any) -> bool:
  +    if not isinstance(pointer_write, dict):
  +        return False
  +    if pointer_write.get("global") != "ok":
  +        return False
  +    provider_status = pointer_write.get("provider_profile", {})
  +    if isinstance(provider_status, dict):
  +        return all(status == "ok" for status in provider_status.values())
  +    return False
  +
  +
  +def _build_publish_section(
  +    *,
  +    s3_meta: Dict[str, Any],
  +    enabled: bool,
  +    required: bool,
  +    bucket: Optional[str],
  +    prefix: Optional[str],
  +    skip_reason: Optional[str] = None,
  +) -> Dict[str, Any]:
  +    pointer_write = s3_meta.get("pointer_write") if isinstance(s3_meta, dict) else None
  +    if not pointer_write:
  +        pointer_write = {"global": "disabled", "provider_profile": {}, "error": None}
  +    return {
  +        "enabled": bool(enabled),
  +        "required": bool(required),
  +        "bucket": bucket,
  +        "prefix": prefix,
  +        "pointer_write": pointer_write,
  +        "skip_reason": skip_reason,
  +    }
  +
  +
  +def _publish_contract_failed(publish_section: Dict[str, Any]) -> bool:
  +    if not publish_section.get("enabled"):
  +        return False
  +    if not publish_section.get("required"):
  +        return False
  +    return not _pointer_write_ok(publish_section.get("pointer_write"))
  +
  +
  +def _resolve_publish_state(
  +    publish_requested: bool, bucket: str, require_s3: bool = False
  +) -> Tuple[bool, bool, Optional[str]]:
  +    if not publish_requested:
  +        return False, False, None
  +    if not bucket:
  +        if require_s3:
  +            return False, True, "missing_bucket_required"
  +        return False, False, "skipped_missing_bucket"
  +    return True, require_s3, None
  +
  +
  +def _score_meta_path(ranked_json: Path) -> Path:
  +    return ranked_json.with_suffix(".score_meta.json")
  +
  +
  +def _scrape_meta_path(provider: str) -> Path:
  +    return OUTPUT_DIR / f"{provider}_scrape_meta.json"
  +
  +
  +def _load_scrape_provenance(providers: List[str]) -> Dict[str, Dict[str, Any]]:
  +    out: Dict[str, Dict[str, Any]] = {}
  +    for provider in providers:
  +        meta_path = _scrape_meta_path(provider)
  +        if not meta_path.exists() and OUTPUT_DIR != DATA_DIR:
  +            meta_path = DATA_DIR / f"{provider}_scrape_meta.json"
  +        if not meta_path.exists():
  +            continue
  +        try:
  +            meta = _read_json(meta_path)
  +        except Exception:
  +            continue
  +        if isinstance(meta, dict):
  +            out[provider] = meta
  +    return out
  +
  +
  +def _get_env_float(name: str, default: float) -> float:
  +    raw = os.environ.get(name)
  +    if raw is None:
  +        return default
  +    try:
  +        return float(raw)
  +    except ValueError:
  +        return default
  +
  +
  +def _get_env_int(name: str, default: int) -> int:
  +    raw = os.environ.get(name)
  +    if raw is None:
  +        return default
  +    try:
  +        return int(raw)
  +    except ValueError:
  +        return default
  +
  +
  +def _provider_policy_thresholds() -> Dict[str, float]:
  +    return {
  +        "error_rate_max": _get_env_float("JOBINTEL_PROVIDER_ERROR_RATE_MAX", 0.25),
  +        "min_jobs": float(_get_env_int("JOBINTEL_PROVIDER_MIN_JOBS", 1)),
  +        "min_snapshot_ratio": _get_env_float("JOBINTEL_PROVIDER_MIN_SNAPSHOT_RATIO", 0.2),
  +    }
  +
  +
  +def _load_enrich_stats(enriched_path: Path) -> Dict[str, int]:
  +    stats = {"total": 0, "enriched": 0, "unavailable": 0, "failed": 0}
  +    if not enriched_path.exists():
  +        return stats
  +    try:
  +        payload = _read_json(enriched_path)
  +    except Exception:
  +        return stats
  +    if not isinstance(payload, list):
  +        return stats
  +    for job in payload:
  +        if not isinstance(job, dict):
  +            continue
  +        status = job.get("enrich_status")
  +        if status == "enriched":
  +            stats["enriched"] += 1
  +            stats["total"] += 1
  +        elif status == "unavailable":
  +            stats["unavailable"] += 1
  +            stats["total"] += 1
  +        elif status == "failed":
  +            stats["failed"] += 1
  +            stats["total"] += 1
  +        else:
  +            if job.get("jd_text"):
  +                stats["enriched"] += 1
  +                stats["total"] += 1
  +    return stats
  +
  +
  +def _evaluate_provider_policy(
  +    provider: str,
  +    meta: Dict[str, Any],
  +    *,
  +    enriched_path: Optional[Path],
  +    thresholds: Dict[str, float],
  +    no_enrich: bool,
  +) -> Tuple[bool, str, str]:
  +    scrape_mode = (meta.get("scrape_mode") or "").lower()
  +    parsed_jobs = int(meta.get("parsed_job_count") or 0)
  +    baseline = meta.get("snapshot_baseline_count")
  +    baseline_count = int(baseline) if isinstance(baseline, (int, float)) else None
  +
  +    enrich_stats = _load_enrich_stats(enriched_path) if enriched_path and not no_enrich else {"total": 0}
  +    error_count = int(enrich_stats.get("unavailable", 0)) + int(enrich_stats.get("failed", 0))
  +    total = int(enrich_stats.get("total", 0))
  +    error_rate = (error_count / total) if total > 0 else 0.0
  +
  +    decision = "ok"
  +    reason_parts: List[str] = []
  +
  +    if scrape_mode == "live":
  +        if parsed_jobs < int(thresholds["min_jobs"]):
  +            decision = "fail"
  +            reason_parts.append(f"parsed_jobs<{int(thresholds['min_jobs'])}")
  +        if baseline_count is not None and baseline_count > 0:
  +            min_ratio = thresholds["min_snapshot_ratio"]
  +            if parsed_jobs < int(baseline_count * min_ratio):
  +                decision = "fail"
  +                reason_parts.append("parsed_jobs_below_snapshot_ratio")
  +        if total > 0 and error_rate > thresholds["error_rate_max"]:
  +            decision = "fail"
  +            reason_parts.append("enrich_error_rate_exceeded")
  +
  +    reason = ", ".join(reason_parts) if reason_parts else "ok"
  +    policy = {
  +        "decision": decision,
  +        "reason": reason,
  +        "scrape_mode": scrape_mode,
  +        "parsed_job_count": parsed_jobs,
  +        "snapshot_baseline_count": baseline_count,
  +        "error_rate": round(error_rate, 4),
  +        "error_rate_max": thresholds["error_rate_max"],
  +        "min_jobs": int(thresholds["min_jobs"]),
  +        "min_snapshot_ratio": thresholds["min_snapshot_ratio"],
  +        "enrich_stats": enrich_stats,
  +    }
  +    meta["failure_policy"] = policy
  +    line = f"Provider policy ({provider}): {decision} (parsed={parsed_jobs}, error_rate={round(error_rate, 3)})"
  +    return decision == "fail", reason, line
  +
  +
  +def _ecs_task_arn_from_metadata() -> Optional[str]:
  +    metadata_env = ("ECS_CONTAINER_METADATA_URI_V4", "ECS_CONTAINER_METADATA_URI")
  +    for key in metadata_env:
  +        base_uri = os.environ.get(key)
  +        if not base_uri:
  +            continue
  +        task_uri = f"{base_uri.rstrip('/')}/task"
  +        try:
  +            with urllib.request.urlopen(task_uri, timeout=2) as response:
  +                payload = json.loads(response.read().decode("utf-8"))
  +        except (urllib.error.URLError, json.JSONDecodeError, ValueError):
  +            continue
  +        if isinstance(payload, dict):
  +            task_arn = payload.get("TaskARN") or payload.get("TaskArn")
  +            if isinstance(task_arn, str) and task_arn:
  +                return task_arn
  +    return None
  +
  +
  +def _resolve_ecs_task_arn() -> str:
  +    env_value = os.environ.get("JOBINTEL_ECS_TASK_ARN") or os.environ.get("ECS_TASK_ARN")
  +    if env_value and env_value not in {"unknown", "metadata", "from_metadata"}:
  +        return env_value
  +    metadata_value = _ecs_task_arn_from_metadata()
  +    if metadata_value:
  +        return metadata_value
  +    if env_value:
  +        return env_value
  +    return "unknown"
  +
  +
  +def _apply_score_fallback_metadata(selection: Dict[str, Any], ranked_json: Path) -> None:
  +    meta_path = _score_meta_path(ranked_json)
  +    if not meta_path.exists():
  +        return
  +    try:
  +        meta = _read_json(meta_path)
  +    except Exception:
  +        return
  +    if isinstance(meta, dict) and meta.get("us_only_fallback"):
  +        selection["us_only_fallback"] = meta["us_only_fallback"]
  +
  +
  +def _run_metadata_path(run_id: str) -> Path:
  +    safe_id = _sanitize_run_id(run_id)
  +    return RUN_METADATA_DIR / f"{safe_id}.json"
  +
  +
  +def _sanitize_run_id(run_id: str) -> str:
  +    return run_id.replace(":", "").replace("-", "").replace(".", "")
  +
  +
  +def _resolve_providers(args: argparse.Namespace) -> List[str]:
  +    providers_cfg = load_providers_config(Path(args.providers_config))
  +    try:
  +        return resolve_provider_ids(args.providers, providers_cfg, default_provider="openai")
  +    except ValueError as exc:
  +        logger.error(str(exc))
  +        raise SystemExit(2) from exc
  +
  +
  +def _resolve_output_dir() -> Path:
  +    env_value = os.environ.get("JOBINTEL_OUTPUT_DIR")
  +    if env_value:
  +        output_dir = Path(env_value).expanduser()
  +    else:
  +        output_dir = DATA_DIR / "ashby_cache"
  +    output_dir.mkdir(parents=True, exist_ok=True)
  +    return output_dir
  +
  +
  +def _provider_raw_jobs_json(provider: str) -> Path:
  +    if provider == "openai":
  +        return RAW_JOBS_JSON
  +    return OUTPUT_DIR / f"{provider}_raw_jobs.json"
  +
  +
  +def _provider_labeled_jobs_json(provider: str) -> Path:
  +    if provider == "openai":
  +        return LABELED_JOBS_JSON
  +    return OUTPUT_DIR / f"{provider}_labeled_jobs.json"
  +
  +
  +def _provider_enriched_jobs_json(provider: str) -> Path:
  +    if provider == "openai":
  +        return ENRICHED_JOBS_JSON
  +    return OUTPUT_DIR / f"{provider}_enriched_jobs.json"
  +
  +
  +def _alerts_paths(provider: str, profile: str) -> Tuple[Path, Path]:
  +    return (
  +        OUTPUT_DIR / f"{provider}_alerts.{profile}.json",
  +        OUTPUT_DIR / f"{provider}_alerts.{profile}.md",
  +    )
  +
  +
  +def _last_seen_path(provider: str, profile: str) -> Path:
  +    return STATE_DIR / "runs" / "last_seen" / f"{provider}.{profile}.json"
  +
  +
  +def _provider_ai_jobs_json(provider: str) -> Path:
  +    if provider == "openai":
  +        return ENRICHED_JOBS_JSON.with_name("openai_enriched_jobs_ai.json")
  +    return OUTPUT_DIR / f"{provider}_enriched_jobs_ai.json"
  +
  +
  +def _provider_ranked_jobs_json(provider: str, profile: str) -> Path:
  +    if provider == "openai":
  +        return OUTPUT_DIR / ranked_jobs_json(profile).name
  +    return OUTPUT_DIR / f"{provider}_ranked_jobs.{profile}.json"
  +
  +
  +def _provider_ranked_jobs_csv(provider: str, profile: str) -> Path:
  +    if provider == "openai":
  +        return OUTPUT_DIR / ranked_jobs_csv(profile).name
  +    return OUTPUT_DIR / f"{provider}_ranked_jobs.{profile}.csv"
  +
  +
  +def _provider_ranked_families_json(provider: str, profile: str) -> Path:
  +    if provider == "openai":
  +        return OUTPUT_DIR / ranked_families_json(profile).name
  +    return OUTPUT_DIR / f"{provider}_ranked_families.{profile}.json"
  +
  +
  +def _provider_shortlist_md(provider: str, profile: str) -> Path:
  +    if provider == "openai":
  +        return OUTPUT_DIR / shortlist_md_path(profile).name
  +    return OUTPUT_DIR / f"{provider}_shortlist.{profile}.md"
  +
  +
  +def _provider_top_md(provider: str, profile: str) -> Path:
  +    return OUTPUT_DIR / f"{provider}_top.{profile}.md"
  +
  +
  +def _provider_diff_paths(provider: str, profile: str) -> Tuple[Path, Path]:
  +    return (
  +        OUTPUT_DIR / f"{provider}_diff.{profile}.json",
  +        OUTPUT_DIR / f"{provider}_diff.{profile}.md",
  +    )
  +
  +
  +def _state_last_ranked(provider: str, profile: str) -> Path:
  +    if provider == "openai":
  +        return state_last_ranked(profile)
  +    return STATE_DIR / f"last_ranked.{provider}.{profile}.json"
  +
  +
  +def _local_last_success_pointer_paths(provider: str, profile: str) -> List[Path]:
  +    return [
  +        STATE_DIR / provider / profile / "last_success.json",
  +        STATE_DIR / "last_success.json",
  +    ]
  +
  +
  +def _resolve_local_last_success_ranked(provider: str, profile: str, current_run_id: str) -> Optional[Path]:
  +    for pointer_path in _local_last_success_pointer_paths(provider, profile):
  +        if not pointer_path.exists():
  +            continue
  +        try:
  +            payload = json.loads(pointer_path.read_text(encoding="utf-8"))
  +        except Exception:
  +            continue
  +        run_id = parse_pointer(payload) if isinstance(payload, dict) else None
  +        if not run_id or run_id == current_run_id:
  +            continue
  +        run_dir = _run_registry_dir(run_id)
  +        ranked_path = run_dir / provider / profile / f"{provider}_ranked_jobs.{profile}.json"
  +        if ranked_path.exists():
  +            return ranked_path
  +    return None
  +
  +
  +def _resolve_latest_run_ranked(provider: str, profile: str, current_run_id: str) -> Optional[Path]:
  +    if not RUN_METADATA_DIR.exists():
  +        return None
  +    candidates: List[Tuple[float, str, Path]] = []
  +    current_name = _sanitize_run_id(current_run_id)
  +    for run_dir in RUN_METADATA_DIR.iterdir():
  +        if not run_dir.is_dir() or run_dir.name == current_name:
  +            continue
  +        ranked_path = run_dir / provider / profile / f"{provider}_ranked_jobs.{profile}.json"
  +        if not ranked_path.exists():
  +            continue
  +        report_path = run_dir / "run_report.json"
  +        try:
  +            mtime = report_path.stat().st_mtime if report_path.exists() else run_dir.stat().st_mtime
  +        except OSError:
  +            continue
  +        candidates.append((mtime, run_dir.name, ranked_path))
  +    if not candidates:
  +        return None
  +    candidates.sort(key=lambda item: (item[0], item[1]))
  +    return candidates[-1][2]
  +
  +
  +def _history_run_dir(run_id: str, profile: str, provider: Optional[str] = None) -> Path:
  +    run_date = run_id.split("T")[0]
  +    sanitized = _sanitize_run_id(run_id)
  +    if provider and provider != "openai":
  +        return HISTORY_DIR / run_date / sanitized / provider / profile
  +    return HISTORY_DIR / run_date / sanitized / profile
  +
  +
  +def _latest_profile_dir(profile: str, provider: Optional[str] = None) -> Path:
  +    if provider and provider != "openai":
  +        return HISTORY_DIR / "latest" / provider / profile
  +    return HISTORY_DIR / "latest" / profile
  +
  +
  +def _copy_artifact(src: Path, dest: Path) -> None:
  +    if not src.exists():
  +        return
  +    dest.parent.mkdir(parents=True, exist_ok=True)
  +    shutil.copy2(src, dest)
  +
  +
  +def _atomic_copy(src: Path, dest: Path) -> None:
  +    dest.parent.mkdir(parents=True, exist_ok=True)
  +    tmp = dest.with_name(dest.name + ".tmp")
  +    try:
  +        shutil.copy2(src, tmp)
  +        os.replace(tmp, dest)
  +    finally:
  +        if tmp.exists():
  +            try:
  +                tmp.unlink()
  +            except OSError:
  +                pass
  +
  +
  +def _archive_input(
  +    run_dir: Path,
  +    src: Path,
  +    dest_rel: Path,
  +    *,
  +    state_dir: Path = STATE_DIR,
  +) -> Dict[str, Any]:
  +    if not src.exists():
  +        logger.error("Archive source missing: %s", src)
  +        raise SystemExit(2)
  +    dest = run_dir / dest_rel
  +    try:
  +        _atomic_copy(src, dest)
  +        sha256 = compute_sha256_file(dest)
  +        size = dest.stat().st_size
  +    except SystemExit:
  +        raise
  +    except Exception as exc:
  +        logger.error("Archive copy failed for %s -> %s: %s", src, dest, exc)
  +        raise SystemExit(3) from exc
  +    return {
  +        "source_path": str(src),
  +        "archived_path": dest.relative_to(state_dir).as_posix(),
  +        "sha256": sha256,
  +        "bytes": size,
  +        "hash_algo": "sha256",
  +    }
  +
  +
  +def _archive_run_inputs(
  +    run_dir: Path,
  +    provider: str,
  +    profile: str,
  +    selected_input_path: Path,
  +    profiles_config_path: Path,
  +) -> Dict[str, Any]:
  +    base = Path("inputs") / provider / profile
  +    if not selected_input_path.exists():
  +        logger.error("Selected scoring input missing for archival: %s", selected_input_path)
  +        raise SystemExit(2)
  +    if not profiles_config_path.exists():
  +        logger.error("Profiles config missing for archival: %s", profiles_config_path)
  +        raise SystemExit(2)
  +    return {
  +        "selected_scoring_input": _archive_input(run_dir, selected_input_path, base / "selected_scoring_input.json"),
  +        "profile_config": _archive_input(run_dir, profiles_config_path, base / "profiles.json"),
  +    }
  +
  +
  +def _run_registry_dir(run_id: str) -> Path:
  +    return RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +
  +
  +def _write_run_registry(
  +    run_id: str,
  +    providers: List[str],
  +    profiles: List[str],
  +    run_metadata_path: Path,
  +    diff_counts_by_provider: Dict[str, Dict[str, Dict[str, int]]],
  +    telemetry: Dict[str, Any],
  +) -> Path:
  +    run_dir = _run_registry_dir(run_id)
  +    run_dir.mkdir(parents=True, exist_ok=True)
  +
  +    artifacts: Dict[str, str] = {}
  +    run_report_dest = run_dir / "run_report.json"
  +    _copy_artifact(run_metadata_path, run_report_dest)
  +    artifacts[run_report_dest.name] = run_report_dest.relative_to(run_dir).as_posix()
  +
  +    providers_payload: Dict[str, Any] = {}
  +    for provider in providers:
  +        provider_dir = run_dir / provider
  +        provider_payload: Dict[str, Any] = {"profiles": {}, "artifacts": {}}
  +        provider_inputs = [
  +            _provider_raw_jobs_json(provider),
  +            _provider_labeled_jobs_json(provider),
  +            _provider_enriched_jobs_json(provider),
  +            _provider_ai_jobs_json(provider),
  +        ]
  +        for src in provider_inputs:
  +            if not src.exists():
  +                continue
  +            dest = provider_dir / src.name
  +            _copy_artifact(src, dest)
  +            rel = dest.relative_to(run_dir).as_posix()
  +            provider_payload["artifacts"][src.name] = rel
  +            artifacts.setdefault(src.name, rel)
  +
  +        for profile in profiles:
  +            profile_dir = provider_dir / profile
  +            profile_payload = {
  +                "diff_counts": diff_counts_by_provider.get(provider, {}).get(
  +                    profile, {"new": 0, "changed": 0, "removed": 0}
  +                ),
  +                "artifacts": {},
  +            }
  +            for src in (run_dir / f"ai_insights.{profile}.json", run_dir / f"ai_insights.{profile}.md"):
  +                if src.exists():
  +                    rel = src.relative_to(run_dir).as_posix()
  +                    profile_payload["artifacts"][src.name] = rel
  +                    artifacts.setdefault(src.name, rel)
  +            for src in (run_dir / f"ai_job_briefs.{profile}.json", run_dir / f"ai_job_briefs.{profile}.md"):
  +                if src.exists():
  +                    rel = src.relative_to(run_dir).as_posix()
  +                    profile_payload["artifacts"][src.name] = rel
  +                    artifacts.setdefault(src.name, rel)
  +            profile_artifacts = [
  +                _provider_ranked_jobs_json(provider, profile),
  +                _provider_ranked_jobs_csv(provider, profile),
  +                _provider_ranked_families_json(provider, profile),
  +                _provider_shortlist_md(provider, profile),
  +                _provider_top_md(provider, profile),
  +            ]
  +            alerts_json, alerts_md = _alerts_paths(provider, profile)
  +            profile_artifacts.extend([alerts_json, alerts_md])
  +
  +            for src in profile_artifacts:
  +                if not src.exists():
  +                    continue
  +                dest = profile_dir / src.name
  +                _copy_artifact(src, dest)
  +                rel = dest.relative_to(run_dir).as_posix()
  +                profile_payload["artifacts"][src.name] = rel
  +                artifacts.setdefault(src.name, rel)
  +
  +            provider_payload["profiles"][profile] = profile_payload
  +
  +        providers_payload[provider] = provider_payload
  +
  +    payload = {
  +        "run_id": run_id,
  +        "timestamp": telemetry.get("ended_at"),
  +        "providers": providers_payload,
  +        "artifacts": artifacts,
  +        "run_report_path": artifacts.get("run_report.json"),
  +    }
  +
  +    index_path = run_dir / "index.json"
  +    index_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2, sort_keys=True), encoding="utf-8")
  +    return index_path
  +
  +
  +def _archive_profile_artifacts(
  +    run_id: str,
  +    profile: str,
  +    run_metadata_path: Path,
  +    summary_payload: Dict[str, object],
  +    provider: Optional[str] = None,
  +) -> None:
  +    history_dir = _history_run_dir(run_id, profile, provider)
  +    latest_dir = _latest_profile_dir(profile, provider)
  +    artifacts = [
  +        _provider_ranked_jobs_json(provider or "openai", profile),
  +        _provider_ranked_jobs_csv(provider or "openai", profile),
  +        _provider_ranked_families_json(provider or "openai", profile),
  +        _provider_shortlist_md(provider or "openai", profile),
  +    ]
  +    for src in artifacts:
  +        dest_history = history_dir / src.name
  +        dest_latest = latest_dir / src.name
  +        _copy_artifact(src, dest_history)
  +        _copy_artifact(src, dest_latest)
  +    _copy_artifact(run_metadata_path, latest_dir / "run_metadata.json")
  +    summary_file = "run_summary.txt"
  +    for dest in (history_dir, latest_dir):
  +        dest_summary = dest / summary_file
  +        dest_summary.parent.mkdir(parents=True, exist_ok=True)
  +        dest_summary.write_text(json.dumps(summary_payload, ensure_ascii=False, sort_keys=True), encoding="utf-8")
  +    # keep run_metadata per run for history and single copy for latest
  +    _copy_artifact(run_metadata_path, history_dir / run_metadata_path.name)
  +    _copy_artifact(run_metadata_path, latest_dir / "run_metadata.json")
  +
  +
  +def _persist_run_metadata(
  +    run_id: str,
  +    telemetry: Dict[str, Any],
  +    profiles: List[str],
  +    flags: Dict[str, Any],
  +    diff_counts: Dict[str, Dict[str, Any]],
  +    provenance_by_provider: Optional[Dict[str, Dict[str, Any]]],
  +    scoring_inputs_by_profile: Dict[str, Dict[str, Optional[str]]],
  +    scoring_input_selection_by_profile: Dict[str, Dict[str, Any]],
  +    archived_inputs_by_provider_profile: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
  +    providers: Optional[List[str]] = None,
  +    inputs_by_provider: Optional[Dict[str, Dict[str, Dict[str, Optional[str]]]]] = None,
  +    scoring_inputs_by_provider: Optional[Dict[str, Dict[str, Dict[str, Optional[str]]]]] = None,
  +    scoring_input_selection_by_provider: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
  +    outputs_by_provider: Optional[Dict[str, Dict[str, Dict[str, Dict[str, Optional[str]]]]]] = None,
  +    delta_summary: Optional[Dict[str, Any]] = None,
  +    user_state_counts_by_provider_profile: Optional[Dict[str, Dict[str, Dict[str, int]]]] = None,
  +    config_fingerprint: Optional[str] = None,
+++    provider_registry: Optional[Dict[str, Any]] = None,
  +    environment_fingerprint: Optional[Dict[str, Optional[str]]] = None,
  +    logs: Optional[Dict[str, Any]] = None,
  +    log_retention: Optional[Dict[str, Any]] = None,
  +    semantic_contract: Optional[Dict[str, Any]] = None,
  +) -> Path:
  +    run_report_schema_version = RUN_REPORT_SCHEMA_VERSION
  +    inputs: Dict[str, Dict[str, Optional[str]]] = {
  +        "raw_jobs_json": _file_metadata(_provider_raw_jobs_json("openai")),
  +        "labeled_jobs_json": _file_metadata(_provider_labeled_jobs_json("openai")),
  +        "enriched_jobs_json": _file_metadata(_provider_enriched_jobs_json("openai")),
  +    }
  +    ai_path = _provider_ai_jobs_json("openai")
  +    if ai_path.exists():
  +        inputs["ai_enriched_jobs_json"] = _file_metadata(ai_path)
  +
  +    outputs_by_profile: Dict[str, Dict[str, Dict[str, Optional[str]]]] = {}
  +    for profile in profiles:
  +        outputs_by_profile[profile] = {
  +            "ranked_json": _output_metadata(_provider_ranked_jobs_json("openai", profile)),
  +            "ranked_csv": _output_metadata(_provider_ranked_jobs_csv("openai", profile)),
  +            "ranked_families_json": _output_metadata(_provider_ranked_families_json("openai", profile)),
  +            "shortlist_md": _output_metadata(_provider_shortlist_md("openai", profile)),
  +            "top_md": _output_metadata(_provider_top_md("openai", profile)),
  +        }
  +
  +    provider_list = providers or ["openai"]
  +    provider_inputs = inputs_by_provider or {"openai": inputs}
  +    provider_scoring_inputs = scoring_inputs_by_provider or {"openai": scoring_inputs_by_profile}
  +    provider_scoring_selection = scoring_input_selection_by_provider or {"openai": scoring_input_selection_by_profile}
  +    provider_outputs = outputs_by_provider or {"openai": outputs_by_profile}
  +    run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +    verifiable_artifacts = _verifiable_artifacts(run_dir, provider_outputs)
  +    config_fingerprint_value = config_fingerprint or _config_fingerprint(flags, None)
  +    environment_fingerprint_value = environment_fingerprint or _environment_fingerprint()
  +
  +    selection = {"scrape_provenance": provenance_by_provider or {}}
  +    if provenance_by_provider:
  +        selection["provider_availability"] = {
  +            provider: {
  +                "status": meta.get("availability"),
  +                "unavailable_reason": meta.get("unavailable_reason"),
  +                "attempts_made": meta.get("attempts_made"),
  +            }
  +            for provider, meta in provenance_by_provider.items()
  +        }
  +    classified_by_provider = _classified_counts_by_provider(provider_list)
  +    if classified_by_provider:
  +        selection["classified_job_count_by_provider"] = classified_by_provider
  +        if "openai" in classified_by_provider:
  +            selection["classified_job_count"] = classified_by_provider["openai"]
  +        else:
  +            primary_provider = provider_list[0]
  +            if primary_provider in classified_by_provider:
  +                selection["classified_job_count"] = classified_by_provider[primary_provider]
  +
  +    build_provenance = {
  +        "git_sha": os.environ.get("JOBINTEL_GIT_SHA", "unknown"),
  +        "image": os.environ.get("JOBINTEL_IMAGE", "unknown"),
  +        "taskdef": os.environ.get("JOBINTEL_TASKDEF", "unknown"),
  +        "ecs_task_arn": _resolve_ecs_task_arn(),
  +    }
+++    if provider_registry is not None:
+++        build_provenance["provider_registry"] = provider_registry
  +
  +    payload = {
  +        "run_report_schema_version": run_report_schema_version,
  +        "run_id": run_id,
  +        "status": telemetry.get("status"),
  +        "profiles": profiles,
  +        "providers": provider_list,
  +        "flags": flags,
  +        "timestamps": {
  +            "started_at": telemetry.get("started_at"),
  +            "ended_at": telemetry.get("ended_at"),
  +        },
  +        "stage_durations": telemetry.get("stages", {}),
  +        "diff_counts": diff_counts,
  +        "provenance_by_provider": provenance_by_provider or {},
  +        "provenance": {**(provenance_by_provider or {}), "build": build_provenance},
  +        "selection": selection,
  +        "inputs": inputs,
  +        "scoring_inputs_by_profile": scoring_inputs_by_profile,
  +        "scoring_input_selection_by_profile": scoring_input_selection_by_profile,
  +        "outputs_by_profile": outputs_by_profile,
  +        "inputs_by_provider": provider_inputs,
  +        "scoring_inputs_by_provider": provider_scoring_inputs,
  +        "scoring_input_selection_by_provider": provider_scoring_selection,
  +        "outputs_by_provider": provider_outputs,
  +        "verifiable_artifacts": verifiable_artifacts,
  +        "config_fingerprint": config_fingerprint_value,
  +        "environment_fingerprint": environment_fingerprint_value,
  +        "git_sha": _best_effort_git_sha(),
  +        "image_tag": os.environ.get("IMAGE_TAG"),
  +    }
  +    semantic_contract = semantic_contract or {}
  +    payload["semantic_enabled"] = bool(semantic_contract.get("semantic_enabled", False))
  +    payload["semantic_mode"] = str(semantic_contract.get("semantic_mode", "boost"))
  +    payload["semantic_model_id"] = str(semantic_contract.get("semantic_model_id", DEFAULT_SEMANTIC_MODEL_ID))
  +    payload["semantic_threshold"] = float(semantic_contract.get("semantic_threshold", 0.72))
  +    payload["semantic_max_boost"] = float(semantic_contract.get("semantic_max_boost", 5.0))
  +    payload["embedding_backend_version"] = str(
  +        semantic_contract.get("embedding_backend_version", EMBEDDING_BACKEND_VERSION)
  +    )
  +    if archived_inputs_by_provider_profile:
  +        payload["archived_inputs_by_provider_profile"] = archived_inputs_by_provider_profile
  +    if delta_summary is not None:
  +        payload["delta_summary"] = delta_summary
  +    if user_state_counts_by_provider_profile is not None:
  +        payload["user_state_counts_by_provider_profile"] = user_state_counts_by_provider_profile
  +    if logs is not None:
  +        payload["logs"] = logs
  +    if log_retention is not None:
  +        payload["log_retention"] = log_retention
  +    payload["success"] = telemetry.get("success", False)
  +    if telemetry.get("failed_stage"):
  +        payload["failed_stage"] = telemetry["failed_stage"]
  +    path = _run_metadata_path(run_id)
  +    path.parent.mkdir(parents=True, exist_ok=True)
  +    _redaction_guard_json(path, payload)
  +    path.write_text(
  +        json.dumps(payload, ensure_ascii=False, sort_keys=True, separators=(",", ":")) + "\n", encoding="utf-8"
  +    )
  +    return path
  +
  +
  +def _hash_file(path: Path) -> Optional[str]:
  +    if not path.exists():
  +        return None
  +    try:
  +        return compute_sha256_file(path)
  +    except Exception:
  +        return None
  +
  +
+++def _validate_named_schema_payload(schema_name: str, version: int, payload: Dict[str, Any]) -> None:
+++    schema_path = resolve_named_schema_path(schema_name, version)
+++    schema = json.loads(schema_path.read_text(encoding="utf-8"))
+++    errors = validate_payload(payload, schema)
+++    if errors:
+++        joined = "; ".join(errors)
+++        raise RuntimeError(f"{schema_name} schema validation failed: {joined}")
+++
+++
+++def _write_artifact_model_v2(run_report_path: Path, run_report_payload: Dict[str, Any]) -> Dict[str, Any]:
+++    run_id = str(run_report_payload.get("run_id") or "")
+++    if not run_id:
+++        raise RuntimeError("artifact model v2 requires run_id")
+++    run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
+++    run_dir.mkdir(parents=True, exist_ok=True)
+++    generated_at = str(run_report_payload.get("timestamps", {}).get("ended_at") or _utcnow_iso())
+++    ui_safe = build_ui_safe_artifact(
+++        run_report_payload,
+++        run_report_path=run_report_path,
+++        candidate_id=CANDIDATE_ID,
+++        generated_at=generated_at,
+++    )
+++    replay_safe = build_replay_safe_artifact(
+++        run_report_payload,
+++        run_report_path=run_report_path,
+++        candidate_id=CANDIDATE_ID,
+++        generated_at=generated_at,
+++    )
+++    _validate_named_schema_payload("artifact_ui_safe", UI_SAFE_SCHEMA_VERSION, ui_safe)
+++    _validate_named_schema_payload("artifact_replay_safe", REPLAY_SAFE_SCHEMA_VERSION, replay_safe)
+++
+++    ui_path = run_dir / UI_SAFE_FILENAME
+++    replay_path = run_dir / REPLAY_SAFE_FILENAME
+++    _write_canonical_json(ui_path, ui_safe)
+++    _write_canonical_json(replay_path, replay_safe)
+++
+++    return {
+++        "ui_safe": {"schema_version": UI_SAFE_SCHEMA_VERSION, "path": str(ui_path)},
+++        "replay_safe": {"schema_version": REPLAY_SAFE_SCHEMA_VERSION, "path": str(replay_path)},
+++    }
+++
+++
+++def _provider_registry_provenance(providers_config: Optional[str]) -> Dict[str, Any]:
+++    config_path = Path(providers_config) if providers_config else None
+++    schema_version: Optional[int] = None
+++    if config_path and config_path.exists():
+++        try:
+++            payload = json.loads(config_path.read_text(encoding="utf-8"))
+++            if isinstance(payload, dict):
+++                raw_schema_version = payload.get("schema_version")
+++                if isinstance(raw_schema_version, int):
+++                    schema_version = raw_schema_version
+++        except Exception:
+++            schema_version = None
+++    return {
+++        "path": str(config_path) if config_path else None,
+++        "sha256": _hash_file(config_path) if config_path else None,
+++        "schema_version": schema_version,
+++    }
+++
+++
  +def _count_jobs(path: Path) -> Optional[int]:
  +    if not path.exists():
  +        return None
  +    try:
  +        data = json.loads(path.read_text(encoding="utf-8"))
  +    except Exception:
  +        return None
  +    if isinstance(data, list):
  +        return len(data)
  +    return None
  +
  +
  +def _classified_counts_by_provider(providers: List[str]) -> Dict[str, int]:
  +    counts: Dict[str, int] = {}
  +    for provider in providers:
  +        path = _provider_labeled_jobs_json(provider)
  +        count = _count_jobs(path)
  +        if count is not None:
  +            counts[provider] = count
  +    return counts
  +
  +
  +def _baseline_latest_dir(provider: str, profile: str) -> Path:
  +    base_provider = provider if provider != "openai" else None
  +    return _latest_profile_dir(profile, base_provider)
  +
  +
  +def _baseline_ranked_path(provider: str, profile: str, baseline_dir: Path) -> Path:
  +    return baseline_dir / f"{provider}_ranked_jobs.{profile}.json"
  +
  +
  +def _baseline_run_info(baseline_dir: Path) -> Tuple[Optional[str], Optional[str]]:
  +    meta_path = baseline_dir / "run_metadata.json"
  +    if not meta_path.exists():
  +        return None, None
  +    try:
  +        payload = json.loads(meta_path.read_text(encoding="utf-8"))
  +    except Exception:
  +        return None, str(meta_path)
  +    run_id = payload.get("run_id") if isinstance(payload, dict) else None
  +    return run_id, str(meta_path)
  +
  +
  +def _resolve_s3_baseline(
  +    provider: str,
  +    profile: str,
  +    current_run_id: str,
  +    *,
  +    bucket: str,
  +    prefix: str,
  +) -> BaselineInfo:
  +    def _warn_missing(run_id: str, source: str) -> None:
  +        logger.warning(
  +            "Baseline pointer found (%s) but artifacts missing for %s/%s run_id=%s",
  +            source,
  +            provider,
  +            profile,
  +            run_id,
  +        )
  +
  +    state, status, key = read_provider_last_success_state(
  +        bucket,
  +        prefix,
  +        provider,
  +        profile,
  +        candidate_id=CANDIDATE_ID,
  +    )
  +    logger.info(
  +        "Baseline pointer read: s3://%s/%s status=%s",
  +        bucket,
  +        key,
  +        status,
  +    )
  +    if isinstance(state, dict):
  +        run_id = parse_pointer(state)
  +        if run_id and run_id != current_run_id:
  +            ranked_path = download_baseline_ranked(
  +                bucket,
  +                prefix,
  +                run_id,
  +                provider,
  +                profile,
  +                STATE_DIR / "baseline_cache",
  +            )
  +            if ranked_path:
  +                logger.info(
  +                    "Baseline pointer resolved (state_file) for %s/%s run_id=%s",
  +                    provider,
  +                    profile,
  +                    run_id,
  +                )
  +                return BaselineInfo(
  +                    run_id=run_id,
  +                    source="state_file",
  +                    path=state.get("run_path") or f"s3://{bucket}/{prefix.strip('/')}/runs/{run_id}/",
  +                    ranked_path=ranked_path,
  +                )
  +            _warn_missing(run_id, "state_file")
  +    elif status == "access_denied":
  +        logger.warning(
  +            "Baseline pointer access denied for %s/%s (s3://%s/%s)",
  +            provider,
  +            profile,
  +            bucket,
  +            key,
  +        )
  +
  +    state, status, key = read_last_success_state(bucket, prefix, candidate_id=CANDIDATE_ID)
  +    logger.info(
  +        "Baseline pointer read: s3://%s/%s status=%s",
  +        bucket,
  +        key,
  +        status,
  +    )
  +    if isinstance(state, dict):
  +        run_id = parse_pointer(state)
  +        if run_id and run_id != current_run_id:
  +            ranked_path = download_baseline_ranked(
  +                bucket,
  +                prefix,
  +                run_id,
  +                provider,
  +                profile,
  +                STATE_DIR / "baseline_cache",
  +            )
  +            if ranked_path:
  +                logger.info(
  +                    "Baseline pointer resolved (state_file) for %s/%s run_id=%s",
  +                    provider,
  +                    profile,
  +                    run_id,
  +                )
  +                return BaselineInfo(
  +                    run_id=run_id,
  +                    source="state_file",
  +                    path=state.get("run_path") or f"s3://{bucket}/{prefix.strip('/')}/runs/{run_id}/",
  +                    ranked_path=ranked_path,
  +                )
  +            _warn_missing(run_id, "state_file")
  +    elif status == "access_denied":
  +        logger.warning(
  +            "Baseline pointer access denied for %s/%s (s3://%s/%s)",
  +            provider,
  +            profile,
  +            bucket,
  +            key,
  +        )
  +
  +    logger.info("Baseline pointer fallback: listing s3://%s/%s/runs/", bucket, prefix.strip("/"))
  +    run_id = get_most_recent_successful_run_id_before(bucket, prefix, current_run_id)
  +    if run_id:
  +        ranked_path = download_baseline_ranked(
  +            bucket,
  +            prefix,
  +            run_id,
  +            provider,
  +            profile,
  +            STATE_DIR / "baseline_cache",
  +        )
  +        if ranked_path:
  +            logger.info(
  +                "Baseline pointer resolved (s3_latest) for %s/%s run_id=%s",
  +                provider,
  +                profile,
  +                run_id,
  +            )
  +            return BaselineInfo(
  +                run_id=run_id,
  +                source="s3_latest",
  +                path=f"s3://{bucket}/{prefix.strip('/')}/runs/{run_id}/",
  +                ranked_path=ranked_path,
  +            )
  +        _warn_missing(run_id, "s3_latest")
  +    return BaselineInfo(run_id=None, source="none", path=None, ranked_path=None)
  +
  +
  +def _build_delta_summary(run_id: str, providers: List[str], profiles: List[str]) -> Dict[str, Any]:
  +    summary: Dict[str, Any] = {
  +        "baseline_run_id": None,
  +        "baseline_run_path": None,
  +        "current_run_id": run_id,
  +        "provider_profile": {},
  +    }
  +    first_baseline: Optional[Tuple[str, str]] = None
  +
  +    for provider in providers:
  +        summary["provider_profile"].setdefault(provider, {})
  +        for profile in profiles:
  +            baseline_dir = _baseline_latest_dir(provider, profile)
  +            baseline_ranked = _baseline_ranked_path(provider, profile, baseline_dir)
  +            baseline_run_id, baseline_run_path = _baseline_run_info(baseline_dir)
  +            baseline_source = "explicit"
  +            baseline_resolved = baseline_ranked.exists()
  +            baseline_ranked_path = baseline_ranked if baseline_ranked.exists() else None
  +            if not baseline_ranked.exists():
  +                baseline_run_id = None
  +                baseline_run_path = None
  +                baseline_source = "none"
  +                if s3_enabled():
  +                    bucket = os.environ.get("JOBINTEL_S3_BUCKET", "").strip()
  +                    prefix = os.environ.get("JOBINTEL_S3_PREFIX", "jobintel").strip("/")
  +                    if bucket:
  +                        s3_info = _resolve_s3_baseline(provider, profile, run_id, bucket=bucket, prefix=prefix)
  +                        if s3_info.run_id and s3_info.ranked_path:
  +                            baseline_run_id = s3_info.run_id
  +                            baseline_run_path = s3_info.path
  +                            baseline_source = s3_info.source
  +                            baseline_ranked_path = s3_info.ranked_path
  +                            baseline_resolved = True
  +                    else:
  +                        logger.warning("S3 baseline enabled but JOBINTEL_S3_BUCKET is unset.")
  +            if baseline_run_id and baseline_run_path and first_baseline is None:
  +                first_baseline = (baseline_run_id, baseline_run_path)
  +
  +            current_labeled = _provider_labeled_jobs_json(provider)
  +            current_ranked = _provider_ranked_jobs_json(provider, profile)
  +            delta = compute_delta(
  +                current_labeled,
  +                current_ranked,
  +                None,
  +                baseline_ranked_path,
  +                provider,
  +                profile,
  +            )
  +            delta["baseline_run_id"] = baseline_run_id
  +            delta["baseline_run_path"] = baseline_run_path
  +            delta["baseline_source"] = baseline_source
  +            delta["baseline_resolved"] = baseline_resolved
  +            delta["current_run_id"] = run_id
  +            summary["provider_profile"][provider][profile] = delta
  +
  +    if first_baseline:
  +        summary["baseline_run_id"], summary["baseline_run_path"] = first_baseline
  +    return summary
  +
  +
  +def _file_metadata(path: Path) -> Dict[str, Optional[str]]:
  +    return {
  +        "path": str(path),
  +        "mtime_iso": _file_mtime_iso(path),
  +        "sha256": _hash_file(path),
  +    }
  +
  +
  +def _candidate_metadata(path: Path) -> Dict[str, Optional[str]]:
  +    meta = _file_metadata(path)
  +    meta["exists"] = path.exists()
  +    return meta
  +
  +
  +def _output_metadata(path: Path) -> Dict[str, Optional[str]]:
  +    return {
  +        "path": str(path),
  +        "sha256": _hash_file(path),
  +    }
  +
  +
  +def _config_fingerprint(flags: Dict[str, Any], providers_config: Optional[str]) -> str:
  +    allowed_keys = {
  +        "profile",
  +        "profiles",
  +        "providers",
  +        "us_only",
  +        "no_enrich",
  +        "ai",
  +        "ai_only",
  +        "min_score",
  +        "min_alert_score",
  +        "offline",
  +        "scrape_only",
  +        "no_subprocess",
  +        "snapshot_only",
  +    }
  +    filtered_flags = {key: flags.get(key) for key in sorted(allowed_keys)}
  +    providers_config_path = Path(providers_config) if providers_config else None
  +    env_keys = [
  +        "CAREERS_MODE",
  +        "EMBED_PROVIDER",
  +        "EMBEDDING_MODEL",
  +        "OPENAI_MODEL",
  +        "ANTHROPIC_MODEL",
  +        "JOBINTEL_DATA_DIR",
  +        "JOBINTEL_STATE_DIR",
  +        "TZ",
  +        "PYTHONHASHSEED",
  +    ]
  +    env_payload = {key: os.environ.get(key) for key in env_keys}
  +    config_payload = {
  +        "flags": filtered_flags,
  +        "providers_config_path": str(providers_config_path) if providers_config_path else None,
  +        "providers_config_sha256": _hash_file(providers_config_path) if providers_config_path else None,
  +        "env": env_payload,
  +    }
  +    payload = json.dumps(config_payload, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
  +    return compute_sha256_bytes(payload.encode("utf-8"))
  +
  +
  +def _environment_fingerprint() -> Dict[str, Optional[str]]:
  +    return {
  +        "python_version": sys.version.split()[0],
  +        "platform": platform.platform(),
  +        "image_tag": os.environ.get("IMAGE_TAG"),
  +        "git_sha": _best_effort_git_sha(),
  +        "tz": os.environ.get("TZ"),
  +        "pythonhashseed": os.environ.get("PYTHONHASHSEED"),
  +    }
  +
  +
  +def _verifiable_artifacts(
  +    run_dir: Path, provider_outputs: Dict[str, Dict[str, Dict[str, Dict[str, Optional[str]]]]]
  +) -> Dict[str, Dict[str, str]]:
  +    artifacts: Dict[str, Path] = {}
  +    for provider, profiles_payload in provider_outputs.items():
  +        for profile, outputs in profiles_payload.items():
  +            for output_key, meta in outputs.items():
  +                if not isinstance(meta, dict):
  +                    continue
  +                path_str = meta.get("path")
  +                if not path_str:
  +                    continue
  +                logical_key = f"{provider}:{profile}:{output_key}"
  +                artifacts[logical_key] = Path(path_str)
  +    return build_verifiable_artifacts(DATA_DIR, artifacts)
  +
  +
  +def _best_effort_git_sha() -> Optional[str]:
  +    env_sha = os.environ.get("GIT_SHA")
  +    if env_sha:
  +        return env_sha
  +    try:
  +        result = subprocess.run(
  +            ["git", "rev-parse", "HEAD"],
  +            cwd=str(REPO_ROOT),
  +            text=True,
  +            capture_output=True,
  +        )
  +        if result.returncode == 0:
  +            return (result.stdout or "").strip() or None
  +    except Exception:
  +        return None
  +    return None
  +
  +
  +def _normalize_exit_code(code: Any) -> int:
  +    if code is None:
  +        return 0
  +    if isinstance(code, int):
  +        return code
  +    if isinstance(code, str):
  +        try:
  +            return int(code)
  +        except ValueError:
  +            pass
  +    return 1
  +
  +
  +def _resolve_score_input_path_for(args: argparse.Namespace, provider: str) -> Tuple[Optional[Path], Optional[str]]:
  +    """
  +    Decide which input file to feed into score_jobs based on CLI flags.
  +    Returns (path, error_message). If error_message is not None, caller should abort.
  +    """
  +    ai_path = _provider_ai_jobs_json(provider)
  +    enriched_path = _provider_enriched_jobs_json(provider)
  +    labeled_path = _provider_labeled_jobs_json(provider)
  +
  +    if args.ai_only:
  +        if not ai_path.exists():
  +            return None, (
  +                f"AI-only mode requires AI-enriched input at {ai_path}. "
  +                "Ensure --ai is set and run_ai_augment has produced this file."
  +            )
  +        return ai_path, None
  +
  +    if args.no_enrich:
  +        # Prefer enriched if it already exists and is newer than labeled; otherwise fall back to labeled.
  +        enriched_exists = enriched_path.exists()
  +        labeled_exists = labeled_path.exists()
  +
  +        if enriched_exists and labeled_exists:
  +            m_enriched = enriched_path.stat().st_mtime
  +            m_labeled = labeled_path.stat().st_mtime
  +            if m_enriched > m_labeled:
  +                return enriched_path, None
  +            logger.warning(
  +                "Enriched input is older than labeled; using labeled for scoring. enriched_mtime=%s labeled_mtime=%s",
  +                m_enriched,
  +                m_labeled,
  +            )
  +            return labeled_path, None
  +
  +        if enriched_exists:
  +            return enriched_path, None
  +        if labeled_exists:
  +            return labeled_path, None
  +        return None, (
  +            f"Scoring input not found: {enriched_path} or {labeled_path}. "
  +            "Run without --no_enrich to generate enrichment, or ensure labeled data exists."
  +        )
  +
  +    # Default: expect enriched output
  +    if enriched_path.exists():
  +        return enriched_path, None
  +
  +    return None, (f"Scoring input not found: {enriched_path}. Re-run without --no_enrich to produce enrichment output.")
  +
  +
  +def _score_input_selection_detail_for(args: argparse.Namespace, provider: str) -> Dict[str, Any]:
  +    ai_path = _provider_ai_jobs_json(provider)
  +    enriched_path = _provider_enriched_jobs_json(provider)
  +    labeled_path = _provider_labeled_jobs_json(provider)
  +    enriched_meta = _candidate_metadata(enriched_path)
  +    labeled_meta = _candidate_metadata(labeled_path)
  +    ai_meta = _candidate_metadata(ai_path)
  +    candidates = [ai_meta, enriched_meta, labeled_meta]
  +    candidate_paths_considered = candidates
  +    flags = {"no_enrich": bool(args.no_enrich), "ai": bool(args.ai), "ai_only": bool(args.ai_only)}
  +
  +    decision: Dict[str, Any] = {"flags": flags, "comparisons": {}}
  +    selected_path: Optional[Path] = None
  +    reason = ""
  +    selection_reason = ""
  +    comparison_details: Dict[str, Any] = {}
  +    selection_reason_labeled_vs_enriched = "not_applicable"
  +    selection_reason_enriched_vs_ai = "not_applicable"
  +    decision_timestamp = utc_now_z(seconds_precision=True)
  +
  +    def _selection_reason_detail(
  +        rule_id: str,
  +        chosen_path: Optional[Path],
  +        candidate_paths: List[Path],
  +        compared_fields: Dict[str, Any],
  +        decision: str,
  +    ) -> Dict[str, Any]:
  +        return {
  +            "rule_id": rule_id,
  +            "chosen_path": str(chosen_path) if chosen_path else None,
  +            "candidate_paths": [str(path) for path in candidate_paths],
  +            "compared_fields": compared_fields,
  +            "decision": decision,
  +            "decision_timestamp": decision_timestamp,
  +        }
  +
  +    def _candidate_field_map(paths: List[Path]) -> Dict[str, Any]:
  +        result: Dict[str, Any] = {}
  +        for path in paths:
  +            result[str(path)] = _file_metadata(path) if path.exists() else None
  +        return result
  +
  +    def _ai_note() -> str:
  +        if args.ai and not args.ai_only:
  +            return " (ai does not change selection; prefer_ai affects scoring only)"
  +        return ""
  +
  +    if args.ai_only:
  +        decision["rule"] = "ai_only"
  +        reason = "ai_only requires AI-enriched input"
  +        selected_path = ai_path if ai_path.exists() else None
  +        selection_reason = "ai_only"
  +        selection_reason_enriched_vs_ai = "ai_only_required"
  +        decision["reason"] = reason
  +        labeled_vs_enriched_detail = _selection_reason_detail(
  +            f"labeled_vs_enriched.{selection_reason_labeled_vs_enriched}",
  +            None,
  +            [enriched_path, labeled_path],
  +            {"candidates": _candidate_field_map([enriched_path, labeled_path])},
  +            selection_reason_labeled_vs_enriched,
  +        )
  +        enriched_vs_ai_detail = _selection_reason_detail(
  +            f"enriched_vs_ai.{selection_reason_enriched_vs_ai}",
  +            selected_path,
  +            [enriched_path, ai_path],
  +            {"candidates": _candidate_field_map([enriched_path, ai_path])},
  +            selection_reason_enriched_vs_ai,
  +        )
  +        return {
  +            "selected": _file_metadata(selected_path) if selected_path else None,
  +            "selected_path": str(selected_path) if selected_path else None,
  +            "candidate_paths_considered": candidate_paths_considered,
  +            "selection_reason": selection_reason,
  +            "selection_reason_labeled_vs_enriched": selection_reason_labeled_vs_enriched,
  +            "selection_reason_enriched_vs_ai": selection_reason_enriched_vs_ai,
  +            "selection_reason_details": {
  +                "labeled_vs_enriched": labeled_vs_enriched_detail,
  +                "enriched_vs_ai": enriched_vs_ai_detail,
  +            },
  +            "comparison_details": comparison_details,
  +            "candidates": candidates,
  +            "decision": decision,
  +        }
  +
  +    if args.no_enrich:
  +        decision["rule"] = "no_enrich_compare"
  +        comparisons: Dict[str, Any] = {}
  +        if enriched_path.exists() and labeled_path.exists():
  +            enriched_mtime = _file_mtime(enriched_path)
  +            labeled_mtime = _file_mtime(labeled_path)
  +            comparisons["enriched_mtime"] = enriched_mtime
  +            comparisons["labeled_mtime"] = labeled_mtime
  +            comparison_details["newer_by_seconds"] = (enriched_mtime or 0) - (labeled_mtime or 0)
  +            if (enriched_mtime or 0) > (labeled_mtime or 0):
  +                selected_path = enriched_path
  +                reason = "enriched newer than labeled"
  +                selection_reason = "no_enrich_enriched_newer"
  +                selection_reason_labeled_vs_enriched = "enriched_newer"
  +                comparisons["winner"] = "enriched"
  +            else:
  +                selected_path = labeled_path
  +                reason = "labeled newer or same mtime as enriched"
  +                selection_reason = "no_enrich_labeled_newer_or_equal"
  +                selection_reason_labeled_vs_enriched = "labeled_newer_or_equal"
  +                comparisons["winner"] = "labeled"
  +        elif enriched_path.exists():
  +            selected_path = enriched_path
  +            reason = "enriched exists and labeled missing"
  +            selection_reason = "no_enrich_enriched_only"
  +            selection_reason_labeled_vs_enriched = "enriched_only"
  +            comparisons["winner"] = "enriched"
  +        elif labeled_path.exists():
  +            selected_path = labeled_path
  +            reason = "labeled exists and enriched missing"
  +            selection_reason = "no_enrich_labeled_only"
  +            selection_reason_labeled_vs_enriched = "labeled_only"
  +            comparisons["winner"] = "labeled"
  +        else:
  +            reason = "no_enrich requires labeled or enriched input"
  +            selection_reason = "no_enrich_missing"
  +            selection_reason_labeled_vs_enriched = "missing"
  +        decision["comparisons"] = comparisons
  +        decision["reason"] = reason + _ai_note()
  +        base_selected_path = selected_path
  +        if selected_path == enriched_path and args.ai and ai_path.exists():
  +            selection_reason = "prefer_ai_enriched"
  +            selected_path = ai_path
  +            comparison_details["prefer_ai"] = True
  +            selection_reason_enriched_vs_ai = "ai_enriched_preferred"
  +        elif selected_path == enriched_path and args.ai:
  +            selection_reason_enriched_vs_ai = "ai_enriched_missing"
  +        labeled_vs_enriched_detail = _selection_reason_detail(
  +            f"labeled_vs_enriched.{selection_reason_labeled_vs_enriched}",
  +            base_selected_path,
  +            [enriched_path, labeled_path],
  +            {
  +                "candidates": _candidate_field_map([enriched_path, labeled_path]),
  +                "comparisons": comparisons,
  +            },
  +            selection_reason_labeled_vs_enriched,
  +        )
  +        enriched_vs_ai_detail = _selection_reason_detail(
  +            f"enriched_vs_ai.{selection_reason_enriched_vs_ai}",
  +            selected_path,
  +            [enriched_path, ai_path],
  +            {"candidates": _candidate_field_map([enriched_path, ai_path])},
  +            selection_reason_enriched_vs_ai,
  +        )
  +        return {
  +            "selected": _file_metadata(selected_path) if selected_path else None,
  +            "selected_path": str(selected_path) if selected_path else None,
  +            "candidate_paths_considered": candidate_paths_considered,
  +            "selection_reason": selection_reason,
  +            "selection_reason_labeled_vs_enriched": selection_reason_labeled_vs_enriched,
  +            "selection_reason_enriched_vs_ai": selection_reason_enriched_vs_ai,
  +            "selection_reason_details": {
  +                "labeled_vs_enriched": labeled_vs_enriched_detail,
  +                "enriched_vs_ai": enriched_vs_ai_detail,
  +            },
  +            "comparison_details": comparison_details,
  +            "candidates": candidates,
  +            "decision": decision,
  +        }
  +
  +    decision["rule"] = "default_enriched_required"
  +    if enriched_path.exists():
  +        selected_path = enriched_path
  +        reason = "default requires enriched input"
  +        selection_reason = "default_enriched_required"
  +        selection_reason_labeled_vs_enriched = "enriched_required"
  +    else:
  +        reason = "enriched input missing"
  +        selection_reason = "default_enriched_missing"
  +        selection_reason_labeled_vs_enriched = "missing"
  +    decision["reason"] = reason + _ai_note()
  +    base_selected_path = selected_path
  +    if selected_path == enriched_path and args.ai and ai_path.exists():
  +        selection_reason = "prefer_ai_enriched"
  +        selected_path = ai_path
  +        comparison_details["prefer_ai"] = True
  +        selection_reason_enriched_vs_ai = "ai_enriched_preferred"
  +    elif selected_path == enriched_path and args.ai:
  +        selection_reason_enriched_vs_ai = "ai_enriched_missing"
  +    labeled_vs_enriched_detail = _selection_reason_detail(
  +        f"labeled_vs_enriched.{selection_reason_labeled_vs_enriched}",
  +        base_selected_path,
  +        [enriched_path, labeled_path],
  +        {"candidates": _candidate_field_map([enriched_path, labeled_path])},
  +        selection_reason_labeled_vs_enriched,
  +    )
  +    enriched_vs_ai_detail = _selection_reason_detail(
  +        f"enriched_vs_ai.{selection_reason_enriched_vs_ai}",
  +        selected_path,
  +        [enriched_path, ai_path],
  +        {"candidates": _candidate_field_map([enriched_path, ai_path])},
  +        selection_reason_enriched_vs_ai,
  +    )
  +    return {
  +        "selected": _file_metadata(selected_path) if selected_path else None,
  +        "selected_path": str(selected_path) if selected_path else None,
  +        "candidate_paths_considered": candidate_paths_considered,
  +        "selection_reason": selection_reason,
  +        "selection_reason_labeled_vs_enriched": selection_reason_labeled_vs_enriched,
  +        "selection_reason_enriched_vs_ai": selection_reason_enriched_vs_ai,
  +        "selection_reason_details": {
  +            "labeled_vs_enriched": labeled_vs_enriched_detail,
  +            "enriched_vs_ai": enriched_vs_ai_detail,
  +        },
  +        "comparison_details": comparison_details,
  +        "candidates": candidates,
  +        "decision": decision,
  +    }
  +
  +
  +def _resolve_score_input_path(args: argparse.Namespace) -> Tuple[Optional[Path], Optional[str]]:
  +    return _resolve_score_input_path_for(args, "openai")
  +
  +
  +def _score_input_selection_detail(args: argparse.Namespace) -> Dict[str, Any]:
  +    return _score_input_selection_detail_for(args, "openai")
  +
  +
  +def _safe_len(path: Path) -> int:
  +    try:
  +        data = json.loads(path.read_text(encoding="utf-8"))
  +        if isinstance(data, list):
  +            return len(data)
  +    except Exception:
  +        return 0
  +    return 0
  +
  +
  +def _load_last_run() -> Dict[str, Any]:
  +    if not LAST_RUN_JSON.exists():
  +        return {}
  +    try:
  +        return json.loads(LAST_RUN_JSON.read_text(encoding="utf-8"))
  +    except Exception:
  +        return {}
  +
  +
  +def _write_last_run(payload: Dict[str, Any]) -> None:
  +    _write_json(LAST_RUN_JSON, payload)
  +
  +
  +def _parse_logical_key(logical_key: str) -> Optional[Tuple[str, str, str]]:
  +    parts = logical_key.split(":")
  +    if len(parts) < 3:
  +        return None
  +    provider, profile = parts[0], parts[1]
  +    output_key = ":".join(parts[2:])
  +    return provider, profile, output_key
  +
  +
  +def _build_last_success_pointer(run_report: Dict[str, Any], run_report_path: Path) -> Dict[str, Any]:
  +    verifiable = run_report.get("verifiable_artifacts") if isinstance(run_report, dict) else None
  +    artifacts: Dict[str, Dict[str, Any]] = {}
  +    provider_profile: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    if isinstance(verifiable, dict):
  +        for logical_key, meta in verifiable.items():
  +            if not isinstance(meta, dict):
  +                continue
  +            sha = meta.get("sha256")
  +            bytes_value = meta.get("bytes")
  +            artifacts[logical_key] = {"sha256": sha, "bytes": bytes_value}
  +            parsed = _parse_logical_key(logical_key)
  +            if parsed:
  +                provider, profile, _ = parsed
  +                provider_profile.setdefault(provider, {}).setdefault(profile, {})[logical_key] = sha
  +
  +    timestamps = run_report.get("timestamps") if isinstance(run_report, dict) else None
  +    completed_at = None
  +    if isinstance(timestamps, dict):
  +        completed_at = timestamps.get("ended_at")
  +    if not completed_at:
  +        completed_at = run_report.get("timestamp") if isinstance(run_report, dict) else None
  +
  +    return {
  +        "run_id": run_report.get("run_id") if isinstance(run_report, dict) else None,
  +        "completed_at_utc": completed_at,
  +        "providers": run_report.get("providers") if isinstance(run_report, dict) else None,
  +        "profiles": run_report.get("profiles") if isinstance(run_report, dict) else None,
  +        "provider_profile": provider_profile,
  +        "artifacts": artifacts,
  +        "run_report_path": str(run_report_path),
  +    }
  +
  +
  +def _write_last_success_pointer(run_report: Dict[str, Any], run_report_path: Path) -> None:
  +    payload = _build_last_success_pointer(run_report, run_report_path)
  +    LAST_SUCCESS_JSON.parent.mkdir(parents=True, exist_ok=True)
  +    atomic_write_text(
  +        LAST_SUCCESS_JSON,
  +        json.dumps(payload, ensure_ascii=False, sort_keys=True, separators=(",", ":")) + "\n",
  +    )
  +
  +
  +def validate_config(args: argparse.Namespace, webhook: str) -> None:
  +    """
  +    Ensure required env/config combos for CLI args before running.
  +    """
  +    if args.test_post and not webhook:
  +        logger.error("test_post requires DISCORD_WEBHOOK_URL; set it or unset --test_post")
  +        raise SystemExit(2)
  +    if args.ai_only and not args.ai:
  +        logger.error("--ai_only depends on --ai")
  +        raise SystemExit(2)
  +    if args.scrape_only and args.ai_only:
  +        logger.error("--scrape_only and --ai_only are mutually exclusive")
  +        raise SystemExit(2)
  +
  +
  +def _file_mtime(path: Path) -> Optional[float]:
  +    try:
  +        return path.stat().st_mtime
  +    except Exception:
  +        return None
  +
  +
  +def _file_mtime_iso(path: Path) -> Optional[str]:
  +    ts = _file_mtime(path)
  +    if ts is None:
  +        return None
  +    return datetime.fromtimestamp(ts, timezone.utc).isoformat()
  +
  +
  +def _setup_logging(json_mode: bool, *, file_sink_path: Optional[Path] = None) -> Optional[str]:
  +    root_logger = logging.getLogger()
  +    if not root_logger.handlers:
  +        root_logger.setLevel(logging.INFO)
  +        stream_handler = logging.StreamHandler()
  +        if json_mode:
  +            stream_handler.setFormatter(JsonFormatter())
  +        else:
  +            stream_handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s", "%Y-%m-%d %H:%M:%S"))
  +        root_logger.addHandler(stream_handler)
  +    logger.setLevel(logging.INFO)
  +    file_sink_value: Optional[str] = None
  +    if file_sink_path is not None:
  +        file_sink_path.parent.mkdir(parents=True, exist_ok=True)
  +        file_sink_value = str(file_sink_path)
  +        if not any(
  +            getattr(handler, "_jobintel_file_sink_path", None) == file_sink_value for handler in root_logger.handlers
  +        ):
  +            file_handler = logging.FileHandler(file_sink_path, encoding="utf-8")
  +            file_handler.setFormatter(StructuredLogFormatter())
  +            file_handler._jobintel_file_sink_path = file_sink_value  # type: ignore[attr-defined]
  +            root_logger.addHandler(file_handler)
  +        file_sink_value = str(file_sink_path)
  +    return file_sink_value
  +
  +
  +def _run_logs_dir(run_id: str) -> Path:
  +    return _run_registry_dir(run_id) / "logs"
  +
  +
  +def _collect_run_log_pointers(run_id: str, file_sink_path: Optional[str]) -> Dict[str, Any]:
  +    run_dir = _run_registry_dir(run_id)
  +    logs_dir = _run_logs_dir(run_id)
  +    local_payload: Dict[str, Any] = {
  +        "run_dir": str(run_dir),
  +        "logs_dir": str(logs_dir),
  +        "stdout": "process_stdout",
  +    }
  +    if file_sink_path:
  +        local_payload["structured_log_jsonl"] = file_sink_path
  +
  +    k8s_namespace = (
  +        os.environ.get("JOBINTEL_K8S_NAMESPACE") or os.environ.get("POD_NAMESPACE") or os.environ.get("K8S_NAMESPACE")
  +    )
  +    k8s_context = os.environ.get("JOBINTEL_K8S_CONTEXT")
  +    k8s_payload: Dict[str, Any] = {}
  +    if k8s_namespace or os.environ.get("KUBERNETES_SERVICE_HOST"):
  +        namespace = k8s_namespace or "jobintel"
  +        context_fragment = f"--context {k8s_context} " if k8s_context else ""
  +        k8s_payload = {
  +            "namespace": namespace,
  +            "context": k8s_context,
  +            "run_id": run_id,
  +            "pod_list_command": (
  +                f"kubectl {context_fragment}-n {namespace} get pods --sort-by=.metadata.creationTimestamp"
  +            ),
  +            "job_list_command": (
  +                f"kubectl {context_fragment}-n {namespace} get jobs --sort-by=.metadata.creationTimestamp"
  +            ),
  +            "logs_command_template": (
  +                f"kubectl {context_fragment}-n {namespace} logs <pod-or-job> | rg 'JOBINTEL_RUN_ID={run_id}'"
  +            ),
  +        }
  +
  +    region = os.environ.get("AWS_REGION") or os.environ.get("AWS_DEFAULT_REGION")
  +    log_group = (
  +        os.environ.get("JOBINTEL_CLOUDWATCH_LOG_GROUP")
  +        or os.environ.get("AWS_LOG_GROUP")
  +        or os.environ.get("ECS_AWSLOGS_GROUP")
  +        or os.environ.get("AWSLOGS_GROUP")
  +    )
  +    log_stream = (
  +        os.environ.get("JOBINTEL_CLOUDWATCH_LOG_STREAM")
  +        or os.environ.get("AWS_LOG_STREAM")
  +        or os.environ.get("ECS_AWSLOGS_STREAM")
  +        or os.environ.get("AWSLOGS_STREAM")
  +    )
  +
  +    cloud_payload: Dict[str, Any] = {}
  +    if region or log_group or log_stream:
  +        cloud_payload = {
  +            "provider": "aws",
  +            "region": region,
  +            "cloudwatch_log_group": log_group,
  +            "cloudwatch_log_stream": log_stream,
  +            "cloudwatch_filter_pattern": f'"JOBINTEL_RUN_ID={run_id}"',
  +        }
  +
  +    return {
  +        "schema_version": 1,
  +        "run_id": run_id,
  +        "local": local_payload,
  +        "k8s": k8s_payload,
  +        "cloud": cloud_payload,
  +    }
  +
  +
  +def _enforce_run_log_retention(*, runs_dir: Path, keep_runs: int) -> Dict[str, Any]:
  +    if keep_runs < 1:
  +        raise ValueError(f"keep_runs must be >= 1 (got {keep_runs})")
  +    run_entries = [p for p in sorted(runs_dir.iterdir(), key=lambda p: p.name) if p.is_dir()]
  +    keep = set(run_entries[-keep_runs:]) if keep_runs > 0 else set()
  +    pruned_paths: List[str] = []
  +    for run_path in run_entries:
  +        if run_path in keep:
  +            continue
  +        logs_dir = run_path / "logs"
  +        if logs_dir.exists():
  +            shutil.rmtree(logs_dir)
  +            pruned_paths.append(str(logs_dir))
  +    return {
  +        "schema_version": 1,
  +        "keep_runs": keep_runs,
  +        "runs_seen": len(run_entries),
  +        "runs_kept": len(keep),
  +        "log_dirs_pruned": len(pruned_paths),
  +        "pruned_log_dirs": sorted(pruned_paths),
  +    }
  +
  +
  +def _should_short_circuit(prev_hashes: Dict[str, Any], curr_hashes: Dict[str, Any]) -> bool:
  +    return all(
  +        curr_hashes.get(k) is not None and curr_hashes.get(k) == prev_hashes.get(k)
  +        for k in ("raw", "labeled", "enriched")
  +    )
  +
  +
  +def _job_key(job: Dict[str, Any]) -> str:
  +    return str(job.get("job_id") or job_identity(job))
  +
  +
  +def _job_description_text(job: Dict[str, Any]) -> str:
  +    return (
  +        job.get("description_text") or job.get("jd_text") or job.get("description") or job.get("descriptionHtml") or ""
  +    )
  +
  +
  +def _job_field_value(job: Dict[str, Any], field: str) -> Any:
  +    if field == "location":
  +        return job.get("location") or job.get("locationName") or ""
  +    if field == "description_text":
  +        return _job_description_text(job)
  +    return job.get(field)
  +
  +
  +_FIELD_DIFF_KEYS: List[Tuple[str, str]] = [
  +    ("title", "title"),
  +    ("location", "location"),
  +    ("team", "team"),
  +    ("score", "score"),
  +    ("description_text", "description"),
  +]
  +_DEPRIORITIZED_USER_STATUSES = {"applied", "interviewing"}
  +
  +
  +def _hash_job(job: Dict[str, Any]) -> str:
  +    return str(job.get("content_fingerprint") or content_fingerprint(job))
  +
  +
  +def _load_profile_user_state(profile: str) -> Dict[str, Dict[str, Any]]:
  +    path = USER_STATE_DIR / f"{profile}.json"
  +    data, warning = load_user_state_checked(path)
  +    if warning:
  +        logger.warning("%s", warning)
  +        return {}
  +    return data
  +
  +
  +def _user_state_sets(
  +    profile: str,
  +    jobs: List[Dict[str, Any]],
  +) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, int], set[str], set[str]]:
  +    state_map = _load_profile_user_state(profile)
  +    counts: Dict[str, int] = dict.fromkeys(("ignore", "saved", "applied", "interviewing"), 0)
  +    ignored_ids: set[str] = set()
  +    suppress_new_ids: set[str] = set()
  +    for job in jobs:
  +        key = _job_key(job)
  +        record = state_map.get(key)
  +        if not isinstance(record, dict):
  +            continue
  +        status = normalize_user_status(record.get("status") or "")
  +        if status not in counts:
  +            continue
  +        counts[status] += 1
  +        if status == "ignore":
  +            ignored_ids.add(key)
  +            suppress_new_ids.add(key)
  +        elif status in {"applied", "interviewing"}:
  +            suppress_new_ids.add(key)
  +    return state_map, counts, ignored_ids, suppress_new_ids
  +
  +
  +def _filter_by_ids(items: List[Dict[str, Any]], blocked_ids: set[str]) -> List[Dict[str, Any]]:
  +    if not blocked_ids:
  +        return list(items)
  +    return [item for item in items if _job_key(item) not in blocked_ids]
  +
  +
  +def _status_for_item(item: Dict[str, Any], state_map: Dict[str, Dict[str, Any]]) -> str:
  +    key = _job_key(item)
  +    record = state_map.get(key)
  +    if not isinstance(record, dict):
  +        return ""
  +    return normalize_user_status(record.get("status") or "")
  +
  +
  +def _annotate_and_deprioritize_items(
  +    items: List[Dict[str, Any]],
  +    state_map: Dict[str, Dict[str, Any]],
  +) -> List[Dict[str, Any]]:
  +    primary: List[Dict[str, Any]] = []
  +    deprioritized: List[Dict[str, Any]] = []
  +    for item in items:
  +        status = _status_for_item(item, state_map)
  +        enriched = dict(item)
  +        if status:
  +            enriched["user_state_status"] = status
  +        if status in _DEPRIORITIZED_USER_STATUSES:
  +            deprioritized.append(enriched)
  +        else:
  +            primary.append(enriched)
  +    return primary + deprioritized
  +
  +
  +def _apply_user_state_to_alerts(
  +    alerts: Dict[str, Any],
  +    *,
  +    suppress_new_ids: set[str],
  +    ignored_ids: set[str],
  +) -> Dict[str, Any]:
  +    adjusted = dict(alerts)
  +    new_items = [item for item in list(adjusted.get("new_jobs") or []) if item.get("job_id") not in suppress_new_ids]
  +    score_items = [item for item in list(adjusted.get("score_changes") or []) if item.get("job_id") not in ignored_ids]
  +    title_items = [
  +        item for item in list(adjusted.get("title_or_location_changes") or []) if item.get("job_id") not in ignored_ids
  +    ]
  +    removed_items = [jid for jid in list(adjusted.get("removed_jobs") or []) if jid not in ignored_ids]
  +    adjusted["new_jobs"] = new_items
  +    adjusted["score_changes"] = score_items
  +    adjusted["title_or_location_changes"] = title_items
  +    adjusted["removed_jobs"] = removed_items
  +    adjusted["counts"] = {
  +        "new": len(new_items),
  +        "removed": len(removed_items),
  +        "score_changes": len(score_items),
  +        "title_or_location_changes": len(title_items),
  +    }
  +    return adjusted
  +
  +
  +def _diff(
  +    prev: List[Dict[str, Any]],
  +    curr: List[Dict[str, Any]],
  +) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]], Dict[str, List[str]]]:
  +    prev_map = {_job_key(j): (j, _hash_job(j)) for j in prev}
  +    curr_map = {_job_key(j): (j, _hash_job(j)) for j in curr}
  +
  +    new_jobs: List[Dict[str, Any]] = []
  +    changed_jobs: List[Dict[str, Any]] = []
  +    removed_jobs: List[Dict[str, Any]] = []
  +    changed_fields: Dict[str, List[str]] = {}
  +
  +    for k, (cj, ch) in curr_map.items():
  +        if k not in prev_map:
  +            new_jobs.append(cj)
  +        else:
  +            pj, ph = prev_map[k]
  +            if ph != ch:
  +                changes: List[str] = []
  +
  +                for key, label in _FIELD_DIFF_KEYS:
  +                    if _job_field_value(pj, key) != _job_field_value(cj, key):
  +                        changes.append(label)
  +
  +                changed_fields[k] = changes
  +                changed_jobs.append(cj)
  +
  +    for k, (pj, _) in prev_map.items():
  +        if k not in curr_map:
  +            removed_jobs.append(pj)
  +
  +    new_jobs.sort(key=lambda x: x.get("score", 0), reverse=True)
  +    changed_jobs.sort(key=lambda x: x.get("score", 0), reverse=True)
  +    removed_jobs.sort(key=lambda x: (x.get("apply_url") or "", x.get("title") or ""))
  +    return new_jobs, changed_jobs, removed_jobs, changed_fields
  +
  +
  +def _format_before_after(
  +    job: Dict[str, Any],
  +    prev_job: Optional[Dict[str, Any]],
  +    diff_labels: List[str],
  +) -> str:
  +    """Format before/after values for changed fields (excluding description content)."""
  +    parts: List[str] = []
  +    for field in ("title", "location", "team", "score"):
  +        if field in diff_labels:
  +            before = _job_field_value(prev_job, field) if prev_job else "?"
  +            after = _job_field_value(job, field)
  +            parts.append(f"{field}: {before} → {after}")
  +    # Description changes: just note it changed, don't dump content
  +    if "description" in diff_labels:
  +        parts.append("description_text")
  +    return ", ".join(parts) if parts else "details"
  +
  +
  +def _sort_key_score_url(job: Dict[str, Any]) -> Tuple[float, str]:
  +    """Sort key: score desc, url asc for deterministic ordering."""
  +    return (-job.get("score", 0), (job.get("apply_url") or "").lower())
  +
  +
  +def _sort_key_url(job: Dict[str, Any]) -> str:
  +    """Sort key: url asc for removed items."""
  +    return (job.get("apply_url") or "").lower()
  +
  +
  +def format_changes_section(
  +    new_jobs: List[Dict[str, Any]],
  +    changed_jobs: List[Dict[str, Any]],
  +    removed_jobs: List[Dict[str, Any]],
  +    changed_fields: Dict[str, List[str]],
  +    prev_map: Dict[str, Dict[str, Any]],
  +    prev_exists: bool,
  +    min_alert_score: int,
  +    limit: int = 10,
  +) -> str:
  +    """
  +    Pure helper: returns markdown for "Changes since last run" section.
  +
  +    Filtering rules:
  +    - Include items where job.score >= min_alert_score OR the item is removed.
  +    - For changed items, show before/after for title, location, team, score.
  +    - For description changes, just note "description_text" (no content dump).
  +
  +    Sorting rules (deterministic):
  +    - New/Changed: score desc, url asc
  +    - Removed: url asc
  +    """
  +    lines: List[str] = ["", "## Changes since last run"]
  +
  +    if not prev_exists:
  +        lines.append("No previous run to diff against.")
  +        return "\n".join(lines)
  +
  +    # Filter by min_alert_score (new/changed only; removed always included)
  +    filtered_new = [j for j in new_jobs if j.get("score", 0) >= min_alert_score]
  +    filtered_changed = [j for j in changed_jobs if j.get("score", 0) >= min_alert_score]
  +    filtered_removed = removed_jobs  # Always include all removed
  +
  +    # Sort deterministically
  +    filtered_new_sorted = sorted(filtered_new, key=_sort_key_score_url)[:limit]
  +    filtered_changed_sorted = sorted(filtered_changed, key=_sort_key_score_url)[:limit]
  +    filtered_removed_sorted = sorted(filtered_removed, key=_sort_key_url)[:limit]
  +
  +    # New section
  +    lines.append(f"### New ({len(filtered_new)}) list items")
  +    if not filtered_new_sorted:
  +        lines.append("_None_")
  +    else:
  +        for job in filtered_new_sorted:
  +            title = job.get("title") or "Untitled"
  +            url = job.get("apply_url") or job.get("detail_url") or job_identity(job) or "—"
  +            lines.append(f"- {title} — {url}")
  +
  +    lines.append("")
  +
  +    # Changed section
  +    lines.append(f"### Changed ({len(filtered_changed)}) list items")
  +    if not filtered_changed_sorted:
  +        lines.append("_None_")
  +    else:
  +        for job in filtered_changed_sorted:
  +            title = job.get("title") or "Untitled"
  +            url = job.get("apply_url") or job.get("detail_url") or job_identity(job) or "—"
  +            key = _job_key(job)
  +            diff_labels = changed_fields.get(key, [])
  +            prev_job = prev_map.get(key)
  +            change_desc = _format_before_after(job, prev_job, diff_labels)
  +            lines.append(f"- {title} — {url} (changed: {change_desc})")
  +
  +    lines.append("")
  +
  +    # Removed section (always include all, no score filtering)
  +    lines.append(f"### Removed ({len(removed_jobs)}) list items")
  +    if not filtered_removed_sorted:
  +        lines.append("_None_")
  +    else:
  +        for job in filtered_removed_sorted:
  +            title = job.get("title") or "Untitled"
  +            url = job.get("apply_url") or job.get("detail_url") or job_identity(job) or "—"
  +            lines.append(f"- {title} — {url}")
  +
  +    return "\n".join(lines)
  +
  +
  +def _append_shortlist_changes_section(
  +    shortlist_path: Path,
  +    profile: str,
  +    new_jobs: List[Dict[str, Any]],
  +    changed_jobs: List[Dict[str, Any]],
  +    removed_jobs: List[Dict[str, Any]],
  +    prev_exists: bool,
  +    changed_fields: Dict[str, List[str]],
  +    prev_jobs: Optional[List[Dict[str, Any]]] = None,
  +    min_alert_score: int = 0,
  +) -> None:
  +    """Append 'Changes since last run' section to shortlist markdown."""
  +    if not shortlist_path.exists():
  +        return
  +
  +    # Build prev_map for looking up before values
  +    prev_map: Dict[str, Dict[str, Any]] = {}
  +    if prev_jobs:
  +        prev_map = {_job_key(j): j for j in prev_jobs}
  +
  +    section_md = format_changes_section(
  +        new_jobs=new_jobs,
  +        changed_jobs=changed_jobs,
  +        removed_jobs=removed_jobs,
  +        changed_fields=changed_fields,
  +        prev_map=prev_map,
  +        prev_exists=prev_exists,
  +        min_alert_score=min_alert_score,
  +    )
  +
  +    content = shortlist_path.read_text(encoding="utf-8")
  +    if not content.endswith("\n"):
  +        content += "\n"
  +    content += section_md + "\n"
  +    shortlist_path.write_text(content, encoding="utf-8")
  +
  +
  +def _diff_summary_entry(
  +    *,
  +    run_id: str,
  +    provider: str,
  +    profile: str,
  +    diff_report: Dict[str, Any],
  +) -> Dict[str, Any]:
  +    added_ids = sorted({item.get("id") for item in diff_report.get("added") or [] if item.get("id")})
  +    changed_ids = sorted({item.get("id") for item in diff_report.get("changed") or [] if item.get("id")})
  +    removed_ids = sorted({item.get("id") for item in diff_report.get("removed") or [] if item.get("id")})
  +    counts = diff_report.get("counts") or {}
  +    return {
  +        "run_id": run_id,
  +        "provider": provider,
  +        "profile": profile,
  +        "first_run": not bool(diff_report.get("baseline_exists")),
  +        "prior_run_id": None,
  +        "baseline_resolved": None,
  +        "baseline_source": None,
  +        "counts": {
  +            "new": counts.get("added", 0),
  +            "changed": counts.get("changed", 0),
  +            "removed": counts.get("removed", 0),
  +        },
  +        "new_ids": added_ids,
  +        "changed_ids": changed_ids,
  +        "removed_ids": removed_ids,
  +        "summary_hash": diff_report.get("summary_hash"),
  +    }
  +
  +
  +def _write_diff_summary(run_dir: Path, payload: Dict[str, Any]) -> None:
  +    run_dir.mkdir(parents=True, exist_ok=True)
  +    json_path = run_dir / "diff_summary.json"
  +    md_path = run_dir / "diff_summary.md"
  +    _write_canonical_json(json_path, payload)
  +    lines = ["# Diff Summary"]
  +    provider_profile = payload.get("provider_profile") or {}
  +    for provider in sorted(provider_profile):
  +        profiles = provider_profile.get(provider) or {}
  +        for profile in sorted(profiles):
  +            entry = profiles.get(profile) or {}
  +            counts = entry.get("counts") or {}
  +            label = f"{provider}:{profile}"
  +            lines.append(
  +                f"- {label}: new={counts.get('new', 0)} changed={counts.get('changed', 0)} removed={counts.get('removed', 0)}"
  +            )
  +    md_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
  +
  +
  +def _write_identity_diff_artifacts(run_dir: Path, payload: Dict[str, Any]) -> None:
  +    run_dir.mkdir(parents=True, exist_ok=True)
  +    json_path = run_dir / "diff.json"
  +    md_path = run_dir / "diff.md"
  +    _write_canonical_json(json_path, payload)
  +
  +    lines = ["# Identity Diff", f"run_id: {payload.get('run_id', '')}", ""]
  +    provider_profile = payload.get("provider_profile") or {}
  +    for provider in sorted(provider_profile):
  +        profiles = provider_profile.get(provider) or {}
  +        for profile in sorted(profiles):
  +            entry = profiles.get(profile) or {}
  +            counts = entry.get("counts") or {}
  +            lines.append(f"## {provider}:{profile}")
  +            new_count = counts.get("new", counts.get("added", 0))
  +            changed_count = counts.get("changed", 0)
  +            removed_count = counts.get("removed", 0)
  +            lines.append(f"new={new_count} changed={changed_count} removed={removed_count}")
  +            for key, entry_key in (("new", "added"), ("changed", "changed"), ("removed", "removed")):
  +                items = entry.get(entry_key) or entry.get(key) or []
  +                lines.append(f"### {key}")
  +                if not items:
  +                    lines.append("- _None_")
  +                    continue
  +                for item in items[:10]:
  +                    title = item.get("title") or "Untitled"
  +                    url = item.get("apply_url") or ""
  +                    lines.append(f"- {title} — {url}" if url else f"- {title}")
  +            lines.append("")
  +
  +    md_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
  +
  +
  +def _dispatch_alerts(
  +    profile: str,
  +    webhook: str,
  +    new_jobs: List[Dict[str, Any]],
  +    changed_jobs: List[Dict[str, Any]],
  +    removed_jobs: List[Dict[str, Any]],
  +    interesting_new: List[Dict[str, Any]],
  +    interesting_changed: List[Dict[str, Any]],
  +    lines: List[str],
  +    args: argparse.Namespace,
  +    unavailable_summary: str,
  +) -> None:
  +    total_changes = len(new_jobs) + len(changed_jobs) + len(removed_jobs)
  +    if total_changes == 0:
  +        logger.info(
  +            "No meaningful changes detected (new=%d, changed=%d, removed=%d); skipping Discord alerts.",
  +            len(new_jobs),
  +            len(changed_jobs),
  +            len(removed_jobs),
  +        )
  +        return
  +
  +    if not webhook:
  +        logger.info(f"ℹ️ No alerts ({profile}) (new={len(new_jobs)}, changed={len(changed_jobs)}; webhook=unset).")
  +        return
  +
  +    if not (interesting_new or interesting_changed):
  +        logger.info(f"ℹ️ No alerts ({profile}) (new={len(new_jobs)}, changed={len(changed_jobs)}; webhook=set).")
  +        return
  +
  +    msg_lines = list(lines)
  +    if args.no_post:
  +        msg = "\n".join(msg_lines)
  +        logger.info(f"Skipping Discord post (--no_post). Message for {profile} would have been:\n")
  +        logger.info(msg)
  +        return
  +
  +    if unavailable_summary:
  +        msg_lines.append(f"Unavailable reasons: {unavailable_summary}")
  +
  +    msg = "\n".join(msg_lines)
  +    ok = _post_discord(webhook, msg)
  +    logger.info(f"✅ Discord alert sent ({profile})." if ok else "⚠️ Discord alert NOT sent (pipeline still completed).")
  +
  +
  +def _resolve_notify_mode(raw_mode: Optional[str]) -> str:
  +    if os.environ.get("JOBINTEL_DISCORD_ALWAYS_POST", "").strip() == "1":
  +        return "always"
  +    mode = (raw_mode or "diff").strip().lower()
  +    if mode not in {"diff", "always"}:
  +        return "diff"
  +    return mode
  +
  +
  +def _should_notify(diff_counts: Dict[str, Any], mode: str) -> bool:
  +    if diff_counts.get("suppressed") is True:
  +        return False
  +    if mode == "always":
  +        return True
  +    if diff_counts.get("first_run") is True:
  +        return True
  +    return any(diff_counts.get(k, 0) > 0 for k in ("new", "changed", "removed"))
  +
  +
  +def _maybe_post_run_summary(
  +    provider: str,
  +    profile: str,
  +    ranked_json: Path,
  +    diff_counts: Dict[str, Any],
  +    min_score: int,
  +    *,
  +    notify_mode: str,
  +    no_post: bool,
  +    extra_lines: Optional[List[str]] = None,
  +    diff_items: Optional[Dict[str, List[Dict[str, Any]]]] = None,
  +) -> str:
  +    if not _should_notify(diff_counts, notify_mode):
  +        logger.info("Discord notify skipped (mode=%s, no diffs).", notify_mode)
  +        return "skipped"
  +    return _post_run_summary(
  +        provider,
  +        profile,
  +        ranked_json,
  +        diff_counts,
  +        min_score,
  +        no_post=no_post,
  +        extra_lines=extra_lines,
  +        diff_items=diff_items,
  +    )
  +
  +
  +def _post_discord(webhook_url: str, message: str) -> bool:
  +    """
  +    Returns True if posted successfully, False otherwise.
  +    Never raises (so your pipeline still completes).
  +    """
  +    if not webhook_url or "discord.com/api/webhooks/" not in webhook_url:
  +        logger.warning("⚠️ DISCORD_WEBHOOK_URL missing or doesn't look like a Discord webhook URL. Skipping post.")
  +        return False
  +
  +    payload = {"content": message}
  +    data = json.dumps(payload).encode("utf-8")
  +
  +    req = urllib.request.Request(
  +        webhook_url,
  +        data=data,
  +        method="POST",
  +        headers={
  +            "Content-Type": "application/json",
  +            "Accept": "application/json",
  +            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X) signalcraft/1.0",
  +        },
  +    )
  +
  +    try:
  +        with urllib.request.urlopen(req, timeout=20) as resp:
  +            resp.read()
  +        return True
  +    except urllib.error.HTTPError as e:
  +        body = e.read().decode("utf-8", errors="replace")
  +        logger.error(f"Discord webhook POST failed: {e.code}")
  +        logger.error(body[:2000])
  +        if e.code == 404 and "10015" in body:
  +            logger.warning(
  +                "⚠️ Discord says: Unknown Webhook (rotated/deleted/wrong URL). Update DISCORD_WEBHOOK_URL in .env."
  +            )
  +        return False
  +    except Exception as e:
  +        logger.error(f"Discord webhook POST failed: {e!r}")
  +        return False
  +
  +
  +def _post_failure(
  +    webhook_url: str, stage: str, error: str, no_post: bool, *, stdout: str = "", stderr: str = ""
  +) -> None:
  +    """Best-effort failure notification. Never raises."""
  +    if no_post or not webhook_url:
  +        return
  +
  +    stdout_tail = (stdout or "")[-1800:]
  +    stderr_tail = (stderr or "")[-1800:]
  +
  +    msg = (
  +        "**🚨 Job Pipeline FAILED**\n"
  +        f"Stage: `{stage}`\n"
  +        f"Time: `{_utcnow_iso()}`\n"
  +        f"Error:\n```{error[-1800:]}```"
  +        f"\n\n**stderr (tail)**:\n```{stderr_tail}```"
  +        f"\n\n**stdout (tail)**:\n```{stdout_tail}```"
  +    )
  +    _post_discord(webhook_url, msg)
  +
  +
  +def _post_run_summary(
  +    provider: str,
  +    profile: str,
  +    ranked_json: Path,
  +    diff_counts: Dict[str, int],
  +    min_score: int,
  +    *,
  +    no_post: bool,
  +    extra_lines: Optional[List[str]] = None,
  +    diff_items: Optional[Dict[str, List[Dict[str, Any]]]] = None,
  +) -> str:
  +    if no_post:
  +        return "disabled"
  +    webhook = resolve_webhook(profile)
  +    if not webhook:
  +        logger.info("Discord webhook unset; skipping run summary alert.")
  +        return "unset"
  +    if "discord.com/api/webhooks/" not in webhook:
  +        logger.warning("⚠️ DISCORD_WEBHOOK_URL missing or doesn't look like a Discord webhook URL. Skipping post.")
  +        return "invalid"
  +    message = build_run_summary_message(
  +        provider=provider,
  +        profile=profile,
  +        ranked_json=ranked_json,
  +        diff_counts=diff_counts,
  +        min_score=min_score,
  +        extra_lines=extra_lines,
  +        diff_items=diff_items,
  +        diff_top_n=max(1, int(os.environ.get("JOBINTEL_DISCORD_DIFF_TOP_N", "5"))),
  +    )
  +    ok = post_discord(webhook, message)
  +    return "ok" if ok else "failed"
  +
  +
  +def _briefs_status_line(run_id: str, profile: str) -> Optional[str]:
  +    run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +    path = run_dir / f"ai_job_briefs.{profile}.json"
  +    if not path.exists():
  +        return None
  +    try:
  +        payload = json.loads(path.read_text(encoding="utf-8"))
  +    except Exception:
  +        return None
  +    if not isinstance(payload, dict):
  +        return None
  +    count = len(payload.get("briefs") or [])
  +    status = payload.get("status") or "unknown"
  +    if status != "ok":
  +        return f"AI briefs: {status}"
  +    return f"AI briefs: generated for top {count}"
  +
  +
  +def _safe_int_env(name: str, default: int = 0) -> int:
  +    raw = (os.environ.get(name) or "").strip()
  +    if not raw:
  +        return default
  +    try:
  +        value = int(raw)
  +    except ValueError:
  +        return default
  +    return max(0, value)
  +
  +
  +def _estimate_tokens_from_text(text: str) -> int:
  +    return max(1, len(text) // 4)
  +
  +
  +def _collect_run_costs(
  +    *,
  +    run_id: str,
  +    profiles: List[str],
  +    semantic_summary: Dict[str, Any],
  +    embedding_token_estimate_per_item: int = 128,
  +) -> Dict[str, int]:
  +    run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +    embeddings_count = int((semantic_summary.get("embedded_job_count") or 0) or 0)
  +    embeddings_estimated_tokens = max(0, embeddings_count) * max(1, int(embedding_token_estimate_per_item))
  +    ai_calls = 0
  +    ai_estimated_tokens = 0
  +
  +    for profile in profiles:
  +        insights_path = run_dir / f"ai_insights.{profile}.json"
  +        if insights_path.exists():
  +            try:
  +                insights_payload = json.loads(insights_path.read_text(encoding="utf-8"))
  +            except Exception:
  +                insights_payload = None
  +            if isinstance(insights_payload, dict) and str(insights_payload.get("status") or "") == "ok":
  +                ai_calls += 1
  +                structured_input = insights_payload.get("structured_input") or {}
  +                ai_estimated_tokens += _estimate_tokens_from_text(
  +                    json.dumps(structured_input, sort_keys=True, separators=(",", ":"), ensure_ascii=False)
  +                )
  +
  +        briefs_path = run_dir / f"ai_job_briefs.{profile}.json"
  +        if briefs_path.exists():
  +            try:
  +                briefs_payload = json.loads(briefs_path.read_text(encoding="utf-8"))
  +            except Exception:
  +                briefs_payload = None
  +            if isinstance(briefs_payload, dict) and str(briefs_payload.get("status") or "") == "ok":
  +                briefs = briefs_payload.get("briefs") or []
  +                ai_calls += len(briefs) if isinstance(briefs, list) else 0
  +                meta = briefs_payload.get("metadata") if isinstance(briefs_payload.get("metadata"), dict) else {}
  +                ai_estimated_tokens += int((meta or {}).get("estimated_tokens_used", 0) or 0)
  +
  +    return {
  +        "embeddings_count": embeddings_count,
  +        "embeddings_estimated_tokens": embeddings_estimated_tokens,
  +        "ai_calls": ai_calls,
  +        "ai_estimated_tokens": ai_estimated_tokens,
  +        "total_estimated_tokens": embeddings_estimated_tokens + ai_estimated_tokens,
  +    }
  +
  +
  +def _write_costs_artifact(run_id: str, payload: Dict[str, int]) -> Path:
  +    run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +    path = run_dir / "costs.json"
  +    path.parent.mkdir(parents=True, exist_ok=True)
  +    _write_canonical_json(path, payload)
  +    return path
  +
  +
  +def _all_providers_unavailable(provenance_by_provider: Dict[str, Dict[str, Any]], providers: List[str]) -> bool:
  +    if not providers:
  +        return False
  +    for provider in providers:
  +        meta = provenance_by_provider.get(provider) or {}
  +        if meta.get("availability") != "unavailable":
  +            return False
  +    return True
  +
  +
  +def _provider_unavailable_line(provider: str, meta: Dict[str, Any]) -> Optional[str]:
  +    if meta.get("availability") != "unavailable":
  +        return None
  +    reason = meta.get("unavailable_reason") or "unknown"
  +    attempts = meta.get("attempts_made")
  +    if attempts is None:
  +        return f"Provider unavailable: {provider} ({reason})"
  +    return f"Provider unavailable: {provider} ({reason}, attempts={attempts})"
  +
  +
  +def _resolve_profiles(args: argparse.Namespace) -> List[str]:
  +    """Resolve --profiles (comma-separated) else fallback to --profile."""
  +    profiles_arg = (args.profiles or "").strip()
  +    if profiles_arg:
  +        profiles = [p.strip() for p in profiles_arg.split(",") if p.strip()]
  +    else:
  +        profiles = [args.profile.strip()]
  +
  +    # de-dupe while preserving order
  +    seen = set()
  +    out: List[str] = []
  +    for p in profiles:
  +        if p not in seen:
  +            seen.add(p)
  +            out.append(p)
  +
  +    if not out:
  +        raise SystemExit("No profiles provided.")
  +    return out
  +
  +
  +def _resolve_history_settings(args: argparse.Namespace) -> Tuple[bool, int, int]:
  +    env_enabled = os.environ.get("HISTORY_ENABLED", "").strip() == "1"
  +    enabled = bool(args.history_enabled) or env_enabled
  +
  +    keep_runs_raw = (
  +        str(args.history_keep_runs)
  +        if args.history_keep_runs is not None
  +        else os.environ.get("HISTORY_KEEP_RUNS", "30").strip()
  +    )
  +    keep_days_raw = (
  +        str(args.history_keep_days)
  +        if args.history_keep_days is not None
  +        else os.environ.get("HISTORY_KEEP_DAYS", "90").strip()
  +    )
  +    try:
  +        keep_runs = int(keep_runs_raw)
  +        keep_days = int(keep_days_raw)
  +    except ValueError as exc:
  +        raise SystemExit(f"HISTORY_KEEP_RUNS/HISTORY_KEEP_DAYS must be integers: {exc}") from exc
  +    if keep_runs < 1:
  +        raise SystemExit("HISTORY_KEEP_RUNS must be >= 1")
  +    if keep_days < 1:
  +        raise SystemExit("HISTORY_KEEP_DAYS must be >= 1")
  +    return enabled, keep_runs, keep_days
  +
  +
  +def _resolve_log_file_enabled(args: argparse.Namespace) -> bool:
  +    if getattr(args, "log_file", False):
  +        return True
  +    env_value = os.environ.get("JOBINTEL_LOG_FILE", "").strip().lower()
  +    return env_value in {"1", "true", "yes", "on"}
  +
  +
  +def _resolve_semantic_settings() -> Dict[str, Any]:
  +    enabled = os.environ.get("SEMANTIC_ENABLED", "").strip() == "1"
  +    semantic_mode = (os.environ.get("SEMANTIC_MODE") or "boost").strip().lower()
  +    if semantic_mode not in {"sidecar", "boost"}:
  +        semantic_mode = "boost"
  +    model_id = (os.environ.get("SEMANTIC_MODEL_ID") or DEFAULT_SEMANTIC_MODEL_ID).strip() or DEFAULT_SEMANTIC_MODEL_ID
  +    max_jobs_raw = (os.environ.get("SEMANTIC_MAX_JOBS") or "200").strip()
  +    try:
  +        max_jobs = int(max_jobs_raw)
  +    except ValueError:
  +        max_jobs = 200
  +    if max_jobs < 1:
  +        max_jobs = 1
  +    try:
  +        max_boost = float((os.environ.get("SEMANTIC_MAX_BOOST") or "5").strip())
  +    except ValueError:
  +        max_boost = 5.0
  +    try:
  +        min_similarity = float((os.environ.get("SEMANTIC_MIN_SIMILARITY") or "0.72").strip())
  +    except ValueError:
  +        min_similarity = 0.72
  +    try:
  +        top_k = int((os.environ.get("SEMANTIC_TOP_K") or "50").strip())
  +    except ValueError:
  +        top_k = 50
  +    if top_k < 1:
  +        top_k = 1
  +    return {
  +        "enabled": enabled,
  +        "mode": semantic_mode,
  +        "model_id": model_id,
  +        "max_jobs": max_jobs,
  +        "max_boost": max(0.0, max_boost),
  +        "min_similarity": max(0.0, min(1.0, min_similarity)),
  +        "top_k": top_k,
  +    }
  +
  +
  +def _resolve_run_id() -> str:
  +    override = (os.environ.get("JOBINTEL_RUN_ID") or "").strip()
  +    if override:
  +        return override
  +    return _utcnow_iso()
  +
  +
  +def main() -> int:
  +    ensure_dirs()
  +    ap = argparse.ArgumentParser(description="Run the SignalCraft daily pipeline (JIE engine).")
  +
  +    ap.add_argument("--profile", default="cs", help="Scoring profile name (cs|tam|se)")
  +    ap.add_argument(
  +        "--profiles",
  +        default="",
  +        help="Comma-separated profiles to run (e.g. cs or cs,tam,se). If set, overrides --profile.",
  +    )
  +    ap.add_argument(
  +        "--providers",
  +        default="openai",
  +        help="Comma-separated provider ids to run (default: openai).",
  +    )
  +    ap.add_argument(
  +        "--providers-config",
  +        default=str(Path("config") / "providers.json"),
  +        help="Path to providers config JSON.",
  +    )
  +    ap.add_argument("--us_only", action="store_true")
  +    ap.add_argument("--min_alert_score", type=int, default=85)
  +    ap.add_argument("--min_score", type=int, default=40, help="Shortlist minimum score threshold.")
  +    ap.add_argument("--offline", action="store_true", help="Force snapshot mode (no live scraping).")
  +    ap.add_argument(
  +        "--snapshot-only",
  +        action="store_true",
  +        help="Fail if any provider would use live scraping; enforce snapshot-only determinism.",
  +    )
  +    ap.add_argument("--no_post", action="store_true", help="Run pipeline but do not send Discord webhook")
  +    ap.add_argument("--test_post", action="store_true", help="Send a test message to Discord and exit")
  +    ap.add_argument("--no_enrich", action="store_true", help="Skip enrichment step (CI / offline safe)")
  +    ap.add_argument("--ai", action="store_true", help="Run AI augment stage after enrichment")
  +    ap.add_argument("--ai_only", action="store_true", help="Run enrich + AI augment only (no scoring/alerts)")
  +    ap.add_argument("--scrape_only", action="store_true", help="Run scrape stage only (no classify/enrich/score)")
  +    ap.add_argument(
  +        "--no_subprocess",
  +        action="store_true",
  +        help="Run stages in-process (library mode). Default uses subprocesses.",
  +    )
  +    ap.add_argument("--log_json", action="store_true", help="Emit JSON logs for aggregation systems")
  +    ap.add_argument(
  +        "--log_file",
  +        action="store_true",
  +        help="Write structured JSONL logs to state/runs/<run_id>/logs/run.log.jsonl (also enable via JOBINTEL_LOG_FILE=1).",
  +    )
  +    ap.add_argument("--print_paths", action="store_true", help="Print resolved data/state/history paths")
  +    ap.add_argument("--publish-s3", action="store_true", help="Publish run artifacts to S3 after completion.")
  +    ap.add_argument(
  +        "--publish-dry-run",
  +        action="store_true",
  +        help="Plan S3 publish without uploading (requires --publish-s3 or implies it).",
  +    )
  +    ap.add_argument(
  +        "--history-enabled",
  +        action="store_true",
  +        help="Enable canonical history pointers + deterministic retention under state/history/<profile>/.",
  +    )
  +    ap.add_argument(
  +        "--history-keep-runs",
  +        type=int,
  +        default=None,
  +        help="Retention: keep this many recent run pointers per profile (env fallback: HISTORY_KEEP_RUNS=30).",
  +    )
  +    ap.add_argument(
  +        "--history-keep-days",
  +        type=int,
  +        default=None,
  +        help="Retention: keep this many recent daily pointers per profile (env fallback: HISTORY_KEEP_DAYS=90).",
  +    )
  +
  +    args = ap.parse_args()
  +    history_enabled, history_keep_runs, history_keep_days = _resolve_history_settings(args)
  +    if args.snapshot_only and not args.offline:
  +        args.offline = True
  +    global OUTPUT_DIR, RAW_JOBS_JSON, LABELED_JOBS_JSON, ENRICHED_JOBS_JSON
  +    OUTPUT_DIR = _resolve_output_dir()
  +    os.environ.setdefault("JOBINTEL_OUTPUT_DIR", str(OUTPUT_DIR))
  +    if RAW_JOBS_JSON.parent != OUTPUT_DIR:
  +        RAW_JOBS_JSON = OUTPUT_DIR / RAW_JOBS_JSON.name
  +        LABELED_JOBS_JSON = OUTPUT_DIR / LABELED_JOBS_JSON.name
  +        ENRICHED_JOBS_JSON = OUTPUT_DIR / ENRICHED_JOBS_JSON.name
  +    providers = _resolve_providers(args)
  +    openai_only = providers == ["openai"]
  +    run_id = _resolve_run_id()
  +    print(f"JOBINTEL_RUN_ID={run_id}", flush=True)
  +    global USE_SUBPROCESS
  +    USE_SUBPROCESS = not args.no_subprocess
  +    log_file_enabled = _resolve_log_file_enabled(args)
  +    log_file_path = _run_logs_dir(run_id) / "run.log.jsonl" if log_file_enabled else None
  +    log_file_pointer = _setup_logging(args.log_json, file_sink_path=log_file_path)
  +    run_log_pointers = _collect_run_log_pointers(run_id, log_file_pointer)
  +    webhook = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
  +    notify_mode = _resolve_notify_mode(os.environ.get("DISCORD_NOTIFY_MODE"))
  +
  +    validate_config(args, webhook)
  +
  +    if args.print_paths:
  +        print("DATA_DIR=", DATA_DIR)
  +        print("STATE_DIR=", STATE_DIR)
  +        print("HISTORY_DIR=", HISTORY_DIR)
  +        print("RUN_METADATA_DIR=", RUN_METADATA_DIR)
  +        return 0
  +
  +    if args.test_post:
  +        if not webhook:
  +            raise SystemExit("DISCORD_WEBHOOK_URL not set (check .env and export).")
  +        ok = _post_discord(webhook, "test_post ✅ (run_daily)")
  +        logger.info("✅ test_post sent" if ok else "⚠️ test_post failed")
  +        return 0
  +
  +    _acquire_lock(timeout_sec=0)
  +    logger.info(f"===== jobintel start {_utcnow_iso()} pid={os.getpid()} =====")
  +
  +    telemetry: Dict[str, Any] = {
  +        "run_id": run_id,
  +        "started_at": _utcnow_iso(),
  +        "status": "started",
  +        "stages": {},
  +        "ai_requested": bool(args.ai),
  +        "ai_ran": False,
  +    }
  +    prev_run = _load_last_run()
  +    prev_hashes = prev_run.get("hashes", {}) if prev_run else {}
  +    prev_ai = prev_run.get("ai", {}) if prev_run else {}
  +    curr_hashes = {
  +        "raw": _hash_file(_provider_raw_jobs_json("openai")),
  +        "labeled": _hash_file(_provider_labeled_jobs_json("openai")),
  +        "enriched": _hash_file(_provider_enriched_jobs_json("openai")),
  +    }
  +    ai_path = _provider_ai_jobs_json("openai")
  +    ai_hash = _hash_file(ai_path)
  +    ai_mtime = _file_mtime(ai_path)
  +
  +    profiles_list: List[str] = []
  +    diff_counts_by_profile: Dict[str, Dict[str, Any]] = {}
  +    scoring_inputs_by_profile: Dict[str, Dict[str, Optional[str]]] = {}
  +    scoring_input_selection_by_profile: Dict[str, Dict[str, Any]] = {}
  +    diff_counts_by_provider: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    diff_summary_by_provider_profile: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    diff_report_by_provider_profile: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    user_state_counts_by_provider_profile: Dict[str, Dict[str, Dict[str, int]]] = {}
  +    scoring_inputs_by_provider: Dict[str, Dict[str, Dict[str, Optional[str]]]] = {}
  +    scoring_input_selection_by_provider: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    archived_inputs_by_provider_profile: Dict[str, Dict[str, Dict[str, Any]]] = {}
  +    provenance_by_provider: Dict[str, Dict[str, Any]] = {}
  +    discord_status_by_provider: Dict[str, Dict[str, str]] = {}
  +    provider_policy_lines: Dict[str, str] = {}
  +    semantic_settings = _resolve_semantic_settings()
  +    flag_payload = {
  +        "profile": args.profile,
  +        "profiles": args.profiles,
  +        "providers": providers,
  +        "us_only": args.us_only,
  +        "no_enrich": args.no_enrich,
  +        "ai": args.ai,
  +        "ai_only": args.ai_only,
  +        "min_score": args.min_score,
  +        "min_alert_score": args.min_alert_score,
  +        "snapshot_only": args.snapshot_only,
  +        "publish_s3": args.publish_s3,
  +        "publish_dry_run": args.publish_dry_run,
  +        "history_enabled": history_enabled,
  +        "history_keep_runs": history_keep_runs,
  +        "history_keep_days": history_keep_days,
  +    }
  +
  +    def _finalize(status: str, extra: Optional[Dict[str, Any]] = None) -> str:
  +        final_status = status
  +        telemetry["status"] = final_status
  +        telemetry["hashes"] = {
  +            "raw": _hash_file(_provider_raw_jobs_json("openai")),
  +            "labeled": _hash_file(_provider_labeled_jobs_json("openai")),
  +            "enriched": _hash_file(_provider_enriched_jobs_json("openai")),
  +        }
  +        telemetry["counts"] = {
  +            "raw": _safe_len(_provider_raw_jobs_json("openai")),
  +            "labeled": _safe_len(_provider_labeled_jobs_json("openai")),
  +            "enriched": _safe_len(_provider_enriched_jobs_json("openai")),
  +        }
  +        # First-class AI telemetry (even on short-circuit runs).
  +        telemetry["ai_requested"] = bool(telemetry.get("ai_requested", False))
  +        telemetry["ai_ran"] = bool(telemetry.get("ai_ran", False))
  +        telemetry["ai_output_hash"] = _hash_file(ai_path)
  +        telemetry["ai_output_mtime"] = _file_mtime_iso(ai_path)
  +        # Back-compat nested structure (keep for existing readers).
  +        telemetry["ai"] = {
  +            "ran": telemetry["ai_ran"],
  +            "output_hash": telemetry["ai_output_hash"],
  +            "output_mtime": telemetry["ai_output_mtime"],
  +        }
  +        telemetry["ended_at"] = _utcnow_iso()
  +        telemetry["success"] = final_status == "success"
  +        if extra:
  +            telemetry.update(extra)
  +        _write_last_run(telemetry)
  +        provider_inputs: Dict[str, Dict[str, Dict[str, Optional[str]]]] = {}
  +        provider_outputs: Dict[str, Dict[str, Dict[str, Dict[str, Optional[str]]]]] = {}
  +        for provider in providers:
  +            inputs: Dict[str, Dict[str, Optional[str]]] = {
  +                "raw_jobs_json": _file_metadata(_provider_raw_jobs_json(provider)),
  +                "labeled_jobs_json": _file_metadata(_provider_labeled_jobs_json(provider)),
  +                "enriched_jobs_json": _file_metadata(_provider_enriched_jobs_json(provider)),
  +            }
  +            ai_path_local = _provider_ai_jobs_json(provider)
  +            if ai_path_local.exists():
  +                inputs["ai_enriched_jobs_json"] = _file_metadata(ai_path_local)
  +            provider_inputs[provider] = inputs
  +
  +            outputs_for_provider: Dict[str, Dict[str, Dict[str, Optional[str]]]] = {}
  +            for profile in profiles_list:
  +                outputs_for_provider[profile] = {
  +                    "ranked_json": _output_metadata(_provider_ranked_jobs_json(provider, profile)),
  +                    "ranked_csv": _output_metadata(_provider_ranked_jobs_csv(provider, profile)),
  +                    "ranked_families_json": _output_metadata(_provider_ranked_families_json(provider, profile)),
  +                    "shortlist_md": _output_metadata(_provider_shortlist_md(provider, profile)),
  +                    "top_md": _output_metadata(_provider_top_md(provider, profile)),
  +                }
  +            provider_outputs[provider] = outputs_for_provider
  +
  +        config_fingerprint = _config_fingerprint(flag_payload, args.providers_config)
+++        provider_registry = _provider_registry_provenance(args.providers_config)
  +        environment_fingerprint = _environment_fingerprint()
  +        log_retention_summary: Dict[str, Any] = {
  +            "enabled": bool(history_enabled),
  +            "keep_runs": history_keep_runs,
  +            "runs_seen": 0,
  +            "runs_kept": 0,
  +            "log_dirs_pruned": 0,
  +            "pruned_log_dirs": [],
  +            "reason": "pending",
  +        }
  +        delta_summary = _build_delta_summary(run_id, providers, profiles_list)
  +        for provider, profiles in diff_summary_by_provider_profile.items():
  +            for profile, entry in profiles.items():
  +                delta = delta_summary.get("provider_profile", {}).get(provider, {}).get(profile, {})
  +                entry["prior_run_id"] = delta.get("baseline_run_id")
  +                entry["baseline_resolved"] = delta.get("baseline_resolved")
  +                entry["baseline_source"] = delta.get("baseline_source")
  +        run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +        if diff_summary_by_provider_profile:
  +            _write_diff_summary(
  +                run_dir,
  +                {
  +                    "run_id": run_id,
  +                    "generated_at": _utcnow_iso(),
  +                    "provider_profile": diff_summary_by_provider_profile,
  +                },
  +            )
  +            _write_identity_diff_artifacts(
  +                run_dir,
  +                {
  +                    "run_id": run_id,
  +                    "generated_at": _utcnow_iso(),
  +                    "provider_profile": diff_report_by_provider_profile,
  +                },
  +            )
  +        semantic_summary, semantic_summary_path, semantic_scores_path = finalize_semantic_artifacts(
  +            run_id=run_id,
  +            run_metadata_dir=RUN_METADATA_DIR,
  +            enabled=bool(semantic_settings["enabled"]),
  +            model_id=str(semantic_settings["model_id"]),
  +            policy={
  +                "max_jobs": int(semantic_settings["max_jobs"]),
  +                "top_k": int(semantic_settings["top_k"]),
  +                "max_boost": float(semantic_settings["max_boost"]),
  +                "min_similarity": float(semantic_settings["min_similarity"]),
  +            },
  +        )
  +        telemetry["semantic"] = {
  +            "enabled": semantic_summary.get("enabled"),
  +            "mode": str(semantic_settings["mode"]),
  +            "model_id": semantic_summary.get("model_id"),
  +            "embedded_job_count": semantic_summary.get("embedded_job_count"),
  +            "skipped_reason": semantic_summary.get("skipped_reason"),
  +            "summary_path": str(semantic_summary_path),
  +            "scores_path": str(semantic_scores_path),
  +        }
  +        costs_payload = _collect_run_costs(
  +            run_id=run_id,
  +            profiles=profiles_list,
  +            semantic_summary=semantic_summary,
  +        )
  +        costs_path = _write_costs_artifact(run_id, costs_payload)
  +        telemetry["costs"] = {
  +            **costs_payload,
  +            "path": str(costs_path),
  +        }
  +
  +        max_ai_tokens_per_run = _safe_int_env("MAX_AI_TOKENS_PER_RUN", 0)
  +        max_embeddings_per_run = _safe_int_env("MAX_EMBEDDINGS_PER_RUN", 0)
  +        budget_errors: List[str] = []
  +        if max_ai_tokens_per_run > 0 and costs_payload["ai_estimated_tokens"] > max_ai_tokens_per_run:
  +            budget_errors.append(
  +                f"ai_estimated_tokens={costs_payload['ai_estimated_tokens']} > MAX_AI_TOKENS_PER_RUN={max_ai_tokens_per_run}"
  +            )
  +        if max_embeddings_per_run > 0 and costs_payload["embeddings_count"] > max_embeddings_per_run:
  +            budget_errors.append(
  +                f"embeddings_count={costs_payload['embeddings_count']} > MAX_EMBEDDINGS_PER_RUN={max_embeddings_per_run}"
  +            )
  +        if budget_errors and final_status != "error":
  +            final_status = "error"
  +            telemetry["status"] = final_status
  +            telemetry["success"] = False
  +            telemetry["failed_stage"] = "cost_guardrails"
  +            telemetry["error"] = "; ".join(budget_errors)
  +            telemetry["cost_guardrails"] = {
  +                "max_ai_tokens_per_run": max_ai_tokens_per_run,
  +                "max_embeddings_per_run": max_embeddings_per_run,
  +                "violations": budget_errors,
  +            }
  +            _write_last_run(telemetry)
  +            logger.error("Cost guardrail violation: %s", telemetry["error"])
  +        run_metadata_path = _persist_run_metadata(
  +            run_id,
  +            telemetry,
  +            profiles_list,
  +            flag_payload,
  +            diff_counts_by_profile,
  +            provenance_by_provider,
  +            scoring_inputs_by_profile,
  +            scoring_input_selection_by_profile,
  +            archived_inputs_by_provider_profile=archived_inputs_by_provider_profile,
  +            providers=providers,
  +            inputs_by_provider=provider_inputs,
  +            scoring_inputs_by_provider=scoring_inputs_by_provider,
  +            scoring_input_selection_by_provider=scoring_input_selection_by_provider,
  +            outputs_by_provider=provider_outputs,
  +            delta_summary=delta_summary,
  +            user_state_counts_by_provider_profile=user_state_counts_by_provider_profile,
  +            config_fingerprint=config_fingerprint,
+++            provider_registry=provider_registry,
  +            environment_fingerprint=environment_fingerprint,
  +            logs=run_log_pointers,
  +            log_retention=log_retention_summary,
  +            semantic_contract={
  +                "semantic_enabled": bool(semantic_settings["enabled"]),
  +                "semantic_mode": str(semantic_settings["mode"]),
  +                "semantic_model_id": str(semantic_settings["model_id"]),
  +                "semantic_threshold": float(semantic_settings["min_similarity"]),
  +                "semantic_max_boost": float(semantic_settings["max_boost"]),
  +                "embedding_backend_version": EMBEDDING_BACKEND_VERSION,
  +            },
  +        )
+++        run_report_payload = json.loads(run_metadata_path.read_text(encoding="utf-8"))
+++        if isinstance(run_report_payload, dict):
+++            artifact_model_meta = _write_artifact_model_v2(run_metadata_path, run_report_payload)
+++            _update_run_metadata_artifact_model(run_metadata_path, artifact_model_meta)
  +        if telemetry.get("success", False):
  +            try:
  +                run_report_payload = json.loads(run_metadata_path.read_text(encoding="utf-8"))
  +                if isinstance(run_report_payload, dict):
  +                    _write_last_success_pointer(run_report_payload, run_metadata_path)
  +            except Exception as exc:
  +                logger.warning("Failed to write last_success pointer: %r", exc)
  +        if openai_only:
  +            for profile in profiles_list:
  +                diffs = diff_counts_by_profile.get(profile, {"new": 0, "changed": 0, "removed": 0})
  +                summary_payload = {
  +                    "run_id": run_id,
  +                    "timestamp": telemetry["ended_at"],
  +                    "profile": profile,
  +                    "flags": flag_payload,
  +                    "short_circuit": telemetry["status"] == "short_circuit",
  +                    "diff_counts": diffs,
  +                }
  +                _archive_profile_artifacts(
  +                    run_id,
  +                    profile,
  +                    run_metadata_path,
  +                    summary_payload,
  +                )
  +        else:
  +            for provider in providers:
  +                for profile in profiles_list:
  +                    diffs = diff_counts_by_provider.get(provider, {}).get(
  +                        profile, {"new": 0, "changed": 0, "removed": 0}
  +                    )
  +                    summary_payload = {
  +                        "run_id": run_id,
  +                        "timestamp": telemetry["ended_at"],
  +                        "profile": profile,
  +                        "provider": provider,
  +                        "flags": flag_payload,
  +                        "short_circuit": telemetry["status"] == "short_circuit",
  +                        "diff_counts": diffs,
  +                    }
  +                    _archive_profile_artifacts(
  +                        run_id,
  +                        profile,
  +                        run_metadata_path,
  +                        summary_payload,
  +                        provider=provider,
  +                    )
  +
  +        _write_run_registry(
  +            run_id,
  +            providers,
  +            profiles_list,
  +            run_metadata_path,
  +            diff_counts_by_provider,
  +            telemetry,
  +        )
  +
  +        s3_meta: Dict[str, Any] = {"status": "disabled"}
  +        s3_failed = False
  +        s3_exit_code: Optional[int] = None
  +        dry_run = False
  +        publish_requested_env = os.environ.get("PUBLISH_S3", "0").strip() == "1"
  +        publish_requested_cli = bool(args.publish_s3 or args.publish_dry_run)
  +        publish_requested = publish_requested_env or publish_requested_cli
  +        publish_required_env = (
  +            os.environ.get("PUBLISH_S3_REQUIRE", "0").strip() == "1"
  +            or os.environ.get("JOBINTEL_PUBLISH_REQUIRE", "0").strip() == "1"
  +        )
  +        publish_required = publish_required_env
  +        resolved_bucket, resolved_prefix = publish_s3._resolve_bucket_prefix(None, None)
  +        publish_enabled, require_s3, skip_reason = _resolve_publish_state(
  +            publish_requested, resolved_bucket, publish_required
  +        )
  +        if status != "success":
  +            skip_reason = f"skipped_status_{status}"
  +            publish_enabled = False
  +            require_s3 = False
  +            s3_meta = {"status": "skipped", "reason": skip_reason}
  +        elif skip_reason:
  +            if skip_reason == "missing_bucket_required":
  +                err = "S3 publish required but JOBINTEL_S3_BUCKET is unset."
  +                logger.error(err)
  +                s3_meta = {"status": "error", "reason": "missing_bucket", "error": err}
  +                s3_exit_code = 2
  +                s3_failed = True
  +            else:
  +                logger.warning("S3 publish requested but bucket is unset; skipping.")
  +                s3_meta = {"status": "skipped", "reason": skip_reason}
  +
  +        if publish_enabled:
  +            dry_run = bool(args.publish_dry_run) or os.environ.get("PUBLISH_S3_DRY_RUN", "0").strip() == "1"
  +            logger.info(
  +                "S3 publish enabled: s3://%s/%s (dry_run=%s require=%s)",
  +                resolved_bucket,
  +                resolved_prefix,
  +                dry_run,
  +                require_s3,
  +            )
  +            try:
  +                preflight = publish_s3._run_preflight(
  +                    bucket=None,
  +                    region=None,
  +                    prefix=None,
  +                    dry_run=dry_run,
  +                )
  +                if not preflight.get("ok"):
  +                    logger.error("AWS preflight failed: %s", ", ".join(preflight.get("errors", [])))
  +                    s3_meta = {"status": "error", "reason": "preflight_failed", "preflight": preflight}
  +                    s3_exit_code = 2
  +                    s3_failed = require_s3
  +                else:
  +                    s3_meta = publish_s3.publish_run(
  +                        run_id=run_id,
  +                        bucket=None,
  +                        prefix=None,
  +                        candidate_id=CANDIDATE_ID,
  +                        run_dir=RUN_METADATA_DIR / publish_s3._sanitize_run_id(run_id),
  +                        dry_run=dry_run,
  +                        require_s3=require_s3,
  +                        providers=providers,
  +                        profiles=profiles_list,
  +                        write_last_success=bool(telemetry.get("success", False)),
  +                    )
  +                    if isinstance(s3_meta, dict) and s3_meta.get("status") == "ok":
  +                        _update_run_metadata_s3(run_metadata_path, s3_meta)
  +            except SystemExit as exc:
  +                s3_meta = {"status": "error", "reason": "publish_failed"}
  +                s3_exit_code = _normalize_exit_code(exc.code)
  +                s3_failed = require_s3
  +            except Exception as exc:
  +                s3_meta = {"status": "error", "reason": f"publish_failed:{exc.__class__.__name__}"}
  +                s3_exit_code = 2
  +                s3_failed = require_s3
  +        else:
  +            if publish_requested and skip_reason:
  +                logger.info("S3 publish skipped (%s).", skip_reason)
  +            elif not publish_requested:
  +                logger.info("S3 publish not requested (PUBLISH_S3 != 1).")
  +            else:
  +                logger.info("S3 publish disabled.")
  +
  +        required_contract = require_s3 and not dry_run
  +        publish_section = _build_publish_section(
  +            s3_meta=s3_meta,
  +            enabled=publish_enabled,
  +            required=required_contract,
  +            bucket=resolved_bucket or None,
  +            prefix=resolved_prefix or None,
  +            skip_reason=skip_reason,
  +        )
  +        if _publish_contract_failed(publish_section):
  +            s3_failed = True
  +            s3_exit_code = s3_exit_code or 2
  +            _update_run_metadata_publish(
  +                run_metadata_path,
  +                publish_section,
  +                success_override=False,
  +                status_override="failed",
  +            )
  +        else:
  +            _update_run_metadata_publish(run_metadata_path, publish_section)
  +        logger.info(
  +            "PUBLISH_CONTRACT enabled=%s required=%s bucket=%s prefix=%s pointer_global=%s pointer_profiles=%s error=%s",
  +            publish_section.get("enabled"),
  +            publish_section.get("required"),
  +            publish_section.get("bucket"),
  +            publish_section.get("prefix"),
  +            (publish_section.get("pointer_write") or {}).get("global"),
  +            json.dumps((publish_section.get("pointer_write") or {}).get("provider_profile", {}), sort_keys=True),
  +            (publish_section.get("pointer_write") or {}).get("error"),
  +        )
  +
  +        history_summary: Dict[str, Any] = {
  +            "enabled": history_enabled,
  +            "keep_runs": history_keep_runs,
  +            "keep_days": history_keep_days,
  +            "profiles": {},
  +        }
  +        if status == "success" and history_enabled:
  +            unique_profiles = sorted(set(profiles_list))
  +            for profile in unique_profiles:
  +                artifact_result = None
  +                try:
  +                    artifact_result = write_history_run_artifacts(
  +                        history_dir=HISTORY_DIR,
  +                        run_id=run_id,
  +                        profile=profile,
  +                        run_report_path=run_metadata_path,
  +                        written_at=str(telemetry.get("ended_at") or _utcnow_iso()),
  +                    )
  +                except Exception as exc:
  +                    logger.warning(
  +                        "Failed to write history artifacts for profile=%s run_id=%s: %r", profile, run_id, exc
  +                    )
  +                result = update_history_retention(
  +                    history_dir=HISTORY_DIR,
  +                    runs_dir=RUN_METADATA_DIR,
  +                    profile=profile,
  +                    run_id=run_id,
  +                    run_timestamp=str(telemetry.get("ended_at") or ""),
  +                    keep_runs=history_keep_runs,
  +                    keep_days=history_keep_days,
  +                    written_at=str(telemetry.get("ended_at") or _utcnow_iso()),
  +                )
  +                history_summary["profiles"][profile] = {
  +                    "identity_map_path": artifact_result.identity_map_path if artifact_result else None,
  +                    "provenance_path": artifact_result.provenance_path if artifact_result else None,
  +                    "identity_count": artifact_result.identity_count if artifact_result else 0,
  +                    "run_pointer_path": result.run_pointer_path,
  +                    "daily_pointer_path": result.daily_pointer_path,
  +                    "runs_kept": result.runs_kept,
  +                    "runs_pruned": result.runs_pruned,
  +                    "daily_kept": result.daily_kept,
  +                    "daily_pruned": result.daily_pruned,
  +                }
  +                logger.info(
  +                    "HISTORY_RETENTION profile=%s run_id=%s enabled=1 keep_runs=%d keep_days=%d runs_kept=%d runs_pruned=%d daily_kept=%d daily_pruned=%d identity_count=%d identity_map=%s provenance=%s run_pointer=%s daily_pointer=%s",
  +                    profile,
  +                    run_id,
  +                    history_keep_runs,
  +                    history_keep_days,
  +                    result.runs_kept,
  +                    result.runs_pruned,
  +                    result.daily_kept,
  +                    result.daily_pruned,
  +                    artifact_result.identity_count if artifact_result else 0,
  +                    artifact_result.identity_map_path if artifact_result else "n/a",
  +                    artifact_result.provenance_path if artifact_result else "n/a",
  +                    result.run_pointer_path,
  +                    result.daily_pointer_path,
  +                )
  +        else:
  +            reason = "disabled"
  +            if status != "success":
  +                reason = f"status_{status}"
  +            logger.info(
  +                "HISTORY_RETENTION enabled=%d reason=%s keep_runs=%d keep_days=%d run_id=%s",
  +                1 if history_enabled else 0,
  +                reason,
  +                history_keep_runs,
  +                history_keep_days,
  +                run_id,
  +            )
  +
  +        if history_enabled:
  +            try:
  +                log_retention_summary = _enforce_run_log_retention(
  +                    runs_dir=RUN_METADATA_DIR, keep_runs=history_keep_runs
  +                )
  +                log_retention_summary["enabled"] = True
  +                log_retention_summary["reason"] = "history_keep_runs"
  +                logger.info(
  +                    "LOG_RETENTION enabled=1 keep_runs=%d runs_seen=%d runs_kept=%d log_dirs_pruned=%d run_id=%s",
  +                    history_keep_runs,
  +                    log_retention_summary.get("runs_seen", 0),
  +                    log_retention_summary.get("runs_kept", 0),
  +                    log_retention_summary.get("log_dirs_pruned", 0),
  +                    run_id,
  +                )
  +            except Exception as exc:
  +                log_retention_summary = {
  +                    "enabled": True,
  +                    "keep_runs": history_keep_runs,
  +                    "runs_seen": 0,
  +                    "runs_kept": 0,
  +                    "log_dirs_pruned": 0,
  +                    "pruned_log_dirs": [],
  +                    "reason": "error",
  +                    "error": repr(exc),
  +                }
  +                logger.warning("LOG_RETENTION failed run_id=%s: %r", run_id, exc)
  +        else:
  +            log_retention_summary = {
  +                "enabled": False,
  +                "keep_runs": history_keep_runs,
  +                "runs_seen": 0,
  +                "runs_kept": 0,
  +                "log_dirs_pruned": 0,
  +                "pruned_log_dirs": [],
  +                "reason": "history_disabled",
  +            }
  +
  +        if os.environ.get("JOBINTEL_WRITE_PROOF", "0").strip() == "1":
  +            try:
  +                run_report_payload = json.loads(run_metadata_path.read_text(encoding="utf-8"))
  +                if isinstance(run_report_payload, dict):
  +                    proof_path = _write_proof_receipt(
  +                        run_metadata_path,
  +                        run_report_payload,
  +                        s3_meta=s3_meta if isinstance(s3_meta, dict) else {},
  +                        publish_section=publish_section,
  +                    )
  +                    if proof_path:
  +                        logger.info("Proof receipt written: %s", proof_path)
  +            except Exception as exc:
  +                logger.warning("Failed to write proof receipt: %r", exc)
  +
  +        try:
  +            run_report_payload = json.loads(run_metadata_path.read_text(encoding="utf-8"))
  +        except Exception:
  +            run_report_payload = None
  +        if isinstance(run_report_payload, dict):
  +            run_report_payload["history_retention"] = history_summary
  +            run_report_payload["logs"] = run_log_pointers
  +            run_report_payload["log_retention"] = log_retention_summary
  +            run_metadata_path.write_text(
  +                json.dumps(run_report_payload, ensure_ascii=False, indent=2, sort_keys=True), encoding="utf-8"
  +            )
  +
  +        if os.environ.get("JOBINTEL_PRUNE") == "1":
  +            try:
  +                from scripts import prune_state as prune_state
  +
  +                prune_state.main(["--apply"])
  +            except Exception as e:
  +                logger.warning("Prune step failed (JOBINTEL_PRUNE=1): %r", e)
  +
  +        s3_prefixes = s3_meta.get("prefixes") if isinstance(s3_meta, dict) else None
  +        dashboard_url = None
  +        if isinstance(s3_meta, dict):
  +            dashboard_url = s3_meta.get("dashboard_url")
  +        if not dashboard_url:
  +            env_dashboard = os.environ.get("JOBINTEL_DASHBOARD_URL", "").strip().rstrip("/")
  +            if env_dashboard:
  +                dashboard_url = f"{env_dashboard}/runs/{run_id}"
  +
  +        provider_availability = {}
  +        for provider in providers:
  +            meta = provenance_by_provider.get(provider, {})
  +            provider_availability[provider] = {
  +                "status": meta.get("availability") or "unknown",
  +                "unavailable_reason": meta.get("unavailable_reason"),
  +                "attempts_made": meta.get("attempts_made"),
  +            }
  +
  +        logger.info(
  +            "RUN SUMMARY\n"
  +            "run_id=%s\n"
  +            "providers=%s\n"
  +            "profiles=%s\n"
  +            "s3_status=%s\n"
  +            "s3_reason=%s\n"
  +            "s3_bucket=%s\n"
  +            "s3_prefixes=%s\n"
  +            "dashboard_url=%s\n"
  +            "discord_status=%s\n"
  +            "provider_availability=%s",
  +            run_id,
  +            ",".join(providers),
  +            ",".join(profiles_list),
  +            s3_meta.get("status", "unknown") if isinstance(s3_meta, dict) else "unknown",
  +            s3_meta.get("reason") if isinstance(s3_meta, dict) else None,
  +            s3_meta.get("bucket") if isinstance(s3_meta, dict) else None,
  +            json.dumps(s3_prefixes, sort_keys=True) if s3_prefixes else None,
  +            dashboard_url,
  +            json.dumps(discord_status_by_provider, sort_keys=True) if discord_status_by_provider else None,
  +            json.dumps(provider_availability, sort_keys=True),
  +        )
  +        if s3_failed:
  +            raise SystemExit(s3_exit_code or 2)
  +        return final_status
  +
  +    current_stage = "startup"
  +
  +    def record_stage(name: str, fn) -> Any:
  +        t0 = time.time()
  +        try:
  +            result = fn()
  +        except SystemExit as e:
  +            code = _normalize_exit_code(e.code)
  +            if code == 0:
  +                result = None  # treat as success and continue
  +            else:
  +                raise
  +        telemetry["stages"][name] = {"duration_sec": round(time.time() - t0, 3)}
  +        return result
  +
  +    try:
  +        profiles = _resolve_profiles(args)
  +        profiles_list[:] = profiles
  +        us_only_flag = ["--us_only"] if args.us_only else []
  +        ai_required = args.ai
  +
  +        # Self-check: warn if common artifacts/directories are not writable (e.g., root-owned from Docker).
  +        warn_paths: List[Path] = [DATA_DIR / "ashby_cache", STATE_DIR, LAST_RUN_JSON, Path("/tmp"), Path("/work")]
  +        for provider in providers:
  +            warn_paths.extend(
  +                [
  +                    _provider_raw_jobs_json(provider),
  +                    _provider_labeled_jobs_json(provider),
  +                    _provider_enriched_jobs_json(provider),
  +                    _provider_ai_jobs_json(provider),
  +                ]
  +            )
  +        _warn_if_not_user_writable(warn_paths, context="startup")
  +
  +        # Snapshot presence check (fail fast with alert if missing and needed)
  +        if "openai" in providers:
  +            snapshot_path = SNAPSHOT_DIR / "index.html"
  +            if not snapshot_path.exists():
  +                msg = (
  +                    f"Snapshot not found at {snapshot_path}. "
  +                    "Save https://openai.com/careers/search/ to data/openai_snapshots/index.html or switch mode."
  +                )
  +                raise RuntimeError(msg)
  +
  +        # Short-circuit check (ai-aware) for openai-only runs.
  +        base_short = openai_only and _should_short_circuit(prev_hashes, curr_hashes)
  +
  +        def _ranked_up_to_date() -> bool:
  +            if ai_mtime is None:
  +                return False
  +            for p in profiles:
  +                rjson = _provider_ranked_jobs_json("openai", p)
  +                if (not rjson.exists()) or ((_file_mtime(rjson) or 0) < ai_mtime):
  +                    return False
  +            return True
  +
  +        def _update_ai_telemetry(ran: bool) -> None:
  +            telemetry.update(
  +                {
  +                    "ai_requested": True if ai_required else False,
  +                    "ai_ran": ran,
  +                    "ai_output_hash": _hash_file(ai_path),
  +                    "ai_output_mtime": _file_mtime_iso(ai_path),
  +                }
  +            )
  +            telemetry["ai"] = {
  +                "requested": telemetry["ai_requested"],
  +                "ran": telemetry["ai_ran"],
  +                "output_hash": telemetry["ai_output_hash"],
  +                "output_mtime": telemetry["ai_output_mtime"],
  +            }
  +
  +        if base_short:
  +            semantic_enabled = bool(semantic_settings["enabled"])
  +            # No-AI short-circuit: safe to skip everything downstream IF ranked artifacts exist.
  +            if not ai_required:
  +                missing_artifacts: List[Path] = []
  +                for p in profiles:
  +                    ranked_json = _provider_ranked_jobs_json("openai", p)
  +                    ranked_csv = _provider_ranked_jobs_csv("openai", p)
  +                    ranked_families = _provider_ranked_families_json("openai", p)
  +                    shortlist_md = _provider_shortlist_md("openai", p)
  +                    if not ranked_json.exists():
  +                        missing_artifacts.append(ranked_json)
  +                    if not ranked_csv.exists():
  +                        missing_artifacts.append(ranked_csv)
  +                    if not ranked_families.exists():
  +                        missing_artifacts.append(ranked_families)
  +                    if not shortlist_md.exists():
  +                        missing_artifacts.append(shortlist_md)
  +
  +                if not missing_artifacts and not semantic_enabled:
  +                    telemetry["hashes"] = curr_hashes
  +                    telemetry["counts"] = {
  +                        "raw": _safe_len(_provider_raw_jobs_json("openai")),
  +                        "labeled": _safe_len(_provider_labeled_jobs_json("openai")),
  +                        "enriched": _safe_len(_provider_enriched_jobs_json("openai")),
  +                    }
  +                    telemetry["stages"] = {"short_circuit": {"duration_sec": 0.0}}
  +                    _update_ai_telemetry(False)
  +                    _finalize("short_circuit")
  +                    logger.info(
  +                        "No changes detected (raw/labeled/enriched) and ranked artifacts present. "
  +                        "Short-circuiting downstream stages (scoring not required)."
  +                    )
  +                    return 0
  +                if not missing_artifacts and semantic_enabled:
  +                    logger.info(
  +                        "Semantic enabled; bypassing full short-circuit and re-running deterministic scoring "
  +                        "to produce semantic artifacts."
  +                    )
  +                else:
  +                    logger.info(
  +                        "Short-circuit skipped because ranked artifacts are missing; will re-run scoring. Missing: %s",
  +                        ", ".join(str(p) for p in missing_artifacts),
  +                    )
  +
  +            # AI-aware short-circuit: allow skipping scrape/classify/enrich, but only skip AI+scoring when fresh.
  +            prev_ai_hash = prev_run.get("ai_output_hash") or prev_ai.get("output_hash")
  +            prev_ai_mtime = prev_run.get("ai_output_mtime") or prev_ai.get("output_mtime")
  +            prev_ai_ran = bool(prev_run.get("ai_ran") or prev_ai.get("ran"))
  +
  +            curr_ai_mtime_iso = _file_mtime_iso(ai_path)
  +            ai_fresh = bool(ai_path.exists()) and (
  +                (ai_hash is not None and ai_hash == prev_ai_hash)
  +                or (curr_ai_mtime_iso is not None and curr_ai_mtime_iso == prev_ai_mtime)
  +            )
  +
  +            if ai_fresh and prev_ai_ran and _ranked_up_to_date() and not semantic_enabled:
  +                telemetry["hashes"] = curr_hashes
  +                telemetry["counts"] = {
  +                    "raw": _safe_len(_provider_raw_jobs_json("openai")),
  +                    "labeled": _safe_len(_provider_labeled_jobs_json("openai")),
  +                    "enriched": _safe_len(_provider_enriched_jobs_json("openai")),
  +                }
  +                telemetry["stages"] = {"short_circuit": {"duration_sec": 0.0}}
  +                _update_ai_telemetry(False)
  +                _finalize("short_circuit")
  +                logger.info("No changes detected and AI+ranked outputs fresh. Short-circuiting downstream stages.")
  +                return 0
  +
  +            # We still want AI and/or scoring to run, but we can skip scrape/classify/enrich.
  +            if ai_required and ((not ai_path.exists()) or (not prev_ai_ran) or (not ai_fresh)):
  +                current_stage = "ai_augment"
  +                telemetry["ai_ran"] = True
  +                record_stage(
  +                    current_stage,
  +                    lambda: _run(
  +                        [sys.executable, str(REPO_ROOT / "scripts" / "run_ai_augment.py")], stage=current_stage
  +                    ),
  +                )
  +                ai_mtime = _file_mtime(ai_path)
  +                ai_hash = _hash_file(ai_path)
  +
  +            # Ensure scoring runs if ranked outputs missing or stale vs AI file
  +            for profile in profiles:
  +                ranked_json = _provider_ranked_jobs_json("openai", profile)
  +                ranked_csv = _provider_ranked_jobs_csv("openai", profile)
  +                ranked_families = _provider_ranked_families_json("openai", profile)
  +                shortlist_md = _provider_shortlist_md("openai", profile)
  +                top_md = _provider_top_md("openai", profile)
  +
  +                scoring_input_selection_by_profile[profile] = _score_input_selection_detail(args)
  +                score_in, score_err = _resolve_score_input_path(args)
  +                scoring_inputs_by_profile[profile] = (
  +                    _file_metadata(score_in)
  +                    if score_in
  +                    else {
  +                        "path": None,
  +                        "mtime_iso": None,
  +                        "sha256": None,
  +                    }
  +                )
  +
  +                failed_stage = f"score:{profile}"
  +                if score_err or score_in is None:
  +                    logger.error(score_err or "Unknown scoring input error")
  +                    _finalize("error", {"error": score_err or "score input missing", "failed_stage": failed_stage})
  +                    return 2
  +
  +                run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +                archived_inputs_by_provider_profile.setdefault("openai", {})[profile] = _archive_run_inputs(
  +                    run_dir,
  +                    "openai",
  +                    profile,
  +                    score_in,
  +                    REPO_ROOT / "config" / "profiles.json",
  +                )
  +
  +                need_score = semantic_enabled or (not ai_required) or not ranked_json.exists()
  +                if not semantic_enabled and ai_required:
  +                    if ai_mtime is not None:
  +                        need_score = need_score or ((_file_mtime(ranked_json) or 0) < ai_mtime)
  +                    else:
  +                        need_score = True
  +
  +                if need_score:
  +                    current_stage = f"score:{profile}"
  +
  +                cmd = [
  +                    sys.executable,
  +                    str(REPO_ROOT / "scripts" / "score_jobs.py"),
  +                    "--profile",
  +                    profile,
  +                    "--provider_id",
  +                    "openai",
  +                    "--in_path",
  +                    str(score_in),
  +                    "--out_json",
  +                    str(ranked_json),
  +                    "--out_csv",
  +                    str(ranked_csv),
  +                    "--out_families",
  +                    str(ranked_families),
  +                    "--out_md",
  +                    str(shortlist_md),
  +                    "--min_score",
  +                    str(args.min_score),
  +                    "--out_md_top_n",
  +                    str(top_md),
  +                    "--semantic_scores_out",
  +                    str(
  +                        semantic_score_artifact_path(
  +                            run_id=run_id,
  +                            provider="openai",
  +                            profile=profile,
  +                            run_metadata_dir=RUN_METADATA_DIR,
  +                        )
  +                    ),
  +                ] + us_only_flag
  +                if args.ai or args.ai_only:
  +                    cmd.append("--prefer_ai")
  +                if need_score:
  +                    record_stage(current_stage, lambda cmd=cmd: _run(cmd, stage=current_stage))
  +
  +                    state_path = state_last_ranked(profile)
  +                    curr = _read_json(ranked_json)
  +                    prev = _read_json(state_path) if state_path.exists() else []
  +                    _write_json(state_path, curr)
  +                    # (diff/alerts handled in full path only; for freshness runs, we just persist state)
  +
  +            final_status = _finalize("success")
  +            return 0 if final_status == "success" else 2
  +
  +        def _stage_label(base: str, provider: Optional[str] = None, profile: Optional[str] = None) -> str:
  +            if openai_only and (provider is None or provider == "openai"):
  +                if profile:
  +                    return f"{base}:{profile}"
  +                return base
  +            parts = [base]
  +            if provider:
  +                parts.append(provider)
  +            if profile:
  +                parts.append(profile)
  +            return ":".join(parts)
  +
  +        def _profile_label(provider: str, profile: str) -> str:
  +            if openai_only and provider == "openai":
  +                return profile
  +            return f"{provider}:{profile}"
  +
  +        # 1) Run pipeline stages ONCE (scrape supports multi-provider).
  +        current_stage = _stage_label("scrape")
  +        force_mode = (os.environ.get("JOBINTEL_SCRAPE_MODE") or os.environ.get("CAREERS_MODE") or "").strip()
  +        if args.offline:
  +            scrape_mode = "SNAPSHOT"
  +        elif force_mode:
  +            scrape_mode = force_mode.upper()
  +        else:
  +            scrape_mode = "AUTO"
  +        scrape_cmd = [
  +            sys.executable,
  +            str(REPO_ROOT / "scripts" / "run_scrape.py"),
  +            "--mode",
  +            scrape_mode,
  +            "--providers",
  +            ",".join(providers),
  +            "--providers-config",
  +            args.providers_config,
  +        ]
  +        if scrape_mode in {"LIVE", "AUTO"} and not args.snapshot_only:
  +            env_snapshot_dir = os.environ.get("JOBINTEL_SNAPSHOT_WRITE_DIR")
  +            default_snapshot_dir = env_snapshot_dir or "/tmp/jobintel_snapshots"
  +            snapshot_write_dir = Path(default_snapshot_dir)
  +            if env_snapshot_dir is None:
  +                try:
  +                    snapshot_write_dir.mkdir(parents=True, exist_ok=True)
  +                except OSError as exc:
  +                    fallback = Path(tempfile.mkdtemp(prefix="jobintel_snapshots_"))
  +                    logger.warning(
  +                        "snapshot_write_dir mkdir failed (%s); falling back to %s",
  +                        exc,
  +                        fallback,
  +                    )
  +                    snapshot_write_dir = fallback
  +            else:
  +                snapshot_write_dir.mkdir(parents=True, exist_ok=True)
  +            scrape_cmd += ["--snapshot-write-dir", str(snapshot_write_dir)]
  +        if args.snapshot_only:
  +            scrape_cmd.append("--snapshot-only")
  +        record_stage(current_stage, lambda cmd=scrape_cmd: _run(cmd, stage=current_stage))
  +        provenance_by_provider = _load_scrape_provenance(providers)
  +        all_unavailable = _all_providers_unavailable(provenance_by_provider, providers)
  +        if all_unavailable:
  +            logger.warning("All providers unavailable; suppressing Discord alerts.")
  +
  +        thresholds = _provider_policy_thresholds()
  +
  +        if args.scrape_only:
  +            for provider in providers:
  +                meta = provenance_by_provider.get(provider, {})
  +                failed, reason, line = _evaluate_provider_policy(
  +                    provider,
  +                    meta,
  +                    enriched_path=None,
  +                    thresholds=thresholds,
  +                    no_enrich=True,
  +                )
  +                provider_policy_lines[provider] = line
  +                if failed:
  +                    err_msg = f"Provider policy failed ({provider}): {reason}"
  +                    logger.error(err_msg)
  +                    _post_failure(webhook, stage="provider_policy", error=err_msg, no_post=args.no_post)
  +                    _finalize("error", {"error": err_msg, "failed_stage": "provider_policy"})
  +                    return 3
  +            logger.info("Stopping after scrape (--scrape_only set)")
  +            final_status = _finalize("success")
  +            return 0 if final_status == "success" else 2
  +
  +        # 2) Run classify/enrich/AI per provider.
  +        for provider in providers:
  +            raw_path = _provider_raw_jobs_json(provider)
  +            labeled_path = _provider_labeled_jobs_json(provider)
  +            enriched_path = _provider_enriched_jobs_json(provider)
  +            ai_out_path = _provider_ai_jobs_json(provider)
  +
  +            current_stage = _stage_label("classify", provider)
  +            record_stage(
  +                current_stage,
  +                lambda p=raw_path, o=labeled_path: _run(
  +                    [
  +                        sys.executable,
  +                        str(REPO_ROOT / "scripts" / "run_classify.py"),
  +                        "--in_path",
  +                        str(p),
  +                        "--out_path",
  +                        str(o),
  +                    ],
  +                    stage=current_stage,
  +                ),
  +            )
  +
  +            current_stage = _stage_label("enrich", provider)
  +            if args.no_enrich:
  +                logger.info("Skipping enrichment step (--no_enrich set) [%s]", provider)
  +            else:
  +                record_stage(
  +                    current_stage,
  +                    lambda p=labeled_path, o=enriched_path: _run(
  +                        [
  +                            sys.executable,
  +                            str(REPO_ROOT / "scripts" / "run_enrich.py"),
  +                            "--in_path",
  +                            str(p),
  +                            "--out_path",
  +                            str(o),
  +                        ],
  +                        stage=current_stage,
  +                    ),
  +                )
  +
  +            meta = provenance_by_provider.get(provider, {})
  +            failed, reason, line = _evaluate_provider_policy(
  +                provider,
  +                meta,
  +                enriched_path=enriched_path,
  +                thresholds=thresholds,
  +                no_enrich=args.no_enrich,
  +            )
  +            provider_policy_lines[provider] = line
  +            if failed:
  +                err_msg = f"Provider policy failed ({provider}): {reason}"
  +                logger.error(err_msg)
  +                _post_failure(
  +                    webhook, stage=_stage_label("provider_policy", provider), error=err_msg, no_post=args.no_post
  +                )
  +                _finalize("error", {"error": err_msg, "failed_stage": _stage_label("provider_policy", provider)})
  +                return 3
  +
  +            # Optional AI augment stage
  +            if args.ai:
  +                current_stage = _stage_label("ai_augment", provider)
  +                telemetry["ai_ran"] = True
  +                record_stage(
  +                    current_stage,
  +                    lambda p=enriched_path, o=ai_out_path: _run(
  +                        [
  +                            sys.executable,
  +                            str(REPO_ROOT / "scripts" / "run_ai_augment.py"),
  +                            "--in_path",
  +                            str(p),
  +                            "--out_path",
  +                            str(o),
  +                        ],
  +                        stage=current_stage,
  +                    ),
  +                )
  +            # ai_only still proceeds to scoring; we skip the old early-return so scoring can run with AI outputs.
  +
  +            unavailable_summary = _unavailable_summary_for(provider)
  +
  +            # 3–5) For each profile: score -> diff -> state -> optional alert
  +            for profile in profiles:
  +                ranked_json = _provider_ranked_jobs_json(provider, profile)
  +                ranked_csv = _provider_ranked_jobs_csv(provider, profile)
  +                ranked_families = _provider_ranked_families_json(provider, profile)
  +                shortlist_md = _provider_shortlist_md(provider, profile)
  +                top_md = _provider_top_md(provider, profile)
  +
  +                if openai_only and provider == "openai":
  +                    selection = _score_input_selection_detail(args)
  +                    in_path, score_err = _resolve_score_input_path(args)
  +                else:
  +                    selection = _score_input_selection_detail_for(args, provider)
  +                    in_path, score_err = _resolve_score_input_path_for(args, provider)
  +                scoring_input_selection_by_provider.setdefault(provider, {})[profile] = selection
  +                scoring_inputs_by_provider.setdefault(provider, {})[profile] = (
  +                    _file_metadata(in_path) if in_path else {"path": None, "mtime_iso": None, "sha256": None}
  +                )
  +
  +                if provider == "openai":
  +                    scoring_input_selection_by_profile[profile] = selection
  +                    scoring_inputs_by_profile[profile] = (
  +                        _file_metadata(in_path) if in_path else {"path": None, "mtime_iso": None, "sha256": None}
  +                    )
  +
  +                # Validate scoring prerequisites
  +                if score_err or in_path is None:
  +                    logger.error(score_err or "Unknown scoring input error")
  +                    failed_stage = _stage_label("score", provider, profile)
  +                    _finalize("error", {"error": score_err or "score input missing", "failed_stage": failed_stage})
  +                    return 2
  +
  +                run_dir = RUN_METADATA_DIR / _sanitize_run_id(run_id)
  +                archived_inputs_by_provider_profile.setdefault(provider, {})[profile] = _archive_run_inputs(
  +                    run_dir,
  +                    provider,
  +                    profile,
  +                    in_path,
  +                    REPO_ROOT / "config" / "profiles.json",
  +                )
  +
  +                current_stage = _stage_label("score", provider, profile)
  +                cmd = [
  +                    sys.executable,
  +                    str(REPO_ROOT / "scripts" / "score_jobs.py"),
  +                    "--profile",
  +                    profile,
  +                    "--provider_id",
  +                    provider,
  +                    "--in_path",
  +                    str(in_path),
  +                    "--out_json",
  +                    str(ranked_json),
  +                    "--out_csv",
  +                    str(ranked_csv),
  +                    "--out_families",
  +                    str(ranked_families),
  +                    "--out_md",
  +                    str(shortlist_md),
  +                    "--min_score",
  +                    str(args.min_score),
  +                    "--out_md_top_n",
  +                    str(top_md),
  +                    "--semantic_scores_out",
  +                    str(
  +                        semantic_score_artifact_path(
  +                            run_id=run_id,
  +                            provider=provider,
  +                            profile=profile,
  +                            run_metadata_dir=RUN_METADATA_DIR,
  +                        )
  +                    ),
  +                ] + us_only_flag
  +                if args.ai or args.ai_only:
  +                    cmd.append("--prefer_ai")
  +
  +                record_stage(current_stage, lambda cmd=cmd: _run(cmd, stage=current_stage))
  +                _apply_score_fallback_metadata(selection, ranked_json)
  +
  +                # Warn if freshly produced artifacts are not writable for future runs.
  +                warn_context = _stage_label("after_score", provider, profile)
  +                _warn_if_not_user_writable(
  +                    [
  +                        ranked_json,
  +                        ranked_csv,
  +                        ranked_families,
  +                        shortlist_md,
  +                        top_md,
  +                        _state_last_ranked(provider, profile),
  +                    ],
  +                    context=warn_context,
  +                )
  +
  +                state_path = _state_last_ranked(provider, profile)
  +                state_exists = state_path.exists()
  +                curr = _read_json(ranked_json)
  +                state_map, user_state_counts, ignored_ids, suppress_new_ids = _user_state_sets(profile, curr)
  +                user_state_counts_by_provider_profile.setdefault(provider, {})[profile] = user_state_counts
  +                alerts_json, alerts_md = _alerts_paths(provider, profile)
  +                last_seen_path = _last_seen_path(provider, profile)
  +                prev_last_seen = load_last_seen(last_seen_path)
  +                alerts = compute_alerts(curr, prev_last_seen, score_delta=resolve_score_delta())
  +                alerts = _apply_user_state_to_alerts(
  +                    alerts,
  +                    suppress_new_ids=suppress_new_ids,
  +                    ignored_ids=ignored_ids,
  +                )
  +                write_alerts(alerts_json, alerts_md, alerts, provider, profile)
  +                write_last_seen(last_seen_path, build_last_seen(curr))
  +                fallback_applied = selection.get("us_only_fallback", {}).get("fallback_applied") is True
  +                if fallback_applied:
  +                    label = _profile_label(provider, profile)
  +                    diff_counts = {
  +                        "new": 0,
  +                        "changed": 0,
  +                        "removed": 0,
  +                        "suppressed": True,
  +                        "reason": "us_only_fallback",
  +                        "note": (
  +                            "US-only filter removed all jobs under --no_enrich; changelog suppressed to avoid noise."
  +                        ),
  +                    }
  +                    diff_counts_by_provider.setdefault(provider, {})[profile] = diff_counts
  +                    if provider == "openai":
  +                        diff_counts_by_profile[profile] = diff_counts
  +                    logger.info("Changelog (%s) suppressed due to US-only fallback.", label)
  +                    _write_json(state_path, curr)
  +                    extra_lines: List[str] = []
  +                    policy_line = provider_policy_lines.get(provider)
  +                    if policy_line:
  +                        extra_lines.append(policy_line)
  +                    unavailable_line = _provider_unavailable_line(provider, provenance_by_provider.get(provider, {}))
  +                    if unavailable_line:
  +                        extra_lines.append(unavailable_line)
  +                    discord_status = _maybe_post_run_summary(
  +                        provider,
  +                        profile,
  +                        ranked_json,
  +                        diff_counts,
  +                        args.min_score,
  +                        notify_mode=notify_mode,
  +                        no_post=args.no_post or all_unavailable,
  +                        extra_lines=extra_lines or None,
  +                    )
  +                    discord_status_by_provider.setdefault(provider, {})[profile] = discord_status
  +                    if unavailable_summary:
  +                        logger.info("Unavailable reasons: %s", unavailable_summary)
  +                    logger.info(
  +                        "Done (%s). Ranked outputs:\n - %s\n - %s\n - %s",
  +                        label,
  +                        ranked_json,
  +                        ranked_csv,
  +                        shortlist_md,
  +                    )
  +                    continue
  +
  +                prev: List[Dict[str, Any]] = []
  +                baseline_exists = False
  +                local_ranked = _resolve_local_last_success_ranked(provider, profile, run_id)
  +                if local_ranked:
  +                    prev = _read_json(local_ranked)
  +                    baseline_exists = True
  +                elif state_exists:
  +                    prev = _read_json(state_path)
  +                    baseline_exists = True
  +                else:
  +                    latest_ranked = _resolve_latest_run_ranked(provider, profile, run_id)
  +                    if latest_ranked:
  +                        prev = _read_json(latest_ranked)
  +                        baseline_exists = True
  +
  +                if not baseline_exists and s3_enabled():
  +                    bucket = os.environ.get("JOBINTEL_S3_BUCKET", "").strip()
  +                    prefix = os.environ.get("JOBINTEL_S3_PREFIX", "jobintel").strip("/")
  +                    if bucket:
  +                        s3_info = _resolve_s3_baseline(provider, profile, run_id, bucket=bucket, prefix=prefix)
  +                        if s3_info.ranked_path and s3_info.ranked_path.exists():
  +                            prev = _read_json(s3_info.ranked_path)
  +                            baseline_exists = True
  +                new_jobs, changed_jobs, removed_jobs, changed_fields = _diff(prev, curr)
  +                visible_new_jobs = _filter_by_ids(new_jobs, ignored_ids)
  +                visible_changed_jobs = _filter_by_ids(changed_jobs, ignored_ids)
  +                visible_removed_jobs = _filter_by_ids(removed_jobs, ignored_ids)
  +                visible_new_jobs_for_notifications = _filter_by_ids(visible_new_jobs, suppress_new_ids)
  +                visible_changed_jobs = _annotate_and_deprioritize_items(visible_changed_jobs, state_map)
  +                visible_new_jobs_for_notifications = _annotate_and_deprioritize_items(
  +                    visible_new_jobs_for_notifications,
  +                    state_map,
  +                )
  +
  +                # Append "Changes since last run" section to shortlist (filtered by min_alert_score)
  +                _append_shortlist_changes_section(
  +                    shortlist_md,
  +                    profile,
  +                    visible_new_jobs,
  +                    visible_changed_jobs,
  +                    visible_removed_jobs,
  +                    state_exists,
  +                    changed_fields,
  +                    prev_jobs=prev,
  +                    min_alert_score=args.min_alert_score,
  +                )
  +
  +                diff_json_path, diff_md_path = _provider_diff_paths(provider, profile)
  +                diff_report = build_diff_report(
  +                    prev,
  +                    curr,
  +                    provider=provider,
  +                    profile=profile,
  +                    baseline_exists=baseline_exists,
  +                    ignored_ids=ignored_ids,
  +                )
  +                _write_canonical_json(diff_json_path, diff_report)
  +                diff_markdown = build_diff_markdown(diff_report)
  +                _redaction_guard_text(diff_md_path, diff_markdown)
  +                diff_md_path.write_text(diff_markdown, encoding="utf-8")
  +                diff_summary_by_provider_profile.setdefault(provider, {})[profile] = _diff_summary_entry(
  +                    run_id=run_id,
  +                    provider=provider,
  +                    profile=profile,
  +                    diff_report=diff_report,
  +                )
  +                diff_report_by_provider_profile.setdefault(provider, {})[profile] = diff_report
  +
  +                label = _profile_label(provider, profile)
  +                logger.info(
  +                    "Changelog (%s): new=%d changed=%d removed=%d",
  +                    label,
  +                    diff_report.get("counts", {}).get("added", 0),
  +                    diff_report.get("counts", {}).get("changed", 0),
  +                    diff_report.get("counts", {}).get("removed", 0),
  +                )
  +                diff_counts = {
  +                    "new": diff_report.get("counts", {}).get("added", 0),
  +                    "changed": diff_report.get("counts", {}).get("changed", 0),
  +                    "removed": diff_report.get("counts", {}).get("removed", 0),
  +                }
  +                if not baseline_exists:
  +                    diff_counts["first_run"] = True
  +                suppressed = int(((diff_report.get("suppressed") or {}).get("ignored", 0)) or 0)
  +                if suppressed > 0:
  +                    diff_counts["suppressed_ignored"] = suppressed
  +                diff_counts_by_provider.setdefault(provider, {})[profile] = diff_counts
  +                if provider == "openai":
  +                    diff_counts_by_profile[profile] = diff_counts
  +
  +                if provider in providers and os.environ.get("AI_ENABLED", "0").strip() == "1":
  +                    current_stage = _stage_label("ai_insights", provider, profile)
  +                    prev_path_arg = str(state_path) if state_exists else ""
  +                    cmd = [
  +                        sys.executable,
  +                        str(REPO_ROOT / "scripts" / "run_ai_insights.py"),
  +                        "--provider",
  +                        provider,
  +                        "--profile",
  +                        profile,
  +                        "--ranked_path",
  +                        str(ranked_json),
  +                        "--run_id",
  +                        run_id,
  +                    ]
  +                    if prev_path_arg:
  +                        cmd.extend(["--prev_path", prev_path_arg])
  +                    record_stage(current_stage, lambda cmd=cmd: _run(cmd, stage=current_stage))
  +
  +                if (
  +                    provider in providers
  +                    and os.environ.get("AI_ENABLED", "0").strip() == "1"
  +                    and os.environ.get("AI_JOB_BRIEFS_ENABLED", "0").strip() == "1"
  +                ):
  +                    current_stage = _stage_label("ai_job_briefs", provider, profile)
  +                    cmd = [
  +                        sys.executable,
  +                        str(REPO_ROOT / "scripts" / "run_ai_job_briefs.py"),
  +                        "--provider",
  +                        provider,
  +                        "--profile",
  +                        profile,
  +                        "--ranked_path",
  +                        str(ranked_json),
  +                        "--run_id",
  +                        run_id,
  +                        "--max_jobs",
  +                        os.environ.get("AI_JOB_BRIEFS_MAX_JOBS", "10"),
  +                        "--max_tokens_per_job",
  +                        os.environ.get("AI_JOB_BRIEFS_MAX_TOKENS", "400"),
  +                        "--total_budget",
  +                        os.environ.get("AI_JOB_BRIEFS_TOTAL_BUDGET", "2000"),
  +                    ]
  +                    record_stage(current_stage, lambda cmd=cmd: _run(cmd, stage=current_stage))
  +
  +                _write_json(state_path, curr)
  +                extra_lines: List[str] = []
  +                policy_line = provider_policy_lines.get(provider)
  +                if policy_line:
  +                    extra_lines.append(policy_line)
  +                briefs_line = _briefs_status_line(run_id, profile)
  +                if briefs_line:
  +                    extra_lines.append(briefs_line)
  +                unavailable_line = _provider_unavailable_line(provider, provenance_by_provider.get(provider, {}))
  +                if unavailable_line:
  +                    extra_lines.append(unavailable_line)
  +                discord_status = _maybe_post_run_summary(
  +                    provider,
  +                    profile,
  +                    ranked_json,
  +                    diff_counts,
  +                    args.min_score,
  +                    notify_mode=notify_mode,
  +                    no_post=args.no_post or all_unavailable,
  +                    extra_lines=extra_lines or None,
  +                    diff_items={
  +                        "new": _annotate_and_deprioritize_items(
  +                            [
  +                                item
  +                                for item in (diff_report.get("added") or [])
  +                                if item.get("id") not in suppress_new_ids
  +                            ],
  +                            state_map,
  +                        ),
  +                        "changed": _annotate_and_deprioritize_items(diff_report.get("changed") or [], state_map),
  +                    },
  +                )
  +                discord_status_by_provider.setdefault(provider, {})[profile] = discord_status
  +
  +                interesting_new = [
  +                    j for j in visible_new_jobs_for_notifications if j.get("score", 0) >= args.min_alert_score
  +                ]
  +                interesting_changed = [j for j in visible_changed_jobs if j.get("score", 0) >= args.min_alert_score]
  +
  +                if not webhook:
  +                    logger.info(
  +                        "ℹ️ No alerts (%s) (new=%d, changed=%d; webhook=unset).",
  +                        label,
  +                        len(visible_new_jobs_for_notifications),
  +                        len(visible_changed_jobs),
  +                    )
  +                    if unavailable_summary:
  +                        logger.info("Unavailable reasons: %s", unavailable_summary)
  +                    logger.info(
  +                        "Done (%s). Ranked outputs:\n - %s\n - %s\n - %s",
  +                        label,
  +                        ranked_json,
  +                        ranked_csv,
  +                        shortlist_md,
  +                    )
  +                    continue
  +
  +                if not (interesting_new or interesting_changed):
  +                    logger.info(
  +                        "ℹ️ No alerts (%s) (new=%d, changed=%d; webhook=set).",
  +                        label,
  +                        len(visible_new_jobs_for_notifications),
  +                        len(visible_changed_jobs),
  +                    )
  +                    if unavailable_summary:
  +                        logger.info("Unavailable reasons: %s", unavailable_summary)
  +                    logger.info(
  +                        "Done (%s). Ranked outputs:\n - %s\n - %s\n - %s",
  +                        label,
  +                        ranked_json,
  +                        ranked_csv,
  +                        shortlist_md,
  +                    )
  +                    continue
  +
  +                lines = [f"**Job alerts ({label})** — {_utcnow_iso()}"]
  +                if args.us_only:
  +                    lines.append("_US-only filter: ON_")
  +                lines.append("")
  +
  +                if interesting_new:
  +                    lines.append(f"🆕 **New high-scoring jobs (>= {args.min_alert_score})**")
  +                    for j in interesting_new[:8]:
  +                        loc = j.get("location") or j.get("locationName") or ""
  +                        status = str(j.get("user_state_status") or "").strip()
  +                        status_tag = f" [{status}]" if status else ""
  +                        lines.append(
  +                            f"- **{j.get('score')}** [{j.get('role_band')}] {j.get('title')} ({loc}){status_tag}"
  +                        )
  +                        if j.get("apply_url"):
  +                            lines.append(f"  {j['apply_url']}")
  +                    lines.append("")
  +
  +                if interesting_changed:
  +                    lines.append(f"♻️ **Changed high-scoring jobs (>= {args.min_alert_score})**")
  +                    for j in interesting_changed[:8]:
  +                        loc = j.get("location") or j.get("locationName") or ""
  +                        status = str(j.get("user_state_status") or "").strip()
  +                        status_tag = f" [{status}]" if status else ""
  +                        lines.append(
  +                            f"- **{j.get('score')}** [{j.get('role_band')}] {j.get('title')} ({loc}){status_tag}"
  +                        )
  +                        if j.get("apply_url"):
  +                            lines.append(f"  {j['apply_url']}")
  +                    lines.append("")
  +
  +                if not _should_notify(diff_counts, notify_mode):
  +                    logger.info("Discord alerts skipped (mode=%s, no diffs).", notify_mode)
  +                elif all_unavailable:
  +                    logger.info("All providers unavailable; suppressing alerts.")
  +                else:
  +                    _dispatch_alerts(
  +                        label,
  +                        webhook,
  +                        new_jobs,
  +                        changed_jobs,
  +                        removed_jobs,
  +                        interesting_new,
  +                        interesting_changed,
  +                        lines,
  +                        args,
  +                        unavailable_summary,
  +                    )
  +
  +                if unavailable_summary:
  +                    logger.info("Unavailable reasons: %s", unavailable_summary)
  +                logger.info(
  +                    "Done (%s). Ranked outputs:\n - %s\n - %s\n - %s",
  +                    label,
  +                    ranked_json,
  +                    ranked_csv,
  +                    shortlist_md,
  +                )
  +
  +        final_status = _finalize("success")
  +        return 0 if final_status == "success" else 2
  +
  +    except subprocess.CalledProcessError as e:
  +        cmd_str = " ".join(e.cmd) if isinstance(e.cmd, (list, tuple)) else str(e.cmd)
  +        logger.error(
  +            f"Stage '{current_stage}' failed (returncode={e.returncode}) cmd={cmd_str}\n"
  +            f"stdout_tail:\n{(getattr(e, 'output', '') or '')[-4000:]}\n"
  +            f"stderr_tail:\n{(getattr(e, 'stderr', '') or '')[-4000:]}"
  +        )
  +        _post_failure(
  +            webhook,
  +            stage=current_stage,
  +            error=f"{e}\ncmd={cmd_str}",
  +            no_post=args.no_post,
  +            stdout=getattr(e, "output", "") or "",
  +            stderr=getattr(e, "stderr", "") or "",
  +        )
  +        _finalize("error", {"error": str(e), "failed_stage": current_stage})
  +        if e.returncode == 2:
  +            return 2
  +        return max(3, e.returncode or 0)
  +    except SystemExit as e:
  +        exit_code = _normalize_exit_code(e.code)
  +        if exit_code == 0:
  +            raise
  +        err_msg = str(e) if str(e) else f"Stage '{current_stage}' exited"
  +        logger.error(f"Stage '{current_stage}' raised SystemExit({exit_code}): {err_msg}")
  +        _post_failure(
  +            webhook,
  +            stage=current_stage,
  +            error=err_msg,
  +            no_post=args.no_post,
  +        )
  +        _finalize("error", {"error": err_msg, "failed_stage": current_stage})
  +        return exit_code
  +    except Exception as e:
  +        logger.error(f"Stage '{current_stage}' failed unexpectedly: {e!r}")
  +        _post_failure(
  +            webhook,
  +            stage=current_stage or "unexpected",
  +            error=repr(e),
  +            no_post=args.no_post,
  +        )
  +        _finalize("error", {"error": repr(e), "failed_stage": current_stage})
  +        return 3
  +    finally:
  +        logger.info(f"===== jobintel end {_utcnow_iso()} =====")
  +
  +
  +if __name__ == "__main__":
  +    raise SystemExit(main())
  +
  +"""
  +{
  +  "run_id": "2026-01-09T18:02:11Z",
  +  "profiles": ["cs"],
  +  "flags": {
  +    "us_only": true,
  +    "no_enrich": true,
  +    "ai": false
  +  },
  +  "artifacts": {
  +    "raw": "openai_raw_jobs.json",
  +    "labeled": "openai_labeled_jobs.json",
  +    "ranked_cs": "openai_ranked_jobs.cs.json"
  +  },
  +  "counts": {
  +    "scraped": 456,
  +    "relevant": 10,
  +    "ranked": 29
  +  }
  +}
  +"""
diff --cc src/ji_engine/providers/registry.py
index c5eaa76,c5eaa76,0000000..e8a99e0
mode 100644,100644,000000..100644
--- a/src/ji_engine/providers/registry.py
+++ b/src/ji_engine/providers/registry.py
@@@@ -1,518 -1,518 -1,0 +1,586 @@@@
  +"""
  +SignalCraft
  +Copyright (c) 2026 Chris Menendez.
  +All Rights Reserved.
  +See LICENSE for permitted use.
  +"""
  +
  +from __future__ import annotations
  +
  +import json
  +import re
  +from pathlib import Path
  +from typing import Any, Dict, List
  +from urllib.parse import urlparse
  +
  +SUPPORTED_EXTRACTION_MODES = {"ashby", "jsonld", "snapshot_json", "html_list"}
  +_EXTRACTION_MODE_ALIASES = {
  +    "ashby_api": "ashby",
  +    "ashby": "ashby",
  +    "openai": "ashby",
  +    "jsonld": "jsonld",
  +    "llm_fallback": "jsonld",
  +    "snapshot_json": "snapshot_json",
  +    "snapshot": "snapshot_json",
  +    "html_rules": "html_list",
  +    "html_list": "html_list",
  +}
  +SUPPORTED_SCRAPE_MODES = {"snapshot", "live", "auto"}
  +SUPPORTED_UPDATE_PRIORITIES = {"low", "normal", "high"}
  +_PROVIDER_ID_RE = re.compile(r"^[a-z0-9_-]+$")
  +_PROVIDERS_SCHEMA_CACHE: Dict[str, Any] | None = None
  +_SCHEMA_PATH = Path(__file__).resolve().parents[3] / "schemas" / "providers.schema.v1.json"
  +
  +
  +def _load_providers_schema() -> Dict[str, Any]:
  +    global _PROVIDERS_SCHEMA_CACHE
  +    if _PROVIDERS_SCHEMA_CACHE is None:
  +        _PROVIDERS_SCHEMA_CACHE = json.loads(_SCHEMA_PATH.read_text(encoding="utf-8"))
  +    return _PROVIDERS_SCHEMA_CACHE
  +
  +
  +def _schema_provider_keys(schema: Dict[str, Any]) -> set[str]:
  +    provider = schema.get("$defs", {}).get("provider", {})
  +    props = provider.get("properties", {})
  +    return set(props.keys())
  +
  +
  +def _schema_top_level_keys(schema: Dict[str, Any]) -> set[str]:
  +    props = schema.get("properties", {})
  +    return set(props.keys())
  +
  +
  +def _validate_top_level_schema(data: Dict[str, Any], schema: Dict[str, Any]) -> None:
  +    allowed = _schema_top_level_keys(schema)
  +    unknown = sorted(set(data.keys()) - allowed)
  +    if unknown:
  +        raise ValueError(f"unsupported providers config keys: {', '.join(unknown)}")
  +    if "schema_version" not in data:
  +        raise ValueError("providers config missing schema_version")
  +    if data.get("schema_version") != 1:
  +        raise ValueError(f"unsupported providers schema_version '{data.get('schema_version')}'")
  +    if "providers" not in data:
  +        raise ValueError("providers config missing providers list")
  +
  +
  +def _validate_provider_entry_schema(entry: Dict[str, Any], schema: Dict[str, Any]) -> None:
  +    allowed = _schema_provider_keys(schema)
  +    unknown = sorted(set(entry.keys()) - allowed)
  +    if unknown:
  +        raise ValueError(f"unsupported provider keys: {', '.join(unknown)}")
  +    if not entry.get("provider_id"):
  +        raise ValueError("provider entry missing provider_id")
--     has_url = any(entry.get(field) for field in ("careers_urls", "careers_url", "board_url"))
--     if not has_url:
--         raise ValueError("provider entry missing careers_url/careers_urls/board_url")
--     if not (entry.get("extraction_mode") or entry.get("type")):
--         raise ValueError("provider entry missing extraction_mode/type")
+++    tombstone = entry.get("tombstone")
+++    tombstoned = tombstone is not None
+++    if tombstoned and not isinstance(tombstone, dict):
+++        raise ValueError("tombstone must be an object when provided")
+++    if not tombstoned:
+++        has_url = any(entry.get(field) for field in ("careers_urls", "careers_url", "board_url"))
+++        if not has_url:
+++            raise ValueError("provider entry missing careers_url/careers_urls/board_url")
+++        if not (entry.get("extraction_mode") or entry.get("type")):
+++            raise ValueError("provider entry missing extraction_mode/type")
+++    if tombstoned and bool(entry.get("enabled", False)):
+++        raise ValueError("tombstoned provider must set enabled=false")
  +    if "careers_urls" in entry and not isinstance(entry.get("careers_urls"), list):
  +        raise ValueError("careers_urls must be a list when provided")
  +    if "careers_url" in entry and not isinstance(entry.get("careers_url"), str):
  +        raise ValueError("careers_url must be a string when provided")
  +    if "board_url" in entry and not isinstance(entry.get("board_url"), str):
  +        raise ValueError("board_url must be a string when provided")
  +    if "display_name" in entry and not isinstance(entry.get("display_name"), str):
  +        raise ValueError("display_name must be a string when provided")
  +    if "allowed_domains" in entry and not isinstance(entry.get("allowed_domains"), list):
  +        raise ValueError("allowed_domains must be a list when provided")
  +    if "update_cadence" in entry and not isinstance(entry.get("update_cadence"), (dict, str)):
  +        raise ValueError("update_cadence must be an object or string when provided")
  +    if "politeness" in entry and not isinstance(entry.get("politeness"), dict):
  +        raise ValueError("politeness must be an object when provided")
  +    if "enabled" in entry and not isinstance(entry.get("enabled"), bool):
  +        raise ValueError("enabled must be a boolean when provided")
  +    if "live_enabled" in entry and not isinstance(entry.get("live_enabled"), bool):
  +        raise ValueError("live_enabled must be a boolean when provided")
  +    if "snapshot_enabled" in entry and not isinstance(entry.get("snapshot_enabled"), bool):
  +        raise ValueError("snapshot_enabled must be a boolean when provided")
  +    if "llm_fallback" in entry and not isinstance(entry.get("llm_fallback"), dict):
  +        raise ValueError("llm_fallback must be an object when provided")
  +
  +
+++def _normalize_tombstone(entry: Dict[str, Any]) -> Dict[str, str] | None:
+++    raw = entry.get("tombstone")
+++    if raw is None:
+++        return None
+++    if not isinstance(raw, dict):
+++        raise ValueError("tombstone must be an object when provided")
+++    unknown = sorted(set(raw.keys()) - {"reason", "ticket", "replaced_by", "removed_at"})
+++    if unknown:
+++        raise ValueError(f"unsupported tombstone keys: {', '.join(unknown)}")
+++    reason = str(raw.get("reason") or "").strip()
+++    if not reason:
+++        raise ValueError("tombstone.reason is required when tombstone is set")
+++    out: Dict[str, str] = {"reason": reason}
+++    for key in ("ticket", "replaced_by", "removed_at"):
+++        value = str(raw.get(key) or "").strip()
+++        if value:
+++            out[key] = value
+++    return out
+++
+++
  +def _coerce_extraction_mode(raw_mode: Any, raw_type: Any) -> str:
  +    candidate = str(raw_mode or raw_type or "snapshot_json").strip().lower()
  +    candidate = _EXTRACTION_MODE_ALIASES.get(candidate, candidate)
  +    if candidate not in SUPPORTED_EXTRACTION_MODES:
  +        raise ValueError(f"unsupported extraction_mode '{candidate}'")
  +    return candidate
  +
  +
  +def _coerce_mode(value: Any) -> str:
  +    mode = str(value or "snapshot").strip().lower()
  +    if mode not in SUPPORTED_SCRAPE_MODES:
  +        raise ValueError(f"unsupported mode '{mode}'")
  +    return mode
  +
  +
  +def _normalized_url_list(entry: Dict[str, Any]) -> List[str]:
  +    careers_urls = entry.get("careers_urls")
  +    if isinstance(careers_urls, list):
  +        raw_urls = [str(v).strip() for v in careers_urls if str(v).strip()]
  +    else:
  +        single = entry.get("careers_url") or entry.get("board_url")
  +        raw_urls = [str(single).strip()] if single else []
  +    urls: List[str] = []
  +    seen: set[str] = set()
  +    for value in raw_urls:
  +        parsed = urlparse(value)
  +        if parsed.scheme not in {"http", "https"} or not parsed.netloc:
  +            raise ValueError(f"invalid careers URL '{value}'")
  +        canonical = parsed.geturl()
  +        if canonical not in seen:
  +            seen.add(canonical)
  +            urls.append(canonical)
  +    if not urls:
  +        raise ValueError("provider entry missing careers_url/careers_urls")
  +    return urls
  +
  +
  +def _allowed_domains(urls: List[str], configured: Any) -> List[str]:
  +    if isinstance(configured, list):
  +        domains = sorted({str(v).strip().lower() for v in configured if str(v).strip()})
  +        if domains:
  +            return domains
  +    parsed = []
  +    for url in urls:
  +        host = urlparse(url).netloc.strip().lower()
  +        if host:
  +            parsed.append(host)
  +    return sorted(set(parsed))
  +
  +
  +def _normalize_update_cadence(entry: Dict[str, Any]) -> Dict[str, Any] | str:
  +    raw = entry.get("update_cadence")
  +    if raw is None:
  +        return {}
  +    if isinstance(raw, str):
  +        value = raw.strip()
  +        if not value:
  +            raise ValueError("update_cadence must be non-empty when provided")
  +        return value
  +    if not isinstance(raw, dict):
  +        raise ValueError("update_cadence must be an object or string when provided")
  +    out: Dict[str, Any] = {}
  +    if "min_interval_hours" in raw:
  +        try:
  +            value = int(raw["min_interval_hours"])
  +        except (TypeError, ValueError) as exc:
  +            raise ValueError(f"invalid update_cadence.min_interval_hours: {raw['min_interval_hours']!r}") from exc
  +        if value < 1:
  +            raise ValueError("update_cadence.min_interval_hours must be >= 1")
  +        out["min_interval_hours"] = value
  +    if "max_staleness_hours" in raw:
  +        try:
  +            value = int(raw["max_staleness_hours"])
  +        except (TypeError, ValueError) as exc:
  +            raise ValueError(f"invalid update_cadence.max_staleness_hours: {raw['max_staleness_hours']!r}") from exc
  +        if value < 1:
  +            raise ValueError("update_cadence.max_staleness_hours must be >= 1")
  +        out["max_staleness_hours"] = value
  +    if "priority" in raw:
  +        priority = str(raw["priority"]).strip().lower()
  +        if priority not in SUPPORTED_UPDATE_PRIORITIES:
  +            raise ValueError(f"invalid update_cadence.priority '{priority}'")
  +        out["priority"] = priority
  +    if "schedule_hint" in raw:
  +        hint = str(raw["schedule_hint"]).strip()
  +        if not hint:
  +            raise ValueError("update_cadence.schedule_hint must be non-empty when provided")
  +        out["schedule_hint"] = hint
  +    unknown = sorted(set(raw.keys()) - {"min_interval_hours", "max_staleness_hours", "priority", "schedule_hint"})
  +    if unknown:
  +        raise ValueError(f"unsupported update_cadence keys: {', '.join(unknown)}")
  +    return out
  +
  +
  +def _normalize_llm_fallback(entry: Dict[str, Any]) -> Dict[str, Any]:
  +    raw = entry.get("llm_fallback")
  +    if raw is None:
  +        return {"enabled": False}
  +    if not isinstance(raw, dict):
  +        raise ValueError("llm_fallback must be an object when provided")
  +    unknown = sorted(set(raw.keys()) - {"enabled", "cache_dir", "temperature"})
  +    if unknown:
  +        raise ValueError(f"unsupported llm_fallback keys: {', '.join(unknown)}")
  +    enabled = bool(raw.get("enabled", False))
  +    cache_dir = str(raw.get("cache_dir") or "").strip()
  +    temperature = raw.get("temperature", 0)
  +    try:
  +        temp_value = float(temperature)
  +    except (TypeError, ValueError) as exc:
  +        raise ValueError(f"llm_fallback.temperature must be 0 (got {temperature!r})") from exc
  +    if abs(temp_value) > 1e-9:
  +        raise ValueError("llm_fallback.temperature must be 0 for deterministic cache usage")
  +    if enabled and not cache_dir:
  +        raise ValueError("llm_fallback.cache_dir is required when enabled")
  +    return {
  +        "enabled": enabled,
  +        "cache_dir": cache_dir,
  +        "temperature": 0.0,
  +    }
  +
  +
  +def _cast_float(raw: Any, field: str) -> float:
  +    try:
  +        value = float(raw)
  +    except (TypeError, ValueError) as exc:
  +        raise ValueError(f"invalid {field}: {raw!r}") from exc
  +    return value
  +
  +
  +def _cast_int(raw: Any, field: str) -> int:
  +    try:
  +        value = int(raw)
  +    except (TypeError, ValueError) as exc:
  +        raise ValueError(f"invalid {field}: {raw!r}") from exc
  +    return value
  +
  +
  +def _normalize_politeness_defaults(raw: Dict[str, Any], *, field_prefix: str) -> Dict[str, Any]:
  +    out: Dict[str, Any] = {}
  +    float_fields = {
  +        "min_delay_s",
  +        "rate_jitter_s",
  +        "backoff_base_s",
  +        "backoff_max_s",
  +        "backoff_jitter_s",
  +        "cooldown_s",
  +        "max_qps",
  +    }
  +    int_fields = {
  +        "max_attempts",
  +        "max_consecutive_failures",
  +        "max_inflight_per_host",
  +    }
  +    allowed = float_fields | int_fields
  +    unknown = sorted(set(raw.keys()) - allowed)
  +    if unknown:
  +        raise ValueError(f"unsupported {field_prefix} keys: {', '.join(unknown)}")
  +
  +    for key in sorted(float_fields):
  +        if key not in raw:
  +            continue
  +        value = _cast_float(raw[key], f"{field_prefix}.{key}")
  +        if value < 0:
  +            raise ValueError(f"{field_prefix}.{key} must be >= 0")
  +        out[key] = value
  +    for key in sorted(int_fields):
  +        if key not in raw:
  +            continue
  +        value = _cast_int(raw[key], f"{field_prefix}.{key}")
  +        if key in {"max_attempts", "max_inflight_per_host"} and value < 1:
  +            raise ValueError(f"{field_prefix}.{key} must be >= 1")
  +        if key == "max_consecutive_failures" and value < 0:
  +            raise ValueError(f"{field_prefix}.{key} must be >= 0")
  +        out[key] = value
  +
  +    max_qps = out.get("max_qps")
  +    min_delay_s = out.get("min_delay_s")
  +    if max_qps is not None and max_qps <= 0:
  +        raise ValueError(f"{field_prefix}.max_qps must be > 0")
  +    if max_qps is not None and min_delay_s is None:
  +        out["min_delay_s"] = 1.0 / max_qps
  +    elif max_qps is None and min_delay_s is not None and min_delay_s > 0:
  +        out["max_qps"] = 1.0 / min_delay_s
  +    elif max_qps is not None and min_delay_s is not None:
  +        derived_qps = 1.0 / min_delay_s if min_delay_s > 0 else 0.0
  +        if abs(derived_qps - max_qps) > 1e-6:
  +            raise ValueError(f"{field_prefix}.max_qps does not match min_delay_s")
  +    return out
  +
  +
  +def _normalize_host(host: str, *, field: str) -> str:
  +    candidate = host.strip().lower()
  +    if not candidate:
  +        raise ValueError(f"{field} host key must be non-empty")
  +    parsed = urlparse(candidate if "://" in candidate else f"https://{candidate}")
  +    netloc = parsed.netloc.strip().lower()
  +    if not netloc:
  +        raise ValueError(f"{field} host key '{host}' is invalid")
  +    return netloc
  +
  +
  +def _normalize_politeness(entry: Dict[str, Any]) -> Dict[str, Any]:
  +    raw = entry.get("politeness") or {}
  +    if not isinstance(raw, dict):
  +        raise ValueError("politeness must be an object when provided")
  +    raw_defaults = raw.get("defaults") or {}
  +    if not isinstance(raw_defaults, dict):
  +        raise ValueError("politeness.defaults must be an object when provided")
  +
  +    legacy_defaults: Dict[str, Any] = {}
  +    for key in (
  +        "min_delay_s",
  +        "rate_jitter_s",
  +        "max_attempts",
  +        "backoff_base_s",
  +        "backoff_max_s",
  +        "backoff_jitter_s",
  +        "max_consecutive_failures",
  +        "cooldown_s",
  +        "max_inflight_per_host",
  +        "max_qps",
  +    ):
  +        if key in raw:
  +            legacy_defaults[key] = raw[key]
  +    defaults = _normalize_politeness_defaults({**raw_defaults, **legacy_defaults}, field_prefix="politeness.defaults")
  +
  +    host_overrides: Dict[str, Dict[str, Any]] = {}
  +    raw_host_overrides = raw.get("host_overrides") or {}
  +    if not isinstance(raw_host_overrides, dict):
  +        raise ValueError("politeness.host_overrides must be an object when provided")
  +    for raw_host in sorted(raw_host_overrides.keys()):
  +        host_cfg = raw_host_overrides[raw_host]
  +        if not isinstance(host_cfg, dict):
  +            raise ValueError(f"politeness.host_overrides.{raw_host} must be an object")
  +        host = _normalize_host(raw_host, field="politeness.host_overrides")
  +        host_overrides[host] = _normalize_politeness_defaults(
  +            host_cfg,
  +            field_prefix=f"politeness.host_overrides.{host}",
  +        )
  +
  +    raw_qps_caps = raw.get("host_qps_caps") or {}
  +    if raw_qps_caps and not isinstance(raw_qps_caps, dict):
  +        raise ValueError("politeness.host_qps_caps must be an object when provided")
  +    for raw_host in sorted(raw_qps_caps.keys()):
  +        host = _normalize_host(raw_host, field="politeness.host_qps_caps")
  +        value = _cast_float(raw_qps_caps[raw_host], f"politeness.host_qps_caps.{host}")
  +        if value <= 0:
  +            raise ValueError(f"politeness.host_qps_caps.{host} must be > 0")
  +        host_overrides.setdefault(host, {})["max_qps"] = value
  +        host_overrides[host]["min_delay_s"] = 1.0 / value
  +
  +    raw_concurrency_caps = raw.get("host_concurrency_caps") or {}
  +    if raw_concurrency_caps and not isinstance(raw_concurrency_caps, dict):
  +        raise ValueError("politeness.host_concurrency_caps must be an object when provided")
  +    for raw_host in sorted(raw_concurrency_caps.keys()):
  +        host = _normalize_host(raw_host, field="politeness.host_concurrency_caps")
  +        value = _cast_int(raw_concurrency_caps[raw_host], f"politeness.host_concurrency_caps.{host}")
  +        if value < 1:
  +            raise ValueError(f"politeness.host_concurrency_caps.{host} must be >= 1")
  +        host_overrides.setdefault(host, {})["max_inflight_per_host"] = value
  +
  +    allowed_root_keys = {
  +        "defaults",
  +        "host_overrides",
  +        "host_qps_caps",
  +        "host_concurrency_caps",
  +        "min_delay_s",
  +        "rate_jitter_s",
  +        "max_attempts",
  +        "backoff_base_s",
  +        "backoff_max_s",
  +        "backoff_jitter_s",
  +        "max_consecutive_failures",
  +        "cooldown_s",
  +        "max_inflight_per_host",
  +        "max_qps",
  +    }
  +    unknown = sorted(set(raw.keys()) - allowed_root_keys)
  +    if unknown:
  +        raise ValueError(f"unsupported politeness keys: {', '.join(unknown)}")
  +
  +    out: Dict[str, Any] = dict(defaults)
  +    if defaults:
  +        out["defaults"] = dict(defaults)
  +    if host_overrides:
  +        out["host_overrides"] = {host: host_overrides[host] for host in sorted(host_overrides)}
  +        host_qps_caps: Dict[str, float] = {}
  +        host_concurrency_caps: Dict[str, int] = {}
  +        for host, override in out["host_overrides"].items():
  +            if "max_qps" in override:
  +                host_qps_caps[host] = float(override["max_qps"])
  +            if "max_inflight_per_host" in override:
  +                host_concurrency_caps[host] = int(override["max_inflight_per_host"])
  +        if host_qps_caps:
  +            out["host_qps_caps"] = host_qps_caps
  +        if host_concurrency_caps:
  +            out["host_concurrency_caps"] = host_concurrency_caps
  +    return out
  +
  +
  +def _normalize_provider_entry(entry: Dict[str, Any]) -> Dict[str, Any]:
  +    provider_id = str(entry.get("provider_id") or "").strip()
  +    if not provider_id:
  +        raise ValueError("provider entry missing provider_id")
  +    if not _PROVIDER_ID_RE.fullmatch(provider_id):
  +        raise ValueError(f"invalid provider_id '{provider_id}'")
--     urls = _normalized_url_list(entry)
--     raw_extraction_mode = str(entry.get("extraction_mode") or entry.get("type") or "").strip().lower()
--     extraction_mode = _coerce_extraction_mode(entry.get("extraction_mode"), entry.get("type"))
+++    tombstone = _normalize_tombstone(entry)
+++    if tombstone and bool(entry.get("enabled", False)):
+++        raise ValueError("tombstoned provider must set enabled=false")
+++
  +    mode = _coerce_mode(entry.get("mode"))
+++    urls = _normalized_url_list(entry) if not tombstone else []
+++    raw_extraction_mode = str(entry.get("extraction_mode") or entry.get("type") or "").strip().lower()
+++    extraction_mode = (
+++        _coerce_extraction_mode(entry.get("extraction_mode"), entry.get("type"))
+++        if not tombstone
+++        else str(entry.get("extraction_mode") or entry.get("type") or "snapshot_json")
+++    )
  +
  +    snapshot_path_raw = entry.get("snapshot_path")
  +    snapshot_dir_raw = entry.get("snapshot_dir")
  +    if snapshot_path_raw:
  +        snapshot_path = str(snapshot_path_raw)
  +    elif snapshot_dir_raw:
  +        snapshot_path = str(Path(str(snapshot_dir_raw)) / "index.html")
  +    elif extraction_mode in {"ashby", "jsonld", "html_list"}:
  +        snapshot_path = str(Path("data") / f"{provider_id}_snapshots" / "index.html")
  +    else:
  +        snapshot_path = str(Path("data") / f"{provider_id}_snapshots" / "jobs.json")
  +
  +    update_cadence = _normalize_update_cadence(entry)
  +    display_name = str(entry.get("display_name") or entry.get("name") or provider_id).strip()
  +    if not display_name:
  +        raise ValueError("provider entry display_name/name must be non-empty")
  +    llm_fallback = _normalize_llm_fallback(entry)
  +    if raw_extraction_mode == "llm_fallback" and not llm_fallback.get("enabled"):
  +        raise ValueError("extraction_mode 'llm_fallback' requires llm_fallback.enabled=true")
+++    if tombstone:
+++        normalized_tombstone: Dict[str, Any] = {
+++            "provider_id": provider_id,
+++            "display_name": display_name,
+++            "name": display_name,
+++            "schema_version": 1,
+++            "enabled": False,
+++            "tombstone": tombstone,
+++            "careers_urls": [],
+++            "careers_url": "",
+++            "board_url": "",
+++            "allowed_domains": [],
+++            "extraction_mode": extraction_mode,
+++            "type": extraction_mode,
+++            "mode": "snapshot",
+++            "live_enabled": False,
+++            "snapshot_enabled": False,
+++            "snapshot_path": snapshot_path,
+++            "snapshot_dir": str(Path(snapshot_path).parent),
+++            "llm_fallback": {"enabled": False},
+++            "update_cadence": {},
+++            "politeness": {},
+++        }
+++        return normalized_tombstone
  +    normalized: Dict[str, Any] = {
  +        "provider_id": provider_id,
  +        "display_name": display_name,
  +        "name": display_name,  # back-compat for existing callers
  +        "schema_version": 1,
  +        "enabled": bool(entry.get("enabled", True)),
  +        "careers_urls": urls,
  +        "careers_url": urls[0],
  +        "board_url": urls[0],  # back-compat for existing callers
  +        "allowed_domains": _allowed_domains(urls, entry.get("allowed_domains")),
  +        "extraction_mode": extraction_mode,
  +        "type": extraction_mode,  # back-compat
  +        "mode": mode,
  +        "live_enabled": bool(entry.get("live_enabled", True)),
  +        "snapshot_enabled": bool(entry.get("snapshot_enabled", True)),
  +        "snapshot_path": snapshot_path,
  +        "snapshot_dir": str(Path(snapshot_path).parent),
  +        "llm_fallback": llm_fallback,
  +        "update_cadence": update_cadence,
  +        "politeness": _normalize_politeness(entry),
  +    }
  +    return normalized
  +
  +
  +def load_providers_config(path: Path) -> List[Dict[str, Any]]:
  +    data = json.loads(path.read_text(encoding="utf-8"))
  +    schema = _load_providers_schema()
  +    if isinstance(data, list):
  +        entries = data
  +    elif isinstance(data, dict):
  +        _validate_top_level_schema(data, schema)
  +        entries = data.get("providers")
  +    else:
  +        raise ValueError("providers config must be a list or object with providers")
  +
  +    if not isinstance(entries, list):
  +        raise ValueError("providers config must include a providers list")
  +
  +    providers: List[Dict[str, Any]] = []
  +    seen_ids: set[str] = set()
  +    for item in entries:
  +        if not isinstance(item, dict):
  +            raise ValueError("provider entry must be a dict")
  +        _validate_provider_entry_schema(item, schema)
  +        normalized = _normalize_provider_entry(item)
  +        provider_id = normalized["provider_id"]
  +        if provider_id in seen_ids:
  +            raise ValueError(f"duplicate provider_id '{provider_id}'")
  +        seen_ids.add(provider_id)
  +        providers.append(normalized)
  +    providers.sort(key=lambda item: item["provider_id"])
  +    return providers
  +
  +
  +def resolve_provider_ids(
  +    providers_arg: str | None,
  +    providers_cfg: List[Dict[str, Any]],
  +    *,
  +    default_provider: str = "openai",
  +) -> List[str]:
  +    enabled_map = {entry["provider_id"]: bool(entry.get("enabled", True)) for entry in providers_cfg}
+++    tombstone_map = {
+++        entry["provider_id"]: str((entry.get("tombstone") or {}).get("reason") or "").strip()
+++        for entry in providers_cfg
+++        if isinstance(entry.get("tombstone"), dict)
+++    }
  +    requested = (providers_arg or "").strip()
  +    if requested.lower() == "all":
  +        providers = [entry["provider_id"] for entry in providers_cfg if enabled_map.get(entry["provider_id"], True)]
  +    else:
  +        providers = [p.strip() for p in requested.split(",") if p.strip()]
  +    if requested.lower() == "all" and not providers:
  +        raise ValueError("No enabled providers configured")
  +    if not providers:
  +        providers = [default_provider]
  +
  +    seen = set()
  +    out: List[str] = []
  +    for provider in providers:
  +        if provider not in seen:
  +            seen.add(provider)
  +            out.append(provider)
  +
  +    known = {entry["provider_id"] for entry in providers_cfg}
  +    unknown = [provider for provider in out if provider not in known]
  +    if unknown:
  +        unknown_list = ", ".join(unknown)
  +        raise ValueError(f"Unknown provider_id(s): {unknown_list}")
+++    tombstoned = [provider for provider in out if provider in tombstone_map]
+++    if tombstoned:
+++        tombstoned_list = ", ".join(tombstoned)
+++        raise ValueError(f"Provider(s) tombstoned in config: {tombstoned_list}")
  +    disabled = [provider for provider in out if not enabled_map.get(provider, True)]
  +    if disabled:
  +        disabled_list = ", ".join(disabled)
  +        raise ValueError(f"Provider(s) disabled in config: {disabled_list}")
  +    return out
diff --cc src/ji_engine/utils/redaction.py
index f4a4649,f4a4649,0000000..cf27698
mode 100644,100644,000000..100644
--- a/src/ji_engine/utils/redaction.py
+++ b/src/ji_engine/utils/redaction.py
@@@@ -1,98 -1,98 -1,0 +1,159 @@@@
  +"""
  +SignalCraft
  +Copyright (c) 2026 Chris Menendez.
  +All Rights Reserved.
  +See LICENSE for permitted use.
  +"""
  +
  +from __future__ import annotations
  +
  +import re
  +from dataclasses import dataclass
  +from typing import Any
  +
  +
  +@dataclass(frozen=True)
  +class Finding:
  +    pattern: str
  +    snippet: str
  +    location: str
  +
  +
  +_AWS_ACCESS_KEY_RE = re.compile(r"\b(AKIA|ASIA)[0-9A-Z]{16}\b")
  +_AWS_SECRET_KV_RE = re.compile(r"(?i)aws_secret_access_key\s*[:=]\s*([A-Za-z0-9/+=]{40})")
  +_DISCORD_WEBHOOK_RE = re.compile(r"https://(?:canary\.|ptb\.)?discord(?:app)?\.com/api/webhooks/\d+/[A-Za-z0-9._-]+")
  +_BEARER_RE = re.compile(r"\b[Bb]earer\s+[A-Za-z0-9._-]{20,}\b")
  +_GITHUB_TOKEN_RE = re.compile(r"\b(?:ghp|gho|ghu|ghs|ghr)_[A-Za-z0-9]{20,}\b")
  +_GITHUB_PAT_RE = re.compile(r"\bgithub_pat_[A-Za-z0-9_]{20,}\b")
  +_OPENAI_KEY_RE = re.compile(r"\bsk-[A-Za-z0-9]{20,}\b")
+++_JD_KEY_RE = re.compile(
+++    r'(?i)(?:"|\b)(jd_text|description_text|description_html|descriptionHtml|description|job_description)(?:"|\b)\s*[:=]'
+++)
+++_RAW_JD_KEYS = {
+++    "jd_text",
+++    "description",
+++    "description_text",
+++    "description_html",
+++    "descriptionhtml",
+++    "job_description",
+++}
  +
  +_TEXT_RULES: tuple[tuple[str, re.Pattern[str]], ...] = (
  +    ("aws_access_key_id", _AWS_ACCESS_KEY_RE),
  +    ("discord_webhook", _DISCORD_WEBHOOK_RE),
  +    ("bearer_token", _BEARER_RE),
  +    ("github_token", _GITHUB_TOKEN_RE),
  +    ("github_pat", _GITHUB_PAT_RE),
  +    ("openai_api_key", _OPENAI_KEY_RE),
  +)
  +
  +
  +def _clip(text: str, limit: int = 120) -> str:
  +    if len(text) <= limit:
  +        return text
  +    return text[: limit - 3] + "..."
  +
  +
  +def scan_text_for_secrets(text: str) -> list[Finding]:
  +    findings: list[Finding] = []
  +    for name, pattern in _TEXT_RULES:
  +        for match in pattern.finditer(text):
  +            findings.append(
  +                Finding(
  +                    pattern=name,
  +                    snippet=_clip(match.group(0)),
  +                    location=f"offset:{match.start()}",
  +                )
  +            )
  +
  +    # Tighten AWS secret detection to avoid random-string false positives:
  +    # only flag secret key when an access key id is also present in the same text.
  +    if _AWS_ACCESS_KEY_RE.search(text):
  +        for match in _AWS_SECRET_KV_RE.finditer(text):
  +            findings.append(
  +                Finding(
  +                    pattern="aws_secret_access_key_pair",
  +                    snippet=_clip(match.group(0)),
  +                    location=f"offset:{match.start()}",
  +                )
  +            )
  +    return findings
  +
  +
+++def scan_text_for_raw_jd(text: str) -> list[Finding]:
+++    findings: list[Finding] = []
+++    for match in _JD_KEY_RE.finditer(text):
+++        findings.append(
+++            Finding(
+++                pattern="raw_jd_field",
+++                snippet=_clip(match.group(0)),
+++                location=f"offset:{match.start()}",
+++            )
+++        )
+++    return findings
+++
+++
  +def scan_json_for_secrets(obj: Any) -> list[Finding]:
  +    findings: list[Finding] = []
  +
  +    def walk(value: Any, path: str) -> None:
  +        if isinstance(value, dict):
  +            for key in sorted(value.keys(), key=lambda k: str(k)):
  +                child = value[key]
  +                next_path = f"{path}.{key}" if path else str(key)
  +                walk(child, next_path)
  +            return
  +        if isinstance(value, list):
  +            for idx, child in enumerate(value):
  +                next_path = f"{path}[{idx}]"
  +                walk(child, next_path)
  +            return
  +        if isinstance(value, str):
  +            for finding in scan_text_for_secrets(value):
  +                findings.append(
  +                    Finding(
  +                        pattern=finding.pattern,
  +                        snippet=finding.snippet,
  +                        location=path or "$",
  +                    )
  +                )
  +
  +    walk(obj, "")
  +    return findings
+++
+++
+++def scan_json_for_raw_jd(obj: Any) -> list[Finding]:
+++    findings: list[Finding] = []
+++
+++    def walk(value: Any, path: str) -> None:
+++        if isinstance(value, dict):
+++            for key in sorted(value.keys(), key=lambda k: str(k)):
+++                key_text = str(key)
+++                normalized = "".join(ch for ch in key_text.lower() if ch.isalnum() or ch == "_")
+++                next_path = f"{path}.{key_text}" if path else key_text
+++                if normalized in _RAW_JD_KEYS:
+++                    findings.append(
+++                        Finding(
+++                            pattern="raw_jd_field",
+++                            snippet=_clip(key_text),
+++                            location=next_path,
+++                        )
+++                    )
+++                walk(value[key], next_path)
+++            return
+++        if isinstance(value, list):
+++            for idx, child in enumerate(value):
+++                walk(child, f"{path}[{idx}]")
+++            return
+++        if isinstance(value, str):
+++            for finding in scan_text_for_raw_jd(value):
+++                findings.append(
+++                    Finding(
+++                        pattern=finding.pattern,
+++                        snippet=finding.snippet,
+++                        location=path or "$",
+++                    )
+++                )
+++
+++    walk(obj, "")
+++    return findings
diff --cc tests/test_provider_registry.py
index d04fe75,d04fe75,0000000..cba4498
mode 100644,100644,000000..100644
--- a/tests/test_provider_registry.py
+++ b/tests/test_provider_registry.py
@@@@ -1,412 -1,412 -1,0 +1,482 @@@@
  +import json
  +from pathlib import Path
  +
  +import pytest
  +
  +from ji_engine.providers.registry import load_providers_config, resolve_provider_ids
  +
  +
  +def test_load_providers_config_defaults(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +[
  +  {
  +    "provider_id": "alpha",
  +    "careers_url": "https://example.com/alpha",
  +    "extraction_mode": "snapshot_json",
  +    "snapshot_path": "data/alpha.json"
  +  }
  +]
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    providers = load_providers_config(config_path)
  +    assert providers[0]["provider_id"] == "alpha"
  +    assert providers[0]["mode"] == "snapshot"
  +    assert providers[0]["enabled"] is True
  +    assert providers[0]["display_name"] == "alpha"
  +    assert providers[0]["extraction_mode"] == "snapshot_json"
  +    assert providers[0]["careers_urls"] == ["https://example.com/alpha"]
  +    assert providers[0]["allowed_domains"] == ["example.com"]
  +
  +
  +def test_load_providers_config_is_sorted_and_deterministic(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "zeta",
  +      "careers_urls": ["https://zeta.example/jobs"],
  +      "extraction_mode": "snapshot_json",
  +      "snapshot_path": "data/zeta.json"
  +    },
  +    {
  +      "provider_id": "alpha",
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "snapshot_json",
  +      "snapshot_path": "data/alpha.json"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    first = load_providers_config(config_path)
  +    second = load_providers_config(config_path)
  +    assert [p["provider_id"] for p in first] == ["alpha", "zeta"]
  +    assert first == second
  +
  +
  +def test_load_providers_config_rejects_duplicate_provider_ids(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +[
  +  {"provider_id":"dup","careers_url":"https://a.example/jobs","snapshot_path":"data/a.json","extraction_mode":"snapshot_json"},
  +  {"provider_id":"dup","careers_url":"https://b.example/jobs","snapshot_path":"data/b.json","extraction_mode":"snapshot_json"}
  +]
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    with pytest.raises(ValueError, match="duplicate provider_id"):
  +        load_providers_config(config_path)
  +
  +
  +def test_load_providers_config_politeness_defaults_and_overrides(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "jsonld",
  +      "snapshot_path": "data/alpha/index.html",
  +      "politeness": {
  +        "defaults": {
  +          "max_qps": 2.0,
  +          "max_attempts": 3
  +        },
  +        "host_overrides": {
  +          "alpha.example": {
  +            "max_qps": 1.0,
  +            "max_inflight_per_host": 1
  +          }
  +        }
  +      }
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    providers = load_providers_config(config_path)
  +    politeness = providers[0]["politeness"]
  +    assert politeness["max_qps"] == 2.0
  +    assert politeness["min_delay_s"] == 0.5
  +    assert politeness["host_qps_caps"] == {"alpha.example": 1.0}
  +    assert politeness["host_concurrency_caps"] == {"alpha.example": 1}
  +
  +
  +def test_adding_provider_entry_changes_registry_predictably(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    base_payload = {
  +        "schema_version": 1,
  +        "providers": [
  +            {
  +                "provider_id": "alpha",
  +                "careers_urls": ["https://alpha.example/jobs"],
  +                "extraction_mode": "snapshot_json",
  +                "snapshot_path": "data/alpha.json",
  +            }
  +        ],
  +    }
  +    config_path.write_text(json.dumps(base_payload), encoding="utf-8")
  +    before = load_providers_config(config_path)
  +    assert [entry["provider_id"] for entry in before] == ["alpha"]
  +
  +    base_payload["providers"].append(
  +        {
  +            "provider_id": "beta",
  +            "careers_urls": ["https://beta.example/jobs"],
  +            "extraction_mode": "snapshot_json",
  +            "snapshot_path": "data/beta.json",
  +        }
  +    )
  +    config_path.write_text(json.dumps(base_payload), encoding="utf-8")
  +    after = load_providers_config(config_path)
  +
  +    assert [entry["provider_id"] for entry in after] == ["alpha", "beta"]
  +    assert resolve_provider_ids("all", after) == ["alpha", "beta"]
  +
  +
  +def test_provider_registry_entry_interface_is_stable() -> None:
  +    providers = load_providers_config(Path("config/providers.json"))
  +    openai = next(entry for entry in providers if entry["provider_id"] == "openai")
  +    assert openai["display_name"] == openai["name"]
  +    assert openai["careers_url"] == openai["careers_urls"][0]
  +    assert openai["board_url"] == openai["careers_urls"][0]
  +    assert openai["type"] == openai["extraction_mode"]
  +
  +
  +def test_load_providers_config_rejects_unknown_keys(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "jsonld",
  +      "snapshot_path": "data/alpha/index.html",
  +      "unknown_key": "nope"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    with pytest.raises(ValueError, match="unsupported provider keys"):
  +        load_providers_config(config_path)
  +
  +
  +def test_load_providers_config_rejects_missing_extraction_mode(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "snapshot_path": "data/alpha/index.html"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    with pytest.raises(ValueError, match="missing extraction_mode/type"):
  +        load_providers_config(config_path)
  +
  +
  +def test_load_providers_config_rejects_llm_fallback_temp(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "jsonld",
  +      "snapshot_path": "data/alpha/index.html",
  +      "llm_fallback": {
  +        "enabled": true,
  +        "cache_dir": "state/llm_cache",
  +        "temperature": 0.2
  +      }
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    with pytest.raises(ValueError, match="llm_fallback.temperature"):
  +        load_providers_config(config_path)
  +
  +
  +def test_load_providers_config_supports_new_contract_fields_and_aliases(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "beta",
  +      "display_name": "Beta Corp",
  +      "enabled": false,
  +      "careers_urls": ["https://beta.example/jobs"],
  +      "allowed_domains": ["beta.example"],
  +      "extraction_mode": "html_rules",
  +      "update_cadence": "daily",
  +      "snapshot_path": "data/beta/index.html"
  +    },
  +    {
  +      "provider_id": "alpha",
  +      "display_name": "Alpha Corp",
  +      "enabled": true,
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "allowed_domains": ["alpha.example"],
  +      "extraction_mode": "ashby_api",
  +      "update_cadence": "hourly",
  +      "snapshot_path": "data/alpha/index.html"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +
  +    providers = load_providers_config(config_path)
  +    assert [item["provider_id"] for item in providers] == ["alpha", "beta"]
  +    assert providers[0]["display_name"] == "Alpha Corp"
  +    assert providers[0]["enabled"] is True
  +    assert providers[0]["extraction_mode"] == "ashby"
  +    assert providers[0]["update_cadence"] == "hourly"
  +    assert providers[1]["enabled"] is False
  +    assert providers[1]["extraction_mode"] == "html_list"
  +
  +
  +def test_resolve_provider_ids_all_includes_enabled_only(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "display_name": "Alpha",
  +      "enabled": true,
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "jsonld"
  +    },
  +    {
  +      "provider_id": "beta",
  +      "display_name": "Beta",
  +      "enabled": false,
  +      "careers_urls": ["https://beta.example/jobs"],
  +      "extraction_mode": "jsonld"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    providers = load_providers_config(config_path)
  +    assert resolve_provider_ids("all", providers) == ["alpha"]
  +
  +
  +def test_resolve_provider_ids_rejects_disabled_provider(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "display_name": "Alpha",
  +      "enabled": false,
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "jsonld"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    providers = load_providers_config(config_path)
  +    with pytest.raises(ValueError, match="disabled in config"):
  +        resolve_provider_ids("alpha", providers)
  +
  +
  +def test_load_providers_config_rejects_llm_fallback_mode_without_enabled_fallback(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "alpha",
  +      "display_name": "Alpha",
  +      "enabled": true,
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "extraction_mode": "llm_fallback"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +    with pytest.raises(ValueError, match="requires llm_fallback.enabled=true"):
  +        load_providers_config(config_path)
  +
  +
  +def test_load_providers_config_integration_expected_objects_stable_order(tmp_path: Path) -> None:
  +    config_path = tmp_path / "providers.json"
  +    config_path.write_text(
  +        """
  +{
  +  "schema_version": 1,
  +  "providers": [
  +    {
  +      "provider_id": "zeta",
  +      "display_name": "Zeta",
  +      "enabled": true,
  +      "careers_urls": ["https://zeta.example/jobs"],
  +      "allowed_domains": ["zeta.example"],
  +      "extraction_mode": "html_rules",
  +      "update_cadence": "daily"
  +    },
  +    {
  +      "provider_id": "alpha",
  +      "display_name": "Alpha",
  +      "enabled": true,
  +      "careers_urls": ["https://alpha.example/jobs"],
  +      "allowed_domains": ["alpha.example"],
  +      "extraction_mode": "ashby_api",
  +      "update_cadence": "hourly"
  +    }
  +  ]
  +}
  +""".strip(),
  +        encoding="utf-8",
  +    )
  +
  +    providers = load_providers_config(config_path)
  +    projection = [
  +        {
  +            "provider_id": item["provider_id"],
  +            "display_name": item["display_name"],
  +            "enabled": item["enabled"],
  +            "careers_urls": item["careers_urls"],
  +            "allowed_domains": item["allowed_domains"],
  +            "extraction_mode": item["extraction_mode"],
  +            "update_cadence": item["update_cadence"],
  +        }
  +        for item in providers
  +    ]
  +    assert projection == [
  +        {
  +            "provider_id": "alpha",
  +            "display_name": "Alpha",
  +            "enabled": True,
  +            "careers_urls": ["https://alpha.example/jobs"],
  +            "allowed_domains": ["alpha.example"],
  +            "extraction_mode": "ashby",
  +            "update_cadence": "hourly",
  +        },
  +        {
  +            "provider_id": "zeta",
  +            "display_name": "Zeta",
  +            "enabled": True,
  +            "careers_urls": ["https://zeta.example/jobs"],
  +            "allowed_domains": ["zeta.example"],
  +            "extraction_mode": "html_list",
  +            "update_cadence": "daily",
  +        },
  +    ]
+++
+++
+++def test_load_providers_config_supports_tombstone_entries(tmp_path: Path) -> None:
+++    config_path = tmp_path / "providers.json"
+++    config_path.write_text(
+++        """
+++{
+++  "schema_version": 1,
+++  "providers": [
+++    {
+++      "provider_id": "alpha",
+++      "display_name": "Alpha",
+++      "enabled": true,
+++      "careers_urls": ["https://alpha.example/jobs"],
+++      "extraction_mode": "jsonld"
+++    },
+++    {
+++      "provider_id": "legacy",
+++      "display_name": "Legacy Provider",
+++      "enabled": false,
+++      "tombstone": {
+++        "reason": "legal_takedown",
+++        "ticket": "LEGAL-42",
+++        "replaced_by": "alpha"
+++      }
+++    }
+++  ]
+++}
+++""".strip(),
+++        encoding="utf-8",
+++    )
+++    providers = load_providers_config(config_path)
+++    legacy = next(item for item in providers if item["provider_id"] == "legacy")
+++    assert legacy["enabled"] is False
+++    assert legacy["tombstone"]["reason"] == "legal_takedown"
+++    assert legacy["snapshot_enabled"] is False
+++    assert resolve_provider_ids("all", providers) == ["alpha"]
+++
+++
+++def test_resolve_provider_ids_rejects_tombstoned_provider(tmp_path: Path) -> None:
+++    config_path = tmp_path / "providers.json"
+++    config_path.write_text(
+++        """
+++{
+++  "schema_version": 1,
+++  "providers": [
+++    {
+++      "provider_id": "legacy",
+++      "display_name": "Legacy Provider",
+++      "enabled": false,
+++      "tombstone": {
+++        "reason": "provider_opt_out"
+++      }
+++    }
+++  ]
+++}
+++""".strip(),
+++        encoding="utf-8",
+++    )
+++    providers = load_providers_config(config_path)
+++    with pytest.raises(ValueError, match="tombstoned in config"):
+++        resolve_provider_ids("legacy", providers)
+++
+++
+++def test_repo_providers_config_schema_and_invariants() -> None:
+++    providers = load_providers_config(Path("config/providers.json"))
+++    assert providers
+++    ids = [entry["provider_id"] for entry in providers]
+++    assert ids == sorted(ids)
+++    assert len(ids) == len(set(ids))
diff --cc tests/test_redaction_scan.py
index 23b5e17,23b5e17,0000000..0c4d250
mode 100644,100644,000000..100644
--- a/tests/test_redaction_scan.py
+++ b/tests/test_redaction_scan.py
@@@@ -1,55 -1,55 -1,0 +1,76 @@@@
  +from __future__ import annotations
  +
-- from ji_engine.utils.redaction import scan_json_for_secrets, scan_text_for_secrets
+++from ji_engine.utils.redaction import (
+++    scan_json_for_raw_jd,
+++    scan_json_for_secrets,
+++    scan_text_for_raw_jd,
+++    scan_text_for_secrets,
+++)
  +
  +
  +def test_scan_text_for_secrets_detects_known_patterns() -> None:
  +    text = "\n".join(
  +        [
  +            "AWS_ACCESS_KEY_ID=AKIAABCDEFGHIJKLMNOP",
  +            "Authorization: Bearer token_abcdefghijklmnopqrstuvwxyz12345",
  +            "https://discord.com/api/webhooks/123456789012345678/abcdEFGHijklMNOP",
  +            "github_pat_abcdEFGHijklMNOPqrstUVWX1234567890",
  +            "ghp_abcdefghijklmnopqrstuvwxyzABCDE12345",
  +            "OPENAI_API_KEY=sk-ABCDEFGHIJKLMNOPQRSTUVWXYZ123456",
  +            "aws_secret_access_key = abcdEFGHijklMNOPqrstUVWXyz0123456789ABCD",
  +        ]
  +    )
  +    findings = scan_text_for_secrets(text)
  +    patterns = sorted({item.pattern for item in findings})
  +    assert patterns == [
  +        "aws_access_key_id",
  +        "aws_secret_access_key_pair",
  +        "bearer_token",
  +        "discord_webhook",
  +        "github_pat",
  +        "github_token",
  +        "openai_api_key",
  +    ]
  +
  +
  +def test_scan_text_for_secrets_does_not_flag_random_strings() -> None:
  +    text = "hash=0123456789abcdef0123456789abcdef01234567 and tokenish value abcdefghijklmnopqrs"
  +    assert scan_text_for_secrets(text) == []
  +
  +
  +def test_aws_secret_heuristic_requires_access_key_pairing() -> None:
  +    text = "aws_secret_access_key=abcdEFGHijklMNOPqrstUVWXyz0123456789ABCD"
  +    assert scan_text_for_secrets(text) == []
  +
  +
  +def test_scan_json_for_secrets_reports_deterministic_locations() -> None:
  +    payload = {
  +        "provenance": {
  +            "token": "Bearer token_abcdefghijklmnopqrstuvwxyz12345",
  +        },
  +        "items": [
  +            {"url": "https://discord.com/api/webhooks/123456789012345678/abcdEFGHijklMNOP"},
  +            {"note": "safe"},
  +        ],
  +    }
  +    findings = scan_json_for_secrets(payload)
  +    assert [(item.pattern, item.location) for item in findings] == [
  +        ("discord_webhook", "items[0].url"),
  +        ("bearer_token", "provenance.token"),
  +    ]
+++
+++
+++def test_scan_text_for_raw_jd_detects_keyed_payloads() -> None:
+++    text = '{"job":{"title":"Example","jd_text":"secret body"}}'
+++    findings = scan_text_for_raw_jd(text)
+++    assert len(findings) == 1
+++    assert findings[0].pattern == "raw_jd_field"
+++
+++
+++def test_scan_json_for_raw_jd_detects_nested_keys() -> None:
+++    payload = {"jobs": [{"title": "A", "description": "secret"}, {"title": "B", "meta": {"jd_text": "x"}}]}
+++    findings = scan_json_for_raw_jd(payload)
+++    assert [(item.pattern, item.location) for item in findings] == [
+++        ("raw_jd_field", "jobs[0].description"),
+++        ("raw_jd_field", "jobs[1].meta.jd_text"),
+++    ]
diff --cc tests/test_run_daily_provenance.py
index 6d7d16e,6d7d16e,0000000..0432862
mode 100644,100644,000000..100644
--- a/tests/test_run_daily_provenance.py
+++ b/tests/test_run_daily_provenance.py
@@@@ -1,66 -1,66 -1,0 +1,77 @@@@
  +import importlib
  +import json
  +import sys
  +from pathlib import Path
  +
  +import ji_engine.config as config
  +import scripts.run_daily as run_daily_module
  +import scripts.run_scrape as run_scrape_module
  +
  +
  +def test_run_daily_metadata_includes_provenance(tmp_path, monkeypatch) -> None:
  +    data_dir = tmp_path / "data"
  +    state_dir = tmp_path / "state"
  +    monkeypatch.setenv("JOBINTEL_DATA_DIR", str(data_dir))
  +    monkeypatch.setenv("JOBINTEL_STATE_DIR", str(state_dir))
  +    monkeypatch.setenv("CAREERS_MODE", "SNAPSHOT")
  +    monkeypatch.setenv("DISCORD_WEBHOOK_URL", "")
  +    monkeypatch.setenv("JOBINTEL_ECS_TASK_ARN", "arn:aws:ecs:us-east-1:123456789012:task/cluster/abc123")
  +    importlib.reload(config)
  +    run_daily = importlib.reload(run_daily_module)
  +    run_scrape = importlib.reload(run_scrape_module)
  +
  +    snapshot_src = Path("data/openai_snapshots/index.html")
  +    snapshot_dest = data_dir / "openai_snapshots" / "index.html"
  +    snapshot_dest.parent.mkdir(parents=True, exist_ok=True)
  +    snapshot_dest.write_text(snapshot_src.read_text(encoding="utf-8"), encoding="utf-8")
  +
  +    def fake_scrape_live(self):
  +        raise RuntimeError("Live scrape failed with status 403 at https://openai.com/careers/search/")
  +
  +    monkeypatch.setattr(run_scrape.OpenAICareersProvider, "scrape_live", fake_scrape_live)
  +
  +    def fake_run(cmd, *, stage: str):
  +        argv = cmd[1:] if cmd and cmd[0] == sys.executable else cmd
  +        script_path = Path(argv[0]).name if argv else ""
  +        if script_path == "run_scrape.py":
  +            sys.argv = [script_path, *argv[1:]]
  +            rc = run_scrape.main()
  +            if rc not in (None, 0):
  +                raise SystemExit(rc)
  +            return
  +        raise RuntimeError(f"Unexpected stage {stage}")
  +
  +    monkeypatch.setattr(run_daily, "_run", fake_run)
  +    monkeypatch.setattr(
  +        sys,
  +        "argv",
  +        ["run_daily.py", "--no_subprocess", "--scrape_only", "--providers", "openai", "--profiles", "cs"],
  +    )
  +    rc = run_daily.main()
  +    assert rc == 0
  +
  +    metadata_files = sorted(run_daily.RUN_METADATA_DIR.glob("*.json"))
  +    assert metadata_files
  +    data = json.loads(metadata_files[-1].read_text(encoding="utf-8"))
  +    build_provenance = data["provenance"]["build"]
  +    provenance = data["provenance_by_provider"]["openai"]
  +    assert data["provenance"]["openai"]["provider_id"] == "openai"
  +    assert build_provenance["ecs_task_arn"] == "arn:aws:ecs:us-east-1:123456789012:task/cluster/abc123"
+++    registry = build_provenance["provider_registry"]
+++    assert registry["path"] == "config/providers.json"
+++    assert isinstance(registry["sha256"], str) and len(registry["sha256"]) == 64
+++    assert registry["schema_version"] == 1
+++    artifact_model = data["artifact_model"]
+++    ui_path = Path(artifact_model["ui_safe"]["path"])
+++    replay_path = Path(artifact_model["replay_safe"]["path"])
+++    assert ui_path.exists()
+++    assert replay_path.exists()
+++    ui_payload = json.loads(ui_path.read_text(encoding="utf-8"))
+++    assert '"jd_text"' not in json.dumps(ui_payload, ensure_ascii=False)
  +    assert provenance["provider"] == "openai"
  +    assert provenance["scrape_mode"] == "snapshot"
  +    assert provenance["availability"] == "available"
  +    assert provenance["attempts_made"] >= 1
  +    assert provenance["snapshot_path"]
  +    assert provenance["snapshot_sha256"]
  +    assert provenance["parsed_job_count"] > 0
diff --cc tests/test_run_daily_provider_selection.py
index 1bb7ef4,1bb7ef4,0000000..1fdffb7
mode 100644,100644,000000..100644
--- a/tests/test_run_daily_provider_selection.py
+++ b/tests/test_run_daily_provider_selection.py
@@@@ -1,121 -1,121 -1,0 +1,145 @@@@
  +from __future__ import annotations
  +
  +import argparse
  +import json
  +from pathlib import Path
  +
  +import pytest
  +
  +import scripts.run_daily as run_daily
  +
  +
  +def test_resolve_providers_all_uses_registry(tmp_path: Path) -> None:
  +    providers_path = tmp_path / "providers.json"
  +    providers_path.write_text(
  +        json.dumps(
  +            {
  +                "schema_version": 1,
  +                "providers": [
  +                    {
  +                        "provider_id": "beta",
  +                        "careers_urls": ["https://beta.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/beta.json",
  +                    },
  +                    {
  +                        "provider_id": "alpha",
  +                        "careers_urls": ["https://alpha.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/alpha.json",
  +                    },
  +                ],
  +            }
  +        ),
  +        encoding="utf-8",
  +    )
  +    args = argparse.Namespace(providers="all", providers_config=str(providers_path))
  +    assert run_daily._resolve_providers(args) == ["alpha", "beta"]
  +
  +
  +def test_resolve_providers_explicit_order_and_dedupe(tmp_path: Path) -> None:
  +    providers_path = tmp_path / "providers.json"
  +    providers_path.write_text(
  +        json.dumps(
  +            {
  +                "schema_version": 1,
  +                "providers": [
  +                    {
  +                        "provider_id": "alpha",
  +                        "careers_urls": ["https://alpha.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/alpha.json",
  +                    },
  +                    {
  +                        "provider_id": "beta",
  +                        "careers_urls": ["https://beta.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/beta.json",
  +                    },
  +                ],
  +            }
  +        ),
  +        encoding="utf-8",
  +    )
  +    args = argparse.Namespace(providers="beta,alpha,beta", providers_config=str(providers_path))
  +    assert run_daily._resolve_providers(args) == ["beta", "alpha"]
  +
  +
  +def test_resolve_providers_unknown_fails_closed_with_exit_2(tmp_path: Path) -> None:
  +    providers_path = tmp_path / "providers.json"
  +    providers_path.write_text(
  +        json.dumps(
  +            {
  +                "schema_version": 1,
  +                "providers": [
  +                    {
  +                        "provider_id": "alpha",
  +                        "careers_urls": ["https://alpha.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/alpha.json",
  +                    }
  +                ],
  +            }
  +        ),
  +        encoding="utf-8",
  +    )
  +    args = argparse.Namespace(providers="alpha,missing", providers_config=str(providers_path))
  +    with pytest.raises(SystemExit) as exc:
  +        run_daily._resolve_providers(args)
  +    assert exc.value.code == 2
  +
  +
  +def test_resolve_providers_all_excludes_disabled(tmp_path: Path) -> None:
  +    providers_path = tmp_path / "providers.json"
  +    providers_path.write_text(
  +        json.dumps(
  +            {
  +                "schema_version": 1,
  +                "providers": [
  +                    {
  +                        "provider_id": "alpha",
  +                        "display_name": "Alpha",
  +                        "enabled": True,
  +                        "careers_urls": ["https://alpha.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/alpha.json",
  +                    },
  +                    {
  +                        "provider_id": "beta",
  +                        "display_name": "Beta",
  +                        "enabled": False,
  +                        "careers_urls": ["https://beta.example/jobs"],
  +                        "extraction_mode": "snapshot_json",
  +                        "snapshot_path": "data/beta.json",
  +                    },
  +                ],
  +            }
  +        ),
  +        encoding="utf-8",
  +    )
  +    args = argparse.Namespace(providers="all", providers_config=str(providers_path))
  +    assert run_daily._resolve_providers(args) == ["alpha"]
+++
+++
+++def test_resolve_providers_rejects_tombstoned_provider(tmp_path: Path) -> None:
+++    providers_path = tmp_path / "providers.json"
+++    providers_path.write_text(
+++        json.dumps(
+++            {
+++                "schema_version": 1,
+++                "providers": [
+++                    {
+++                        "provider_id": "legacy",
+++                        "display_name": "Legacy",
+++                        "enabled": False,
+++                        "tombstone": {"reason": "provider_opt_out"},
+++                    }
+++                ],
+++            }
+++        ),
+++        encoding="utf-8",
+++    )
+++    args = argparse.Namespace(providers="legacy", providers_config=str(providers_path))
+++    with pytest.raises(SystemExit) as exc:
+++        run_daily._resolve_providers(args)
+++    assert exc.value.code == 2
diff --cc tests/test_snapshot_contract.py
index de0b3ff,de0b3ff,0000000..8784f86
mode 100644,100644,000000..100644
--- a/tests/test_snapshot_contract.py
+++ b/tests/test_snapshot_contract.py
@@@@ -1,39 -1,39 -1,0 +1,47 @@@@
  +from pathlib import Path
  +
  +from ji_engine.providers.ashby_provider import AshbyProvider
  +from ji_engine.providers.registry import load_providers_config
  +from ji_engine.utils.job_id import extract_job_id_from_url
+++from jobintel.snapshots.validate import validate_snapshot_file
  +
  +
  +def test_openai_snapshot_contract() -> None:
  +    snapshot_path = Path("data/openai_snapshots/index.html")
  +    assert snapshot_path.exists(), f"Missing snapshot fixture: {snapshot_path}"
  +    provider = AshbyProvider(
  +        provider_id="openai",
  +        board_url="https://jobs.ashbyhq.com/openai",
  +        snapshot_dir=Path("data/openai_snapshots"),
  +        mode="SNAPSHOT",
  +    )
  +    jobs = provider.load_from_snapshot()
  +    assert len(jobs) > 100
  +
  +    apply_urls = [job.apply_url for job in jobs if job.apply_url]
  +    assert len(apply_urls) / len(jobs) >= 0.8
  +
  +    job_ids = [extract_job_id_from_url(url) for url in apply_urls]
  +    with_job_id = sum(1 for jid in job_ids if jid)
  +    assert with_job_id / len(jobs) >= 0.8
  +
  +
  +def test_ashby_snapshots_exist() -> None:
  +    providers = load_providers_config(Path("config/providers.json"))
--     ashby_entries = [p for p in providers if p.get("type") == "ashby"]
--     assert ashby_entries
+++    snapshot_entries = [p for p in providers if p.get("enabled", True) and p.get("snapshot_enabled", True)]
+++    assert snapshot_entries
  +    missing: list[str] = []
--     for entry in ashby_entries:
+++    invalid: list[str] = []
+++    for entry in snapshot_entries:
  +        snapshot_path = Path(entry["snapshot_path"])
  +        if not snapshot_path.exists():
  +            missing.append(str(snapshot_path))
  +            continue
  +        assert snapshot_path.stat().st_size > 0
--     assert not missing, f"Missing ashby snapshot fixtures: {', '.join(sorted(missing))}"
+++        ok, reason = validate_snapshot_file(
+++            entry["provider_id"], snapshot_path, extraction_mode=entry.get("extraction_mode")
+++        )
+++        if not ok:
+++            invalid.append(f"{entry['provider_id']}={reason}")
+++    assert not missing, f"Missing snapshot fixtures: {', '.join(sorted(missing))}"
+++    assert not invalid, f"Invalid snapshot fixtures: {', '.join(sorted(invalid))}"
